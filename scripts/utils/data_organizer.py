#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•°æ®æ•´ç†å·¥å…·
æ•´ç†çˆ¬å–çš„å°è¯´æ•°æ®ï¼Œå‡†å¤‡ç”¨äºè®­ç»ƒ
"""

import os
import re
import json
import shutil
from typing import List, Dict, Optional, Tuple
from pathlib import Path
from collections import defaultdict


class DataOrganizer:
    """æ•°æ®æ•´ç†å™¨"""
    
    def __init__(self, source_dir: str, target_dir: str = "data/training/processed"):
        """
        åˆå§‹åŒ–æ•°æ®æ•´ç†å™¨
        
        Args:
            source_dir: æºæ•°æ®ç›®å½•ï¼ˆçˆ¬å–çš„å°è¯´æ–‡ä»¶ï¼‰
            target_dir: ç›®æ ‡ç›®å½•ï¼ˆæ•´ç†åçš„æ•°æ®ï¼‰
        """
        self.source_dir = source_dir
        self.target_dir = target_dir
        os.makedirs(target_dir, exist_ok=True)
        
        # ç»Ÿè®¡æ•°æ®
        self.stats = {
            'total_files': 0,
            'processed_files': 0,
            'failed_files': 0,
            'total_chapters': 0,
            'total_chars': 0,
            'by_category': defaultdict(int)
        }
    
    def clean_text(self, text: str) -> str:
        """
        æ¸…ç†æ–‡æœ¬ï¼ˆå¢å¼ºç‰ˆï¼‰
        
        Args:
            text: åŸå§‹æ–‡æœ¬
        
        Returns:
            æ¸…ç†åçš„æ–‡æœ¬
        """
        if not text:
            return ""
        
        # ç§»é™¤å¤šä½™çš„ç©ºç™½ï¼ˆä¿ç•™æ®µè½ç»“æ„ï¼‰
        text = re.sub(r'[ \t]+', ' ', text)  # ç©ºæ ¼å’Œåˆ¶è¡¨ç¬¦
        text = re.sub(r'\n{4,}', '\n\n\n', text)  # è¿‡å¤šæ¢è¡Œ
        
        # ç§»é™¤å¹¿å‘Šå’Œæ— å…³å†…å®¹ï¼ˆæ›´å…¨é¢çš„æ¨¡å¼ï¼‰
        ad_patterns = [
            r'è¯·æ”¶è—.*?ç½‘å€',
            r'å–œæ¬¢.*?è¯·æ”¶è—',
            r'æ¨è.*?ä¸‹è½½',
            r'ã€.*?ã€‘',  # æ–¹æ‹¬å·å†…å®¹
            r'\(.*?\)',  # åœ†æ‹¬å·å†…å®¹ï¼ˆä½†ä¿ç•™å¯¹è¯ï¼‰
            r'www\.[^\s]+',  # ç½‘å€
            r'http[s]?://[^\s]+',  # HTTPé“¾æ¥
            r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}',  # é‚®ç®±
            r'QQ[ï¼š:ï¼š]?\d+',  # QQå·
            r'å¾®ä¿¡[ï¼š:ï¼š]?[^\s]+',  # å¾®ä¿¡å·
        ]
        for pattern in ad_patterns:
            text = re.sub(pattern, '', text, flags=re.IGNORECASE)
        
        # ç§»é™¤é‡å¤çš„ç« èŠ‚æ ‡é¢˜
        text = re.sub(r'ç¬¬\s*\d+\s*ç« [ï¼š:ï¼š]?\s*.*?\n.*?ç¬¬\s*\d+\s*ç« ', 
                     lambda m: m.group(0).split('\n')[-1], text)
        
        # ç§»é™¤é¡µçœ‰é¡µè„šæ ‡è®°
        footer_patterns = [
            r'ä¸Šä¸€é¡µ.*?ä¸‹ä¸€é¡µ',
            r'ç›®å½•.*?è¿”å›',
            r'è¿”å›.*?ç›®å½•',
            r'ä¸Šä¸€ç« .*?ä¸‹ä¸€ç« ',
        ]
        for pattern in footer_patterns:
            text = re.sub(pattern, '', text, flags=re.DOTALL)
        
        # æ ‡å‡†åŒ–æ ‡ç‚¹ç¬¦å·
        text = re.sub(r'[ï¼Œ,]{2,}', 'ï¼Œ', text)  # å¤šä¸ªé€—å·
        text = re.sub(r'[ã€‚.]{2,}', 'ã€‚', text)  # å¤šä¸ªå¥å·
        text = re.sub(r'[ï¼!]{2,}', 'ï¼', text)  # å¤šä¸ªæ„Ÿå¹å·
        text = re.sub(r'[ï¼Ÿ?]{2,}', 'ï¼Ÿ', text)  # å¤šä¸ªé—®å·
        
        return text.strip()
    
    def extract_metadata(self, file_path: str) -> Dict:
        """
        ä»æ–‡ä»¶è·¯å¾„å’Œå†…å®¹æå–å…ƒæ•°æ®
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
        
        Returns:
            å…ƒæ•°æ®å­—å…¸
        """
        metadata = {
            'file': file_path,
            'title': '',
            'author': '',
            'category': '',
            'chapters': 0,
            'total_chars': 0,
            'quality_score': 0
        }
        
        # ä»è·¯å¾„æå–åˆ†ç±»
        path_parts = Path(file_path).parts
        for part in path_parts:
            if part in ['éƒ½å¸‚', 'ç„å¹»', 'è¨€æƒ…', 'æ­¦ä¾ ', 'ç§‘å¹»', 'æ‚¬ç–‘', 'å†å²', 'å†›äº‹']:
                metadata['category'] = part
                break
        
        # ä»æ–‡ä»¶åæå–æ ‡é¢˜
        filename = Path(file_path).stem
        metadata['title'] = filename
        
        # è¯»å–æ–‡ä»¶å†…å®¹
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # æå–åŸºæœ¬ä¿¡æ¯
            title_match = re.search(r'æ ‡é¢˜[ï¼š:ï¼š]?\s*(.+)', content)
            author_match = re.search(r'ä½œè€…[ï¼š:ï¼š]?\s*(.+)', content)
            
            if title_match:
                metadata['title'] = title_match.group(1).strip()
            if author_match:
                metadata['author'] = author_match.group(1).strip()
            
            # ç»Ÿè®¡ç« èŠ‚æ•°
            chapters = re.findall(r'ç¬¬\s*\d+\s*ç« ', content)
            metadata['chapters'] = len(chapters)
            
            # ç»Ÿè®¡å­—ç¬¦æ•°
            metadata['total_chars'] = len(content)
            
            # è®¡ç®—è´¨é‡åˆ†æ•°ï¼ˆåŸºäºç« èŠ‚æ•°ã€å­—ç¬¦æ•°ã€å®Œæ•´æ€§ï¼‰
            quality = 0
            if metadata['chapters'] > 0:
                quality += 30
            if metadata['total_chars'] > 10000:
                quality += 30
            if metadata['total_chars'] > 100000:
                quality += 20
            if metadata['author']:
                quality += 10
            if metadata['title']:
                quality += 10
            
            metadata['quality_score'] = quality
            
        except Exception as e:
            print(f"âš ï¸  è¯»å–æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
        
        return metadata
    
    def organize_by_category(self) -> Dict[str, List[Dict]]:
        """
        æŒ‰åˆ†ç±»æ•´ç†æ•°æ®
        
        Returns:
            æŒ‰åˆ†ç±»ç»„ç»‡çš„æ•°æ®å­—å…¸
        """
        print(f"\nğŸ“ å¼€å§‹æ•´ç†æ•°æ®...")
        print(f"   æºç›®å½•: {self.source_dir}")
        print(f"   ç›®æ ‡ç›®å½•: {self.target_dir}")
        
        organized_data = defaultdict(list)
        
        # éå†æºç›®å½•
        for root, dirs, files in os.walk(self.source_dir):
            for file in files:
                if not file.endswith('.txt'):
                    continue
                
                file_path = os.path.join(root, file)
                self.stats['total_files'] += 1
                
                try:
                    # æå–å…ƒæ•°æ®
                    metadata = self.extract_metadata(file_path)
                    
                    if not metadata.get('category'):
                        # å°è¯•ä»çˆ¶ç›®å½•æ¨æ–­
                        parent_dir = os.path.basename(os.path.dirname(file_path))
                        if parent_dir in ['éƒ½å¸‚', 'ç„å¹»', 'è¨€æƒ…', 'æ­¦ä¾ ', 'ç§‘å¹»', 'æ‚¬ç–‘']:
                            metadata['category'] = parent_dir
                        else:
                            metadata['category'] = 'å…¶ä»–'
                    
                    # è¯»å–å¹¶æ¸…ç†å†…å®¹
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                    
                    cleaned_content = self.clean_text(content)
                    
                    # ä¿å­˜åˆ°ç›®æ ‡ç›®å½•
                    category = metadata['category']
                    category_dir = os.path.join(self.target_dir, category)
                    os.makedirs(category_dir, exist_ok=True)
                    
                    # ä¿å­˜æ¸…ç†åçš„æ–‡ä»¶
                    safe_title = re.sub(r'[<>:"/\\|?*]', '', metadata['title'] or file)
                    target_file = os.path.join(category_dir, f"{safe_title}.txt")
                    
                    with open(target_file, 'w', encoding='utf-8') as f:
                        f.write(cleaned_content)
                    
                    # ä¿å­˜å…ƒæ•°æ®
                    metadata_file = os.path.join(category_dir, f"{safe_title}.json")
                    with open(metadata_file, 'w', encoding='utf-8') as f:
                        json.dump(metadata, f, ensure_ascii=False, indent=2)
                    
                    organized_data[category].append(metadata)
                    self.stats['processed_files'] += 1
                    self.stats['total_chapters'] += metadata['chapters']
                    self.stats['total_chars'] += metadata['total_chars']
                    self.stats['by_category'][category] += 1
                    
                except Exception as e:
                    print(f"âŒ å¤„ç†æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
                    self.stats['failed_files'] += 1
        
        return dict(organized_data)
    
    def generate_summary(self, organized_data: Dict) -> Dict:
        """ç”Ÿæˆæ•´ç†æ‘˜è¦"""
        summary = {
            'stats': dict(self.stats),
            'categories': {},
            'quality_distribution': {
                'high': 0,  # >= 80
                'medium': 0,  # 50-79
                'low': 0  # < 50
            }
        }
        
        for category, novels in organized_data.items():
            category_stats = {
                'count': len(novels),
                'total_chapters': sum(n['chapters'] for n in novels),
                'total_chars': sum(n['total_chars'] for n in novels),
                'avg_quality': sum(n['quality_score'] for n in novels) / len(novels) if novels else 0
            }
            summary['categories'][category] = category_stats
            
            # è´¨é‡åˆ†å¸ƒ
            for novel in novels:
                score = novel['quality_score']
                if score >= 80:
                    summary['quality_distribution']['high'] += 1
                elif score >= 50:
                    summary['quality_distribution']['medium'] += 1
                else:
                    summary['quality_distribution']['low'] += 1
        
        return summary
    
    def organize(self) -> Dict:
        """
        æ‰§è¡Œæ•°æ®æ•´ç†
        
        Returns:
            æ•´ç†æ‘˜è¦
        """
        # æŒ‰åˆ†ç±»æ•´ç†
        organized_data = self.organize_by_category()
        
        # ç”Ÿæˆæ‘˜è¦
        summary = self.generate_summary(organized_data)
        
        # ä¿å­˜æ‘˜è¦
        summary_file = os.path.join(self.target_dir, 'organization_summary.json')
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        
        # æ‰“å°ç»Ÿè®¡
        print(f"\n{'='*60}")
        print("ğŸ“Š æ•°æ®æ•´ç†ç»Ÿè®¡")
        print(f"{'='*60}")
        print(f"  æ€»æ–‡ä»¶æ•°: {self.stats['total_files']}")
        print(f"  æˆåŠŸå¤„ç†: {self.stats['processed_files']}")
        print(f"  å¤±è´¥æ–‡ä»¶: {self.stats['failed_files']}")
        print(f"  æ€»ç« èŠ‚æ•°: {self.stats['total_chapters']}")
        print(f"  æ€»å­—ç¬¦æ•°: {self.stats['total_chars']:,}")
        print(f"\n  åˆ†ç±»åˆ†å¸ƒ:")
        for category, count in self.stats['by_category'].items():
            print(f"    {category}: {count} æœ¬")
        print(f"\n  è´¨é‡åˆ†å¸ƒ:")
        print(f"    é«˜è´¨é‡ (>=80): {summary['quality_distribution']['high']} æœ¬")
        print(f"    ä¸­ç­‰è´¨é‡ (50-79): {summary['quality_distribution']['medium']} æœ¬")
        print(f"    ä½è´¨é‡ (<50): {summary['quality_distribution']['low']} æœ¬")
        print(f"\nğŸ“ æ•´ç†åçš„æ•°æ®ä¿å­˜åœ¨: {self.target_dir}")
        
        return summary


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='æ•°æ®æ•´ç†å·¥å…·')
    parser.add_argument('source_dir', help='æºæ•°æ®ç›®å½•')
    parser.add_argument('--target', '-t', default='data/training/processed',
                       help='ç›®æ ‡ç›®å½•ï¼ˆé»˜è®¤: data/training/processedï¼‰')
    
    args = parser.parse_args()
    
    organizer = DataOrganizer(args.source_dir, args.target)
    organizer.organize()


if __name__ == '__main__':
    main()

