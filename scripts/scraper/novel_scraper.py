#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å°è¯´çˆ¬å–è„šæœ¬
æ”¯æŒçˆ¬å–å°è¯´çš„ç®€ä»‹ã€ç« èŠ‚åˆ—è¡¨å’Œç« èŠ‚å†…å®¹
"""

import requests
from bs4 import BeautifulSoup
import os
import time
import re
from urllib.parse import urljoin, urlparse
import json
from typing import List, Dict, Optional

# å¯¼å…¥é…ç½®æ¨¡å—
try:
    from .scraper_config import ScraperConfig
except ImportError:
    from scraper_config import ScraperConfig


class NovelScraper:
    """å°è¯´çˆ¬å–ç±»"""
    
    # å¸¸é‡å®šä¹‰ï¼ˆä½¿ç”¨é…ç½®æ¨¡å—çš„å€¼ï¼‰
    PROGRESS_SAVE_INTERVAL = ScraperConfig.PROGRESS_SAVE_INTERVAL
    MIN_CONTENT_LENGTH = ScraperConfig.MIN_CHAPTER_LENGTH
    
    def __init__(self, base_url: str, delay: float = 1.0, adaptive_delay: bool = True, output_dir: str = 'novels'):
        """
        åˆå§‹åŒ–çˆ¬è™«
        
        Args:
            base_url: å°è¯´ä¸»é¡µURL
            delay: è¯·æ±‚é—´éš”æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé¿å…è¯·æ±‚è¿‡å¿«
            adaptive_delay: æ˜¯å¦å¯ç”¨è‡ªé€‚åº”å»¶è¿Ÿï¼ˆé‡åˆ°502ç­‰é”™è¯¯æ—¶è‡ªåŠ¨å¢åŠ å»¶è¿Ÿï¼‰
            output_dir: è¾“å‡ºæ–‡ä»¶å¤¹åç§°ï¼Œé»˜è®¤ä¸º'novels'
        """
        self.base_url = base_url
        self.delay = delay
        self.base_delay = delay  # ä¿å­˜åŸºç¡€å»¶è¿Ÿ
        self.adaptive_delay = adaptive_delay
        self.consecutive_errors = 0  # è¿ç»­é”™è¯¯è®¡æ•°
        self.base_output_dir = output_dir  # åŸºç¡€è¾“å‡ºæ–‡ä»¶å¤¹
        self.novel_output_dir = None  # å°è¯´ä¸“ç”¨æ–‡ä»¶å¤¹ï¼ˆåœ¨è·å–æ ‡é¢˜ååˆ›å»ºï¼‰
        
        # åˆ›å»ºåŸºç¡€è¾“å‡ºæ–‡ä»¶å¤¹
        if not os.path.exists(self.base_output_dir):
            os.makedirs(self.base_output_dir)
            print(f"ğŸ“ åˆ›å»ºè¾“å‡ºæ–‡ä»¶å¤¹: {self.base_output_dir}/")
        
        self.session = requests.Session()
        # ä½¿ç”¨é…ç½®æ¨¡å—çš„è¯·æ±‚å¤´
        self.session.headers.update(ScraperConfig.get_headers())
        
        self.novel_info = {
            'title': '',
            'author': '',
            'description': '',
            'chapters': []
        }
    
    def get_page(self, url: str, retry: int = 5, silent: bool = False) -> Optional[BeautifulSoup]:
        """
        è·å–ç½‘é¡µå†…å®¹
        
        Args:
            url: ç½‘é¡µURL
            retry: é‡è¯•æ¬¡æ•°ï¼ˆé»˜è®¤5æ¬¡ï¼Œå¯¹502ç­‰æœåŠ¡å™¨é”™è¯¯æ›´æœ‰æ•ˆï¼‰
            silent: æ˜¯å¦é™é»˜æ¨¡å¼ï¼ˆä¸æ‰“å°é”™è¯¯ä¿¡æ¯ï¼‰
            
        Returns:
            BeautifulSoupå¯¹è±¡æˆ–None
        """
        for i in range(retry):
            try:
                time.sleep(self.delay)
                response = self.session.get(url, timeout=ScraperConfig.REQUEST_TIMEOUT)
                
                if response.status_code == 200:
                    # è®¾ç½®ç¼–ç 
                    response.encoding = response.apparent_encoding or 'utf-8'
                    
                    # æ£€æŸ¥å“åº”å†…å®¹æ˜¯å¦æœ‰æ•ˆ
                    if len(response.text) < 100:
                        # å“åº”å†…å®¹å¤ªçŸ­ï¼Œå¯èƒ½æ˜¯å‹ç¼©é—®é¢˜ï¼Œå°è¯•é‡æ–°è·å–
                        if not silent:
                            print(f"âš ï¸  å“åº”å†…å®¹å¼‚å¸¸ï¼Œå°è¯•é‡æ–°è·å–...")
                        # ç§»é™¤Accept-Encodingä¸­çš„brï¼ˆBrotliï¼‰ï¼ŒæŸäº›æœåŠ¡å™¨ä¸æ”¯æŒ
                        original_encoding = self.session.headers.get('Accept-Encoding', '')
                        self.session.headers['Accept-Encoding'] = 'gzip, deflate'
                        response = self.session.get(url, timeout=20)
                        response.encoding = response.apparent_encoding or 'utf-8'
                        # æ¢å¤åŸå§‹ç¼–ç è®¾ç½®
                        self.session.headers['Accept-Encoding'] = original_encoding
                    
                    # æˆåŠŸè¯·æ±‚ï¼Œé‡ç½®è¿ç»­é”™è¯¯è®¡æ•°
                    if self.consecutive_errors > 0:
                        self.consecutive_errors = 0
                        # å¦‚æœå»¶è¿Ÿè¢«å¢åŠ ï¼Œé€æ¸æ¢å¤
                        if self.delay > self.base_delay:
                            self.delay = max(self.base_delay, self.delay - 0.1)
                    return BeautifulSoup(response.text, 'html.parser')
                elif response.status_code >= 500:
                    # 5xxæœåŠ¡å™¨é”™è¯¯ï¼ˆå¦‚502 Bad Gateway, 503 Service Unavailableï¼‰
                    # ä½¿ç”¨é…ç½®çš„é€€é¿ç­–ç•¥
                    wait_time = ScraperConfig.calculate_retry_wait(i)
                    if not silent:
                        print(f"\nâš ï¸  æœåŠ¡å™¨é”™è¯¯ {response.status_code}ï¼Œç­‰å¾… {wait_time} ç§’åé‡è¯•... ({i+1}/{retry})")
                    
                    # å¦‚æœå¯ç”¨è‡ªé€‚åº”å»¶è¿Ÿï¼Œé‡åˆ°æœåŠ¡å™¨é”™è¯¯æ—¶å¢åŠ åŸºç¡€å»¶è¿Ÿ
                    if self.adaptive_delay and ScraperConfig.ADAPTIVE_DELAY_ENABLED and i == 0:
                        self.consecutive_errors += 1
                        # æ ¹æ®é…ç½®è°ƒæ•´å»¶è¿Ÿ
                        if self.consecutive_errors >= ScraperConfig.ERROR_THRESHOLD:
                            increment = (self.consecutive_errors // ScraperConfig.ERROR_THRESHOLD) * ScraperConfig.DELAY_INCREMENT
                            self.delay = min(self.base_delay + increment, ScraperConfig.MAX_DELAY)
                            if not silent:
                                print(f"   è‡ªåŠ¨è°ƒæ•´å»¶è¿Ÿè‡³ {self.delay:.1f} ç§’ï¼ˆè¿ç»­é”™è¯¯ {self.consecutive_errors} æ¬¡ï¼‰")
                    
                    if i < retry - 1:
                        time.sleep(wait_time)
                        continue
                elif response.status_code == 404:
                    # 404é”™è¯¯é€šå¸¸ä¸ä¼šæ¢å¤ï¼Œç›´æ¥è¿”å›
                    if not silent:
                        print(f"\nâŒ é¡µé¢ä¸å­˜åœ¨ (404): {url}")
                    return None
                else:
                    # å…¶ä»–HTTPé”™è¯¯
                    if not silent:
                        print(f"\nâš ï¸  è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code} - {url}")
                    if i < retry - 1:
                        time.sleep(2 * (i + 1))
                        
            except requests.exceptions.Timeout:
                wait_time = ScraperConfig.calculate_retry_wait(i)
                if not silent:
                    print(f"\nâ±ï¸  è¯·æ±‚è¶…æ—¶ï¼Œç­‰å¾… {wait_time} ç§’åé‡è¯•... ({i+1}/{retry})")
                if i < retry - 1:
                    time.sleep(wait_time)
            except requests.exceptions.ConnectionError as e:
                wait_time = ScraperConfig.calculate_retry_wait(i)
                if not silent:
                    print(f"\nğŸ”Œ è¿æ¥é”™è¯¯ï¼Œç­‰å¾… {wait_time} ç§’åé‡è¯•... ({i+1}/{retry})")
                if i < retry - 1:
                    time.sleep(wait_time)
            except Exception as e:
                wait_time = ScraperConfig.calculate_retry_wait(i)
                if not silent and i == retry - 1:
                    print(f"\nâŒ è¯·æ±‚å‡ºé”™: {e} - {url}")
                if i < retry - 1:
                    time.sleep(wait_time)
        
        if not silent:
            print(f"\nâŒ ç»è¿‡ {retry} æ¬¡é‡è¯•åä»ç„¶å¤±è´¥: {url}")
        return None
    
    def extract_novel_info(self, soup: BeautifulSoup) -> Dict:
        """
        æå–å°è¯´åŸºæœ¬ä¿¡æ¯ï¼ˆæ ‡é¢˜ã€ä½œè€…ã€ç®€ä»‹ï¼‰
        éœ€è¦æ ¹æ®å…·ä½“ç½‘ç«™ç»“æ„è°ƒæ•´é€‰æ‹©å™¨
        
        Args:
            soup: BeautifulSoupå¯¹è±¡
            
        Returns:
            åŒ…å«å°è¯´ä¿¡æ¯çš„å­—å…¸
        """
        info = {}
        
        # é’ˆå¯¹ä¹¦æµ·é˜ç½‘ç«™çš„ç‰¹æ®Šå¤„ç†
        if 'shuhaige.net' in self.base_url:
            # æå–æ ‡é¢˜ï¼ˆé€šå¸¸åœ¨h1ä¸­ï¼Œä½†å¯èƒ½åŒ…å«å…¶ä»–æ–‡æœ¬ï¼‰
            title_elem = soup.select_one('h1')
            if title_elem:
                title_text = title_elem.get_text(strip=True)
                # æ¸…ç†æ ‡é¢˜ï¼Œç§»é™¤"åˆ—è¡¨"ç­‰åç¼€
                title_text = re.sub(r'\s*åˆ—è¡¨\s*$', '', title_text)
                info['title'] = title_text
            
            # æå–ä½œè€…ï¼ˆæ›´ç²¾ç¡®çš„åŒ¹é…ï¼‰
            page_text = soup.get_text()
            
            # æ–¹æ³•1: ä»HTMLç»“æ„ä¸­æå–ï¼ˆæ›´å‡†ç¡®ï¼‰
            author_elem = soup.find(string=re.compile(r'ä½œè€…[ï¼š:]'))
            if author_elem:
                parent = author_elem.parent
                if parent:
                    author_text = parent.get_text(strip=True)
                    # æå–"ä½œè€…ï¼šxxx"ä¸­çš„xxxéƒ¨åˆ†ï¼Œåœ¨é‡åˆ°"éƒ½å¸‚"ã€"å·²å®Œç»“"ç­‰è¯ä¹‹å‰åœæ­¢
                    author_match = re.search(r'ä½œè€…[ï¼š:]\s*([^\s\n]+?)(?=\s*(?:éƒ½å¸‚|å·²å®Œç»“|æœ€æ–°ç« èŠ‚|ä¸‡å­—|æœ€åæ›´æ–°|\d+ç« ))', author_text)
                    if author_match:
                        author_name = author_match.group(1).strip()
                        # æ¸…ç†å¯èƒ½çš„é¢å¤–å­—ç¬¦
                        author_name = re.sub(r'[ï¼š:\s]+$', '', author_name)
                        if author_name and len(author_name) < 30:  # ä½œè€…åä¸åº”è¯¥å¤ªé•¿
                            info['author'] = author_name
            
            # æ–¹æ³•2: å¦‚æœä¸Šé¢æ²¡æ‰¾åˆ°ï¼Œä»é¡µé¢æ–‡æœ¬ä¸­ç²¾ç¡®æå–
            if not info.get('author'):
                # å°è¯•åŒ¹é…"ä½œè€…ï¼šxxx"æ ¼å¼ï¼Œåœ¨é‡åˆ°ç‰¹å®šå…³é”®è¯ä¹‹å‰åœæ­¢
                author_match = re.search(r'ä½œè€…[ï¼š:]\s*([^\s\n]+?)(?=\s*(?:éƒ½å¸‚|å·²å®Œç»“|æœ€æ–°ç« èŠ‚|ä¸‡å­—|æœ€åæ›´æ–°|\d+ç« ))', page_text)
                if not author_match:
                    # æ›´å®½æ¾çš„åŒ¹é…ï¼Œä½†é™åˆ¶é•¿åº¦
                    author_match = re.search(r'ä½œè€…[ï¼š:]\s*([^\n]{1,20}?)(?=\s|$)', page_text)
                if author_match:
                    author_text = author_match.group(1).strip()
                    # æ¸…ç†å¯èƒ½çš„é¢å¤–å­—ç¬¦
                    author_text = re.sub(r'[ï¼š:\s]+$', '', author_text)
                    if author_text and len(author_text) < 30:  # ä½œè€…åä¸åº”è¯¥å¤ªé•¿
                        info['author'] = author_text
            
            # æå–å…¶ä»–ä¿¡æ¯ï¼ˆç±»å‹ã€çŠ¶æ€ã€å­—æ•°ç­‰ï¼‰
            type_match = re.search(r'([^å·²å®Œç»“]+)\s*å·²å®Œç»“', page_text)
            if type_match:
                info['category'] = type_match.group(1).strip()
            
            word_match = re.search(r'(\d+)\s*ä¸‡å­—', page_text)
            if word_match:
                info['word_count'] = word_match.group(1) + 'ä¸‡å­—'
            
            chapter_match = re.search(r'å…±\s*(\d+)\s*ç« ', page_text)
            if chapter_match:
                info['total_chapters'] = int(chapter_match.group(1))
        
        # é€šç”¨é€‰æ‹©å™¨ï¼ˆé€‚ç”¨äºå…¶ä»–ç½‘ç«™ï¼‰
        if not info.get('title'):
            title_selectors = [
                'h1',
                '.book-title',
                '#book-title',
                'title',
                '.novel-title',
                'h2.title'
            ]
            for selector in title_selectors:
                title_elem = soup.select_one(selector)
                if title_elem:
                    info['title'] = title_elem.get_text(strip=True)
                    break
        
        if not info.get('author'):
            author_selectors = [
                '.author',
                '#author',
                '.book-author',
                'span.author',
                'a[href*="author"]'
            ]
            for selector in author_selectors:
                author_elem = soup.select_one(selector)
                if author_elem:
                    info['author'] = author_elem.get_text(strip=True)
                    break
        
        if not info.get('description'):
            desc_selectors = [
                '.description',
                '#description',
                '.book-intro',
                '.intro',
                '.summary',
                'div[class*="intro"]',
                'div[class*="desc"]'
            ]
            for selector in desc_selectors:
                desc_elem = soup.select_one(selector)
                if desc_elem:
                    info['description'] = desc_elem.get_text(strip=True)
                    break
        
        return info
    
    def extract_chapters(self, soup: BeautifulSoup) -> List[Dict]:
        """
        æå–ç« èŠ‚åˆ—è¡¨
        éœ€è¦æ ¹æ®å…·ä½“ç½‘ç«™ç»“æ„è°ƒæ•´é€‰æ‹©å™¨
        
        Args:
            soup: BeautifulSoupå¯¹è±¡
            
        Returns:
            ç« èŠ‚åˆ—è¡¨ï¼Œæ¯ä¸ªç« èŠ‚åŒ…å«æ ‡é¢˜å’Œé“¾æ¥
        """
        chapters = []
        
        # é’ˆå¯¹ä¹¦æµ·é˜ç½‘ç«™çš„ç‰¹æ®Šå¤„ç†
        if 'shuhaige.net' in self.base_url:
            # ä¹¦æµ·é˜çš„ç« èŠ‚é“¾æ¥æ ¼å¼é€šå¸¸æ˜¯ /350415/ç« èŠ‚å·
            # æŸ¥æ‰¾æ‰€æœ‰ç« èŠ‚é“¾æ¥
            all_links = soup.select('a[href]')
            base_path = urlparse(self.base_url).path.rstrip('/')
            base_id = base_path.split('/')[-1] if base_path else ''
            
            for link in all_links:
                href = link.get('href', '')
                title = link.get_text(strip=True)
                
                # åŒ¹é…ç« èŠ‚é“¾æ¥
                if href and title:
                    # æ£€æŸ¥æ˜¯å¦æ˜¯ç« èŠ‚é“¾æ¥ï¼ˆåŒ…å«"ç¬¬Xç« "æ ¼å¼ï¼‰
                    if re.search(r'ç¬¬\d+ç« ', title):
                        full_url = urljoin(self.base_url, href)
                        # ç¡®ä¿URLåŒ…å«ä¹¦ç±IDï¼ˆå¦‚350415ï¼‰
                        if base_id in href or base_path in href or href.startswith('/'):
                            chapters.append({
                                'title': title.strip(),
                                'url': full_url
                            })
                    # ä¹ŸåŒ¹é…æ•°å­—å¼€å¤´çš„é“¾æ¥ï¼ˆå¦‚ "1. ç« èŠ‚å"ï¼‰
                    elif re.match(r'^\d+[\.ã€]', title) and base_id in href:
                        full_url = urljoin(self.base_url, href)
                        chapters.append({
                            'title': title.strip(),
                            'url': full_url
                        })
            
            # å¦‚æœæ‰¾åˆ°äº†ç« èŠ‚ï¼Œå»é‡å¹¶æ’åº
            if chapters:
                # å»é‡ï¼ˆåŸºäºURLï¼‰
                seen_urls = set()
                unique_chapters = []
                for ch in chapters:
                    if ch['url'] not in seen_urls:
                        seen_urls.add(ch['url'])
                        unique_chapters.append(ch)
                
                # å°è¯•æŒ‰ç« èŠ‚å·æ’åº
                def get_chapter_num(title):
                    match = re.search(r'ç¬¬(\d+)ç« ', title)
                    if match:
                        return int(match.group(1))
                    match = re.search(r'^(\d+)\.', title)
                    if match:
                        return int(match.group(1))
                    return 0
                
                unique_chapters.sort(key=lambda x: get_chapter_num(x['title']))
                return unique_chapters
        
        # é€šç”¨é€‰æ‹©å™¨ï¼ˆé€‚ç”¨äºå…¶ä»–ç½‘ç«™ï¼‰
        chapter_selectors = [
            'a[href*="chapter"]',
            'a[href*="book"]',
            '.chapter-list a',
            '#chapter-list a',
            '.chapter a',
            'dd a',
            'li a[href*="/"]'
        ]
        
        for selector in chapter_selectors:
            chapter_links = soup.select(selector)
            if chapter_links and len(chapter_links) > 5:  # è‡³å°‘è¦æœ‰å‡ ä¸ªç« èŠ‚é“¾æ¥
                for link in chapter_links:
                    href = link.get('href', '')
                    title = link.get_text(strip=True)
                    if href and title:
                        full_url = urljoin(self.base_url, href)
                        chapters.append({
                            'title': title,
                            'url': full_url
                        })
                if chapters:
                    break
        
        return chapters
    
    def extract_all_chapters_with_pagination(self, soup: BeautifulSoup) -> List[Dict]:
        """
        æå–æ‰€æœ‰ç« èŠ‚ï¼ˆåŒ…æ‹¬åˆ†é¡µï¼‰
        é’ˆå¯¹ä¹¦æµ·é˜ç­‰æœ‰åˆ†é¡µçš„ç½‘ç«™
        
        Args:
            soup: BeautifulSoupå¯¹è±¡
            
        Returns:
            æ‰€æœ‰ç« èŠ‚åˆ—è¡¨
        """
        all_chapters = []
        
        # é’ˆå¯¹ä¹¦æµ·é˜ç½‘ç«™
        if 'shuhaige.net' in self.base_url:
            # å…ˆæå–å½“å‰é¡µçš„ç« èŠ‚
            current_chapters = self.extract_chapters(soup)
            all_chapters.extend(current_chapters)
            
            # æŸ¥æ‰¾åˆ†é¡µé“¾æ¥ï¼ˆå¦‚ï¼šç¬¬51-100ç« ã€ç¬¬101-150ç« ç­‰ï¼‰
            pagination_links = soup.select('a[href]')
            base_path = urlparse(self.base_url).path.rstrip('/')
            base_id = base_path.split('/')[-1] if base_path else ''
            
            # æå–æ‰€æœ‰åˆ†é¡µURL
            page_urls = set()
            page_text = soup.get_text()
            
            # é¦–å…ˆå°è¯•ä»é¡µé¢æ–‡æœ¬ä¸­æå–æ€»ç« èŠ‚æ•°
            total_match = re.search(r'å…±\s*(\d+)\s*ç« ', page_text)
            total_chapters = 0
            if total_match:
                total_chapters = int(total_match.group(1))
            
            # æ–¹æ³•1: ä»é“¾æ¥ä¸­æå–æ‰€æœ‰åˆ†é¡µé“¾æ¥
            for link in pagination_links:
                href = link.get('href', '')
                text = link.get_text(strip=True)
                
                # åŒ¹é…åˆ†é¡µé“¾æ¥ï¼ˆå¦‚ï¼šç¬¬51-100ç« ã€ç¬¬51 - 100ç« ç­‰ï¼‰
                page_match = re.search(r'ç¬¬(\d+)\s*-\s*(\d+)ç« ', text)
                if page_match or re.match(r'^\d+\s*-\s*\d+$', text):
                    full_url = urljoin(self.base_url, href)
                    # ç¡®ä¿æ˜¯åŒä¸€æœ¬ä¹¦çš„åˆ†é¡µ
                    if base_id in full_url or base_path in full_url or base_id in href or href.startswith('/'):
                        page_urls.add(full_url)
            
            # æ–¹æ³•2: å¦‚æœæ‰¾åˆ°äº†æ€»ç« èŠ‚æ•°ä½†åˆ†é¡µé“¾æ¥ä¸å¤Ÿï¼Œå°è¯•ç”Ÿæˆ
            if total_chapters > 0 and len(page_urls) < 5:
                # å‡è®¾æ¯é¡µ50ç« ï¼ˆæ ¹æ®å®é™…è§‚å¯Ÿï¼‰
                pages = (total_chapters + 49) // 50
                base_url_no_slash = self.base_url.rstrip('/')
                
                print(f"æ£€æµ‹åˆ°å…± {total_chapters} ç« ï¼Œé¢„è®¡ {pages} é¡µï¼Œå°è¯•ç”Ÿæˆåˆ†é¡µURL...")
                
                # å…ˆæµ‹è¯•ä¸€ä¸ªåˆ†é¡µURLæ ¼å¼ï¼ˆåªæµ‹è¯•ä¸€æ¬¡ï¼Œé¿å…æµªè´¹è¯·æ±‚ï¼‰
                test_page = 2
                test_urls = [
                    f"{base_url_no_slash}?page={test_page}",
                    f"{base_url_no_slash}/page/{test_page}",
                    f"{base_url_no_slash}?p={test_page}",
                ]
                
                valid_format = None
                for test_url in test_urls:
                    test_soup = self.get_page(test_url)
                    if test_soup:
                        test_chapters = self.extract_chapters(test_soup)
                        if test_chapters and len(test_chapters) > 0:
                            # æ‰¾åˆ°æœ‰æ•ˆæ ¼å¼
                            if '?page=' in test_url:
                                valid_format = f"{base_url_no_slash}?page={{}}"
                            elif '/page/' in test_url:
                                valid_format = f"{base_url_no_slash}/page/{{}}"
                            elif '?p=' in test_url:
                                valid_format = f"{base_url_no_slash}?p={{}}"
                            break
                    # å¦‚æœè¿™ä¸ªæ ¼å¼æ— æ•ˆï¼Œç«‹å³å°è¯•ä¸‹ä¸€ä¸ªï¼Œä¸ç­‰å¾…
                
                # å¦‚æœæ‰¾åˆ°æœ‰æ•ˆæ ¼å¼ï¼Œç”Ÿæˆæ‰€æœ‰åˆ†é¡µï¼ˆåŒ…æ‹¬ç¬¬ä¸€é¡µï¼‰
                if valid_format:
                    for p in range(1, pages + 1):
                        page_urls.add(valid_format.format(p))
                    print(f"âœ… æˆåŠŸç”Ÿæˆ {len(page_urls)} ä¸ªåˆ†é¡µURL")
                else:
                    print("âš ï¸  è­¦å‘Š: æ— æ³•ç¡®å®šåˆ†é¡µURLæ ¼å¼ï¼Œå°†åªçˆ¬å–å½“å‰é¡µçš„ç« èŠ‚")
            
            # è®¿é—®æ¯ä¸ªåˆ†é¡µ
            print(f"å‘ç° {len(page_urls)} ä¸ªåˆ†é¡µï¼Œå¼€å§‹æå–...")
            for i, page_url in enumerate(sorted(page_urls), 1):
                print(f"  æ­£åœ¨æå–ç¬¬ {i}/{len(page_urls)} é¡µ...")
                page_soup = self.get_page(page_url)
                if page_soup:
                    page_chapters = self.extract_chapters(page_soup)
                    all_chapters.extend(page_chapters)
            
            # å»é‡å¹¶æ’åº
            if all_chapters:
                seen_urls = set()
                unique_chapters = []
                for ch in all_chapters:
                    if ch['url'] not in seen_urls:
                        seen_urls.add(ch['url'])
                        unique_chapters.append(ch)
                
                def get_chapter_num(title):
                    match = re.search(r'ç¬¬(\d+)ç« ', title)
                    if match:
                        return int(match.group(1))
                    match = re.search(r'^(\d+)\.', title)
                    if match:
                        return int(match.group(1))
                    return 0
                
                unique_chapters.sort(key=lambda x: get_chapter_num(x['title']))
                return unique_chapters
        
        return all_chapters
    
    def extract_chapter_content(self, soup: BeautifulSoup) -> str:
        """
        æå–ç« èŠ‚æ­£æ–‡å†…å®¹
        éœ€è¦æ ¹æ®å…·ä½“ç½‘ç«™ç»“æ„è°ƒæ•´é€‰æ‹©å™¨
        
        Args:
            soup: BeautifulSoupå¯¹è±¡
            
        Returns:
            ç« èŠ‚æ­£æ–‡å†…å®¹
        """
        # é’ˆå¯¹ä¹¦æµ·é˜ç½‘ç«™çš„ç‰¹æ®Šå¤„ç†
        if 'shuhaige.net' in self.base_url:
            # å°è¯•å¤šç§å¯èƒ½çš„å†…å®¹é€‰æ‹©å™¨
            content_selectors = [
                '#content',
                '.content',
                '#chaptercontent',
                '.chaptercontent',
                '#chapterContent',
                '.chapterContent',
                'div[id*="content"]',
                'div[class*="content"]',
                'div[class*="text"]',
                'div[id*="text"]'
            ]
            
            for selector in content_selectors:
                content_elem = soup.select_one(selector)
                if content_elem:
                    # ç§»é™¤è„šæœ¬ã€æ ·å¼å’Œå¹¿å‘Š
                    for tag in content_elem(["script", "style", "noscript"]):
                        tag.decompose()
                    
                    # ç§»é™¤å¯èƒ½çš„å¹¿å‘Šå’Œå¯¼èˆªå…ƒç´ 
                    for ad in content_elem.select('.ad, .advertisement, .ads, [class*="ad"]'):
                        ad.decompose()
                    
                    text = content_elem.get_text(separator='\n', strip=True)
                    # æ¸…ç†å¤šä½™ç©ºè¡Œå’Œç‰¹æ®Šå­—ç¬¦
                    text = re.sub(r'\n{3,}', '\n\n', text)
                    text = re.sub(r'^\s*', '', text, flags=re.MULTILINE)
                    
                    # ç§»é™¤å¯èƒ½çš„ç½‘ç«™æ ‡è¯†æ–‡æœ¬å’Œæ— å…³å†…å®¹
                    text = re.sub(r'ä¹¦æµ·é˜.*?$', '', text, flags=re.MULTILINE)
                    text = re.sub(r'www\.shuhaige\.net.*?$', '', text, flags=re.MULTILINE)
                    text = re.sub(r'æ‰‹æœºé˜…è¯».*?$', '', text, flags=re.MULTILINE)
                    text = re.sub(r'è¿”å›ä¹¦é¡µ.*?$', '', text, flags=re.MULTILINE)
                    text = re.sub(r'ä¸Šä¸€ç« .*?ä¸‹ä¸€ç« .*?$', '', text, flags=re.MULTILINE)
                    text = re.sub(r'^\s*ç¬¬\d+ç« .*?$', '', text, flags=re.MULTILINE)  # ç§»é™¤é‡å¤çš„ç« èŠ‚æ ‡é¢˜
                    
                    # ç§»é™¤å¸¸è§çš„å¹¿å‘Šå’Œå¯¼èˆªæ–‡æœ¬
                    text = re.sub(r'(ç‚¹å‡»|æ”¶è—|æ¨è|è®¢é˜…|åŠ å…¥ä¹¦æ¶).*?$', '', text, flags=re.MULTILINE)
                    
                    # ç§»é™¤å¯èƒ½çš„ç« èŠ‚å¯¼èˆªé“¾æ¥æ–‡æœ¬
                    text = re.sub(r'ä¸Šä¸€é¡µ.*?ä¸‹ä¸€é¡µ.*?$', '', text, flags=re.MULTILINE)
                    text = re.sub(r'ç›®å½•.*?è¿”å›.*?$', '', text, flags=re.MULTILINE)
                    
                    if len(text) > 50:  # å†…å®¹åº”è¯¥æœ‰ä¸€å®šé•¿åº¦
                        return text.strip()
            
            # å¦‚æœä¸Šè¿°é€‰æ‹©å™¨éƒ½ä¸è¡Œï¼Œå°è¯•æå–bodyä¸­çš„ä¸»è¦æ–‡æœ¬
            body = soup.select_one('body')
            if body:
                # ç§»é™¤å¯¼èˆªã€é¡µçœ‰ã€é¡µè„šç­‰
                for tag in body.select('header, footer, nav, .header, .footer, .nav'):
                    tag.decompose()
                
                text = body.get_text(separator='\n', strip=True)
                text = re.sub(r'\n{3,}', '\n\n', text)
                if len(text) > 100:
                    return text.strip()
        
        # é€šç”¨é€‰æ‹©å™¨ï¼ˆé€‚ç”¨äºå…¶ä»–ç½‘ç«™ï¼‰
        content_selectors = [
            '#content',
            '.content',
            '.chapter-content',
            '#chapter-content',
            '.text-content',
            '#text-content',
            '.novel-content',
            'div[class*="content"]',
            'div[class*="text"]',
            'div[id*="content"]'
        ]
        
        for selector in content_selectors:
            content_elem = soup.select_one(selector)
            if content_elem:
                # ç§»é™¤è„šæœ¬å’Œæ ·å¼æ ‡ç­¾
                for script in content_elem(["script", "style"]):
                    script.decompose()
                
                text = content_elem.get_text(separator='\n', strip=True)
                # æ¸…ç†å¤šä½™ç©ºè¡Œ
                text = re.sub(r'\n{3,}', '\n\n', text)
                if len(text) > 100:  # å†…å®¹åº”è¯¥æœ‰ä¸€å®šé•¿åº¦
                    return text
        
        return ""
    
    def scrape_novel(self) -> Dict:
        """
        çˆ¬å–å®Œæ•´å°è¯´
        
        Returns:
            åŒ…å«å°è¯´å®Œæ•´ä¿¡æ¯çš„å­—å…¸
        """
        print(f"å¼€å§‹çˆ¬å–å°è¯´: {self.base_url}")
        
        # è·å–ä¸»é¡µ
        soup = self.get_page(self.base_url)
        if not soup:
            print("æ— æ³•è·å–å°è¯´ä¸»é¡µ")
            return self.novel_info
        
        # æå–åŸºæœ¬ä¿¡æ¯
        print("æå–å°è¯´åŸºæœ¬ä¿¡æ¯...")
        info = self.extract_novel_info(soup)
        self.novel_info.update(info)
        print(f"æ ‡é¢˜: {self.novel_info.get('title', 'æœªçŸ¥')}")
        print(f"ä½œè€…: {self.novel_info.get('author', 'æœªçŸ¥')}")
        
        # æå–ç« èŠ‚åˆ—è¡¨ï¼ˆåŒ…æ‹¬åˆ†é¡µï¼‰
        print("æå–ç« èŠ‚åˆ—è¡¨...")
        if 'shuhaige.net' in self.base_url:
            # å¯¹äºä¹¦æµ·é˜ï¼Œä½¿ç”¨æ”¯æŒåˆ†é¡µçš„æ–¹æ³•
            chapters = self.extract_all_chapters_with_pagination(soup)
        else:
            chapters = self.extract_chapters(soup)
        
        if not chapters:
            # å¦‚æœä¸»é¡µæ²¡æœ‰ç« èŠ‚åˆ—è¡¨ï¼Œå¯èƒ½éœ€è¦è®¿é—®ç›®å½•é¡µ
            # å°è¯•æŸ¥æ‰¾ç›®å½•é¡µé“¾æ¥
            catalog_links = soup.select('a[href*="catalog"], a[href*="index"], a[href*="list"], a[href*="chapter"]')
            for link in catalog_links:
                href = link.get('href', '')
                if 'catalog' in href.lower() or 'index' in href.lower() or 'list' in href.lower():
                    catalog_url = urljoin(self.base_url, href)
                    print(f"å°è¯•è®¿é—®ç›®å½•é¡µ: {catalog_url}")
                    catalog_soup = self.get_page(catalog_url)
                    if catalog_soup:
                        if 'shuhaige.net' in self.base_url:
                            chapters = self.extract_all_chapters_with_pagination(catalog_soup)
                        else:
                            chapters = self.extract_chapters(catalog_soup)
                        if chapters:
                            break
        
        if not chapters:
            print("è­¦å‘Š: æœªæ‰¾åˆ°ç« èŠ‚åˆ—è¡¨ï¼Œè¯·æ£€æŸ¥ç½‘ç«™ç»“æ„æˆ–æ‰‹åŠ¨æŒ‡å®šç« èŠ‚é€‰æ‹©å™¨")
            return self.novel_info
        
        print(f"æ‰¾åˆ° {len(chapters)} ä¸ªç« èŠ‚")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ä¿å­˜çš„è¿›åº¦æ–‡ä»¶ï¼ˆä¿å­˜åœ¨å°è¯´æ–‡ä»¶å¤¹ä¸­ï¼‰
        # å¦‚æœnovel_output_dirè¿˜æœªåˆ›å»ºï¼ˆå¯èƒ½æ˜¯ä»è¿›åº¦æ–‡ä»¶æ¢å¤ï¼‰ï¼Œä½¿ç”¨title_safe
        if not self.novel_output_dir:
            title_safe = re.sub(r'[<>:"/\\|?*]', '', self.novel_info.get('title', 'novel'))
            self.novel_output_dir = os.path.join(self.base_output_dir, title_safe)
            # ç¡®ä¿æ–‡ä»¶å¤¹å­˜åœ¨
            if not os.path.exists(self.novel_output_dir):
                os.makedirs(self.novel_output_dir)
        
        progress_file = os.path.join(self.novel_output_dir, f".{os.path.basename(self.novel_output_dir)}_progress.json")
        saved_progress = None
        start_index = 0
        
        if os.path.exists(progress_file):
            try:
                with open(progress_file, 'r', encoding='utf-8') as f:
                    saved_progress = json.load(f)
                saved_count = len(saved_progress.get('chapters', []))
                if saved_count > 0:
                    print(f"å‘ç°è¿›åº¦æ–‡ä»¶ï¼Œå·²çˆ¬å– {saved_count} ç« ")
                    # æ£€æŸ¥æ˜¯å¦åœ¨äº¤äº’å¼ç¯å¢ƒ
                    try:
                        import sys
                        if sys.stdin.isatty():
                            resume = input("æ˜¯å¦ç»§ç»­ä¹‹å‰çš„è¿›åº¦ï¼Ÿ(y/nï¼Œé»˜è®¤y): ").strip().lower()
                        else:
                            resume = 'y'  # éäº¤äº’ç¯å¢ƒé»˜è®¤ç»§ç»­
                    except (EOFError, KeyboardInterrupt):
                        resume = 'y'  # å‡ºé”™æ—¶é»˜è®¤ç»§ç»­
                    
                    if resume != 'n':
                        self.novel_info['chapters'] = saved_progress.get('chapters', [])
                        start_index = len(self.novel_info['chapters'])
                        print(f"ä»ç¬¬ {start_index + 1} ç« å¼€å§‹ç»§ç»­çˆ¬å–...")
                    else:
                        self.novel_info['chapters'] = []
                        start_index = 0
                        # åˆ é™¤æ—§çš„è¿›åº¦æ–‡ä»¶
                        try:
                            os.remove(progress_file)
                        except (OSError, FileNotFoundError):
                            pass  # æ–‡ä»¶ä¸å­˜åœ¨æˆ–æ— æ³•åˆ é™¤ï¼Œç»§ç»­æ‰§è¡Œ
                else:
                    self.novel_info['chapters'] = []
                    start_index = 0
            except Exception as e:
                print(f"è¯»å–è¿›åº¦æ–‡ä»¶å¤±è´¥: {e}ï¼Œå°†é‡æ–°å¼€å§‹")
                self.novel_info['chapters'] = []
                start_index = 0
        else:
            self.novel_info['chapters'] = []
            start_index = 0
        
        # çˆ¬å–æ¯ä¸ªç« èŠ‚çš„å†…å®¹
        total = len(chapters)
        failed_chapters = []  # å®Œå…¨å¤±è´¥çš„ç« èŠ‚
        empty_chapters = []   # å†…å®¹ä¸ºç©ºçš„ç« èŠ‚
        error_stats = {}      # é”™è¯¯ç»Ÿè®¡ï¼ˆæŒ‰é”™è¯¯ç±»å‹ï¼‰
        start_time = time.time()
        
        for i, chapter in enumerate(chapters[start_index:], start_index + 1):
            # æ˜¾ç¤ºè¿›åº¦ï¼ˆå¸¦æ—¶é—´ä¼°ç®—ï¼‰
            progress = (i / total) * 100
            bar_length = 40
            filled = int(bar_length * i / total)
            bar = '=' * filled + '-' * (bar_length - filled)
            
            # è®¡ç®—å‰©ä½™æ—¶é—´
            elapsed = time.time() - start_time
            if i > start_index + 1:
                avg_time = elapsed / (i - start_index)
                remaining = avg_time * (total - i)
                eta_str = f"å‰©ä½™: {int(remaining//60)}åˆ†{int(remaining%60)}ç§’"
            else:
                eta_str = "è®¡ç®—ä¸­..."
            
            chapter_title = chapter['title'][:25] + '...' if len(chapter['title']) > 25 else chapter['title']
            print(f"\r[{bar}] {i}/{total} ({progress:.1f}%) | {eta_str} | {chapter_title}", end='', flush=True)
            
            # å°è¯•çˆ¬å–ç« èŠ‚å†…å®¹ï¼ˆget_pageå†…éƒ¨å·²æœ‰é‡è¯•æœºåˆ¶ï¼Œè¿™é‡Œåªåšå†…å®¹éªŒè¯ï¼‰
            content = ''
            error_type = None
            
            # å…ˆå°è¯•è·å–é¡µé¢ï¼ˆé™é»˜æ¨¡å¼ï¼Œé¿å…æ­£å¸¸æƒ…å†µä¸‹çš„å™ªéŸ³ï¼‰
            # å¦‚æœå¤±è´¥ï¼Œä¼šåœ¨ç»Ÿè®¡ä¿¡æ¯ä¸­æ˜¾ç¤º
            chapter_soup = self.get_page(chapter['url'], retry=5, silent=True)
            
            if chapter_soup:
                # æ£€æŸ¥æ˜¯å¦æ˜¯åçˆ¬è™«é¡µé¢ï¼ˆå»¶è¿Ÿå¯¼å…¥é¿å…å¾ªç¯ä¾èµ–ï¼‰
                try:
                    from .data_validator import DataValidator
                except ImportError:
                    from data_validator import DataValidator
                
                if DataValidator.is_anti_crawl_page(chapter_soup):
                    error_type = 'åçˆ¬è™«é¡µé¢'
                    empty_chapters.append(chapter['title'])
                    content = ''
                else:
                    content = self.extract_chapter_content(chapter_soup)
                    # éªŒè¯å†…å®¹è´¨é‡
                    is_valid, error_msg = DataValidator.validate_chapter_content(content)
                    if is_valid:
                        # æ¸…ç†å†…å®¹
                        content = DataValidator.clean_content(content)
                    else:
                        error_type = error_msg
                        empty_chapters.append(chapter['title'])
                        content = ''  # æ— æ•ˆå†…å®¹ä¸ä¿å­˜
            else:
                # é¡µé¢è·å–å¤±è´¥ï¼ˆå¯èƒ½æ˜¯502ã€503ç­‰æœåŠ¡å™¨é”™è¯¯ï¼‰
                error_type = 'é¡µé¢è·å–å¤±è´¥'
                failed_chapters.append(chapter['title'])
            
            # ç»Ÿè®¡é”™è¯¯ç±»å‹
            if error_type:
                error_stats[error_type] = error_stats.get(error_type, 0) + 1
            
            # ä¿å­˜ç« èŠ‚ï¼ˆæ— è®ºæˆåŠŸä¸å¦éƒ½ä¿å­˜ï¼Œæ–¹ä¾¿åç»­å¤„ç†ï¼‰
            self.novel_info['chapters'].append({
                'title': chapter['title'],
                'url': chapter['url'],
                'content': content if content else '',
                'error': error_type if error_type else None
            })
            
            # æ¯Nç« ä¿å­˜ä¸€æ¬¡è¿›åº¦ï¼ˆé™é»˜ä¿å­˜ï¼Œä¸æ˜¾ç¤ºé”™è¯¯ï¼‰
            if i % self.PROGRESS_SAVE_INTERVAL == 0:
                try:
                    with open(progress_file, 'w', encoding='utf-8') as f:
                        json.dump(self.novel_info, f, ensure_ascii=False, indent=2)
                except (OSError, IOError, PermissionError):
                    pass  # é™é»˜å¤±è´¥ï¼Œä¸å½±å“ä¸»æµç¨‹
        
        print()  # æ¢è¡Œ
        
        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        total_time = time.time() - start_time
        success_count = len([c for c in self.novel_info['chapters'] if c.get('content')])
        print(f"\nâœ… çˆ¬å–å®Œæˆï¼")
        print(f"   æˆåŠŸ: {success_count}/{total} ç«  ({success_count/total*100:.1f}%)")
        
        # æ˜¾ç¤ºè¯¦ç»†é”™è¯¯ç»Ÿè®¡
        if failed_chapters or empty_chapters or error_stats:
            print(f"\nâš ï¸  é”™è¯¯ç»Ÿè®¡:")
            if failed_chapters:
                print(f"   é¡µé¢è·å–å¤±è´¥: {len(failed_chapters)} ç« ")
                if len(failed_chapters) <= 5:
                    for ch in failed_chapters:
                        print(f"     - {ch}")
                else:
                    for ch in failed_chapters[:5]:
                        print(f"     - {ch}")
                    print(f"     ... è¿˜æœ‰ {len(failed_chapters) - 5} ç« å¤±è´¥")
            
            if empty_chapters:
                print(f"   å†…å®¹ä¸ºç©º/è¿‡çŸ­: {len(empty_chapters)} ç« ")
                if len(empty_chapters) <= 3:
                    for ch in empty_chapters:
                        print(f"     - {ch}")
                else:
                    for ch in empty_chapters[:3]:
                        print(f"     - {ch}")
                    print(f"     ... è¿˜æœ‰ {len(empty_chapters) - 3} ç« ")
            
            if error_stats:
                print(f"   é”™è¯¯ç±»å‹åˆ†å¸ƒ:")
                for err_type, count in error_stats.items():
                    print(f"     - {err_type}: {count} æ¬¡")
        
        # è®¡ç®—æ€»å­—æ•°
        total_content_length = sum(len(ch.get('content', '')) for ch in self.novel_info['chapters'])
        
        print(f"\n   æ€»è€—æ—¶: {int(total_time//60)}åˆ†{int(total_time%60)}ç§’")
        print(f"   å¹³å‡é€Ÿåº¦: {total/(total_time/60):.1f} ç« /åˆ†é’Ÿ")
        if total_content_length > 0:
            word_count_mb = total_content_length / (1024 * 1024)
            print(f"   æ€»å­—æ•°: {total_content_length:,} å­— ({word_count_mb:.2f} MB)")
        
        # æ¸…ç†è¿›åº¦æ–‡ä»¶ï¼ˆçˆ¬å–æˆåŠŸååˆ é™¤ï¼‰
        if os.path.exists(progress_file):
            try:
                os.remove(progress_file)
                print(f"   å·²æ¸…ç†è¿›åº¦æ–‡ä»¶")
            except (OSError, FileNotFoundError, PermissionError):
                pass  # æ–‡ä»¶ä¸å­˜åœ¨æˆ–æ— æ³•åˆ é™¤ï¼Œé™é»˜å¤±è´¥
        
        return self.novel_info
    
    def save_to_txt(self, filename: Optional[str] = None):
        """
        ä¿å­˜ä¸ºTXTæ–‡ä»¶åˆ°è¾“å‡ºæ–‡ä»¶å¤¹
        
        Args:
            filename: è¾“å‡ºæ–‡ä»¶åï¼Œé»˜è®¤ä¸ºå°è¯´æ ‡é¢˜.txtï¼ˆä¼šè‡ªåŠ¨ä¿å­˜åˆ°è¾“å‡ºæ–‡ä»¶å¤¹ï¼‰
        """
        if not filename:
            title = self.novel_info.get('title', 'novel')
            # æ¸…ç†æ–‡ä»¶åä¸­çš„éæ³•å­—ç¬¦
            filename = re.sub(r'[<>:"/\\|?*]', '', title) + '.txt'
        
        # ç¡®ä¿æ–‡ä»¶ååœ¨å°è¯´æ–‡ä»¶å¤¹ä¸­
        if not self.novel_output_dir:
            # å¦‚æœæ–‡ä»¶å¤¹è¿˜æœªåˆ›å»ºï¼Œä½¿ç”¨æ ‡é¢˜åˆ›å»º
            title = self.novel_info.get('title', 'novel')
            title_safe = re.sub(r'[<>:"/\\|?*]', '', title)
            self.novel_output_dir = os.path.join(self.base_output_dir, title_safe)
            if not os.path.exists(self.novel_output_dir):
                os.makedirs(self.novel_output_dir)
        
        if not os.path.dirname(filename):
            filepath = os.path.join(self.novel_output_dir, filename)
        else:
            filepath = filename
        
        print(f"\nğŸ’¾ æ­£åœ¨ä¿å­˜TXTæ–‡ä»¶...")
        with open(filepath, 'w', encoding='utf-8') as f:
            # å†™å…¥åŸºæœ¬ä¿¡æ¯
            f.write(f"æ ‡é¢˜: {self.novel_info.get('title', 'æœªçŸ¥')}\n")
            f.write(f"ä½œè€…: {self.novel_info.get('author', 'æœªçŸ¥')}\n")
            f.write(f"\nç®€ä»‹:\n{self.novel_info.get('description', 'æ— ')}\n")
            f.write("\n" + "="*50 + "\n\n")
            
            # å†™å…¥ç« èŠ‚å†…å®¹
            total_chapters = len(self.novel_info['chapters'])
            for i, chapter in enumerate(self.novel_info['chapters'], 1):
                f.write(f"\nç¬¬ {i} ç« : {chapter['title']}\n")
                f.write("="*50 + "\n\n")
                content = chapter.get('content', '')
                if content:
                    f.write(content + "\n\n")
                else:
                    f.write(f"[å†…å®¹è·å–å¤±è´¥: {chapter.get('error', 'æœªçŸ¥é”™è¯¯')}]\n\n")
                
                # æ¯100ç« æ˜¾ç¤ºä¸€æ¬¡ä¿å­˜è¿›åº¦
                if i % 100 == 0:
                    print(f"  å·²ä¿å­˜ {i}/{total_chapters} ç« ...", end='\r', flush=True)
        
        # è·å–æ–‡ä»¶å¤§å°
        file_size = os.path.getsize(filepath)
        size_mb = file_size / (1024 * 1024)
        print(f"\nâœ… å°è¯´å·²ä¿å­˜åˆ°: {filepath}")
        print(f"   æ–‡ä»¶å¤§å°: {size_mb:.2f} MB")
    
    def save_to_json(self, filename: Optional[str] = None):
        """
        ä¿å­˜ä¸ºJSONæ–‡ä»¶åˆ°å°è¯´æ–‡ä»¶å¤¹
        
        Args:
            filename: è¾“å‡ºæ–‡ä»¶åï¼Œé»˜è®¤ä¸ºå°è¯´æ ‡é¢˜.jsonï¼ˆä¼šè‡ªåŠ¨ä¿å­˜åˆ°å°è¯´æ–‡ä»¶å¤¹ï¼‰
        """
        if not filename:
            title = self.novel_info.get('title', 'novel')
            filename = re.sub(r'[<>:"/\\|?*]', '', title) + '.json'
        
        # ç¡®ä¿æ–‡ä»¶ååœ¨å°è¯´æ–‡ä»¶å¤¹ä¸­
        if not self.novel_output_dir:
            # å¦‚æœæ–‡ä»¶å¤¹è¿˜æœªåˆ›å»ºï¼Œä½¿ç”¨æ ‡é¢˜åˆ›å»º
            title = self.novel_info.get('title', 'novel')
            title_safe = re.sub(r'[<>:"/\\|?*]', '', title)
            self.novel_output_dir = os.path.join(self.base_output_dir, title_safe)
            if not os.path.exists(self.novel_output_dir):
                os.makedirs(self.novel_output_dir)
        
        if not os.path.dirname(filename):
            filepath = os.path.join(self.novel_output_dir, filename)
        else:
            filepath = filename
        
        print(f"\nğŸ’¾ æ­£åœ¨ä¿å­˜JSONæ–‡ä»¶...")
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(self.novel_info, f, ensure_ascii=False, indent=2)
        
        # è·å–æ–‡ä»¶å¤§å°
        file_size = os.path.getsize(filepath)
        size_mb = file_size / (1024 * 1024)
        print(f"âœ… å°è¯´æ•°æ®å·²ä¿å­˜åˆ°: {filepath}")
        print(f"   æ–‡ä»¶å¤§å°: {size_mb:.2f} MB")


def main():
    """ä¸»å‡½æ•°"""
    import sys
    
    if len(sys.argv) < 2:
        print("ä½¿ç”¨æ–¹æ³•: python3 novel_scraper.py <å°è¯´URL>")
        print("ç¤ºä¾‹: python3 novel_scraper.py https://m.shuhaige.net/350415/")
        print("\nè¯´æ˜: é»˜è®¤ä¿å­˜ä¸ºTXTæ ¼å¼ï¼Œå¦‚éœ€JSONæ ¼å¼å¯åœ¨URLåæ·»åŠ  'json'")
        print("ç¤ºä¾‹: python3 novel_scraper.py https://m.shuhaige.net/350415/ json")
        sys.exit(1)
    
    url = sys.argv[1]
    # æ£€æŸ¥ç¬¬äºŒä¸ªå‚æ•°æ˜¯å¦æ˜¯è¾“å‡ºæ ¼å¼ï¼Œå¦åˆ™é»˜è®¤ä¸ºtxt
    output_format = 'txt'
    if len(sys.argv) > 2:
        if sys.argv[2].lower() in ['json', 'txt']:
            output_format = sys.argv[2].lower()
        else:
            print(f"è­¦å‘Š: æœªçŸ¥çš„è¾“å‡ºæ ¼å¼ '{sys.argv[2]}'ï¼Œå°†ä½¿ç”¨é»˜è®¤çš„TXTæ ¼å¼")
    
    # åˆ›å»ºçˆ¬è™«å®ä¾‹
    scraper = NovelScraper(url, delay=1.0)
    
    # çˆ¬å–å°è¯´
    novel_info = scraper.scrape_novel()
    
    # ä¿å­˜ç»“æœ
    if output_format == 'json':
        scraper.save_to_json()
    else:
        scraper.save_to_txt()
    
    print("\nâœ… å…¨éƒ¨å®Œæˆï¼")


if __name__ == '__main__':
    main()

