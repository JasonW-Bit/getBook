#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¤šç½‘ç«™æ‰¹é‡çˆ¬å–å™¨
æ”¯æŒå¤šä¸ªç½‘ç«™ï¼Œè‡ªåŠ¨é€‰æ‹©é€‚é…å™¨æˆ–è§£æç½‘ç«™ç»“æ„
"""

import os
import sys
import json
import time
import re
import argparse
import shutil
from typing import List, Dict, Optional
from pathlib import Path

# å¯¼å…¥é€‚é…å™¨å’Œç½‘ç«™ç®¡ç†å™¨
sys.path.insert(0, os.path.dirname(__file__))
from site_manager import SiteManager
from adapters.base_adapter import BaseSiteAdapter
from novel_scraper import NovelScraper
from novel_analyzer import NovelAnalyzer
from data_validator import DataValidator


class MultiSiteScraper:
    """å¤šç½‘ç«™æ‰¹é‡çˆ¬å–å™¨"""
    
    def __init__(self, output_base_dir: str = "data/training"):
        """
        åˆå§‹åŒ–å¤šç½‘ç«™çˆ¬å–å™¨
        
        Args:
            output_base_dir: è¾“å‡ºåŸºç¡€ç›®å½•
        """
        self.output_base_dir = output_base_dir
        self.site_manager = SiteManager()
        self.analyzer = NovelAnalyzer()
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_base_dir, exist_ok=True)
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total': 0,
            'success': 0,
            'failed': 0,
            'total_chapters': 0,
            'total_chars': 0,
            'sites': {}
        }
        
        self.scraped_novels = []
        self.failed_novels = []
    
    def register_site(self, url: str) -> Dict:
        """
        æ³¨å†Œç½‘ç«™
        
        Args:
            url: ç½‘ç«™URL
        
        Returns:
            ç½‘ç«™é…ç½®ä¿¡æ¯
        """
        return self.site_manager.register_site(url)
    
    def get_category_list(self, site_name: str, category: str, filter_completed: bool = True) -> List[Dict]:
        """
        è·å–æŒ‡å®šç½‘ç«™å’Œç±»å‹çš„å°è¯´åˆ—è¡¨
        
        Args:
            site_name: ç½‘ç«™åç§°
            category: å°è¯´ç±»å‹
            filter_completed: æ˜¯å¦åªè·å–å·²å®Œç»“çš„å°è¯´
        
        Returns:
            å°è¯´åˆ—è¡¨
        """
        print(f"\nğŸ“š æ­£åœ¨è·å– {site_name} çš„ {category} ç±»å‹å°è¯´åˆ—è¡¨...")
        if filter_completed:
            print(f"   ğŸ” ç­›é€‰æ¡ä»¶: ä»…å·²å®Œç»“")
        
        # è·å–é€‚é…å™¨
        adapter = self.site_manager.get_adapter_for_site(site_name)
        if not adapter:
            print(f"âŒ æ— æ³•è·å–é€‚é…å™¨ï¼Œè¯·å…ˆæ³¨å†Œç½‘ç«™")
            return []
        
        # è·å–åˆ†ç±»é¡µé¢URL
        category_url = adapter.get_category_url(category)
        print(f"   ğŸ”— åˆ†ç±»é¡µé¢: {category_url}")
        
        # è·å–åˆ†ç±»é¡µé¢
        try:
            scraper = NovelScraper(category_url, delay=1.5, output_dir=self.output_base_dir)
            soup = scraper.get_page(category_url, silent=False)
            if not soup:
                print(f"âŒ æ— æ³•è·å–åˆ†ç±»é¡µé¢")
                return []
        except Exception as e:
            print(f"âŒ è·å–åˆ†ç±»é¡µé¢å¤±è´¥: {e}")
            return []
        
        # è§£æåˆ†ç±»é¡µé¢
        novels = adapter.parse_category_page(soup, category)
        
        # ç­›é€‰å·²å®Œç»“
        if filter_completed:
            # å…ˆæ£€æŸ¥åˆ—è¡¨é¡µæ˜¯å¦æœ‰å®Œç»“æ ‡è¯†
            completed_count = sum(1 for n in novels if n.get('completed', False))
            
            print(f"   ğŸ“Š åˆ—è¡¨é¡µæ£€æµ‹åˆ° {completed_count} æœ¬æ ‡è®°ä¸ºå·²å®Œç»“çš„å°è¯´")
            
            # å¦‚æœåˆ—è¡¨é¡µæ²¡æœ‰å®Œç»“æ ‡è¯†æˆ–æ•°é‡ä¸è¶³ï¼Œè®¿é—®å°è¯´é¡µé¢è¯¦ç»†æ£€æŸ¥
            if completed_count == 0 or completed_count < len(novels) * 0.3:
                print(f"   âš ï¸  åˆ—è¡¨é¡µå®Œç»“æ ‡è¯†ä¸è¶³ï¼Œè®¿é—®å°è¯´è¯¦æƒ…é¡µè¿›è¡Œè¯¦ç»†æ£€æŸ¥...")
                checked_novels = []
                check_count = min(len(novels), 50)  # æ£€æŸ¥å‰50æœ¬
                
                for i, novel in enumerate(novels[:check_count], 1):
                    try:
                        novel_url = novel['url']
                        print(f"   ğŸ” [{i}/{check_count}] æ£€æŸ¥: {novel.get('title', 'æœªçŸ¥')[:30]}...", end=' ')
                        
                        temp_scraper = NovelScraper(novel_url, delay=0.8, output_dir=self.output_base_dir)
                        novel_soup = temp_scraper.get_page(novel_url, silent=True)
                        
                        try:
                            if novel_soup:
                                novel_text = novel_soup.get_text()
                                
                                # æ–¹æ³•1: æ£€æŸ¥é¡µé¢æ–‡æœ¬ä¸­çš„å®Œç»“æ ‡è¯†
                                is_completed = adapter.check_completed(novel_text)
                                
                                # æ–¹æ³•2: å¦‚æœæ–¹æ³•1æœªæ£€æµ‹åˆ°ï¼Œæ£€æŸ¥æœ€åä¸€ç« æ ‡é¢˜
                                if not is_completed:
                                    # å°è¯•æå–ç« èŠ‚åˆ—è¡¨ï¼Œæ£€æŸ¥æœ€åä¸€ç« 
                                    try:
                                        chapters = adapter.extract_chapters(novel_soup)
                                        if chapters:
                                            last_chapter = chapters[-1]
                                            last_chapter_title = last_chapter.get('title', '')
                                            # æ£€æŸ¥æœ€åä¸€ç« æ˜¯å¦åŒ…å«å®Œç»“æ ‡è¯†
                                            if adapter.check_completed(last_chapter_title):
                                                is_completed = True
                                            
                                            # å¦‚æœæœ€åä¸€ç« æ ‡é¢˜åŒ…å«"å¤§ç»“å±€"ã€"å®Œ"ç­‰ï¼Œä¹Ÿè®¤ä¸ºæ˜¯å®Œç»“
                                            if re.search(r'å¤§ç»“å±€|å…¨æ–‡å®Œ|å…¨ä¹¦å®Œ|å®Œ$', last_chapter_title):
                                                is_completed = True
                                    except:
                                        pass
                                
                                if is_completed:
                                    novel['completed'] = True
                                    checked_novels.append(novel)
                                    print("âœ… å·²å®Œç»“")
                                else:
                                    print("âŒ æœªå®Œç»“")
                            else:
                                print("âš ï¸  æ— æ³•è·å–é¡µé¢")
                        finally:
                            # æ¸…ç†ä¸´æ—¶scraperçš„session
                            if hasattr(temp_scraper, 'session'):
                                try:
                                    temp_scraper.session.close()
                                except:
                                    pass
                    except Exception as e:
                        print(f"âŒ æ£€æŸ¥å¤±è´¥: {str(e)[:30]}")
                        # ç¡®ä¿æ¸…ç†ä¸´æ—¶scraper
                        if 'temp_scraper' in locals() and hasattr(temp_scraper, 'session'):
                            try:
                                temp_scraper.session.close()
                            except:
                                pass
                        continue
                
                novels = checked_novels
                print(f"   âœ… è¯¦ç»†æ£€æŸ¥å®Œæˆï¼Œæ‰¾åˆ° {len(checked_novels)} æœ¬å·²å®Œç»“å°è¯´")
            else:
                # å¦‚æœåˆ—è¡¨é¡µæœ‰è¶³å¤Ÿçš„å®Œç»“æ ‡è¯†ï¼Œç›´æ¥ç­›é€‰ï¼Œä½†ä¹Ÿè¦éªŒè¯ä¸€ä¸‹
                print(f"   âœ… åˆ—è¡¨é¡µæœ‰è¶³å¤Ÿçš„å®Œç»“æ ‡è¯†ï¼Œç›´æ¥ç­›é€‰...")
                novels = [n for n in novels if n.get('completed', False)]
                
                # å¯¹ç­›é€‰ç»“æœè¿›è¡ŒäºŒæ¬¡éªŒè¯ï¼ˆéšæœºæŠ½æŸ¥å‡ æœ¬ï¼‰
                if len(novels) > 0:
                    import random
                    sample_size = min(3, len(novels))
                    sample_novels = random.sample(novels, sample_size)
                    print(f"   ğŸ” éšæœºæŠ½æŸ¥ {sample_size} æœ¬è¿›è¡ŒéªŒè¯...")
                    
                    verified_count = 0
                    for novel in sample_novels:
                        try:
                            novel_url = novel['url']
                            temp_scraper = NovelScraper(novel_url, delay=0.5, output_dir=self.output_base_dir)
                            novel_soup = temp_scraper.get_page(novel_url, silent=True)
                            if novel_soup:
                                novel_text = novel_soup.get_text()
                                if adapter.check_completed(novel_text):
                                    verified_count += 1
                        except:
                            pass
                    
                    if verified_count < sample_size * 0.5:
                        print(f"   âš ï¸  éªŒè¯é€šè¿‡ç‡è¾ƒä½ ({verified_count}/{sample_size})ï¼Œå»ºè®®ä½¿ç”¨è¯¦ç»†æ£€æŸ¥æ¨¡å¼")
        
        print(f"âœ… æ‰¾åˆ° {len(novels)} æœ¬ {category} ç±»å‹çš„å°è¯´" + 
              (f"ï¼ˆå·²ç­›é€‰ï¼šä»…å·²å®Œç»“ï¼‰" if filter_completed else ""))
        
        return novels
    
    def scrape_novel(self, site_name: str, novel_info: Dict) -> bool:
        """
        çˆ¬å–å•æœ¬å°è¯´
        
        Args:
            site_name: ç½‘ç«™åç§°
            novel_info: å°è¯´ä¿¡æ¯
        
        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        url = novel_info['url']
        title = novel_info['title']
        category = novel_info.get('category', 'æœªçŸ¥')
        
        print(f"\n{'='*60}")
        print(f"ğŸ“– æ­£åœ¨çˆ¬å–: {title}")
        print(f"   ç½‘ç«™: {site_name}")
        print(f"   ç±»å‹: {category}")
        print(f"   URL: {url}")
        print(f"{'='*60}")
        
        try:
            # åˆ›å»ºè¾“å‡ºç›®å½•ï¼šdata/training/novels/ç½‘ç«™å/ç±»å‹/å°è¯´å/
            # å…ˆä½¿ç”¨ä¸´æ—¶ç›®å½•è®©NovelScraperçˆ¬å–
            temp_dir = os.path.join(self.output_base_dir, 'novels', site_name, category, '.temp')
            os.makedirs(temp_dir, exist_ok=True)
            
            # ä½¿ç”¨NovelScraperçˆ¬å–
            scraper = NovelScraper(url, delay=1.5, output_dir=temp_dir)
            novel_info_dict = scraper.scrape_novel()
            
            if novel_info_dict and novel_info_dict.get('title'):
                # è·å–å®é™…æ ‡é¢˜
                actual_title = novel_info_dict.get('title', title)
                safe_title = re.sub(r'[<>:"/\\|?*]', '', actual_title)
                
                # åˆ›å»ºæœ€ç»ˆç›®å½•ï¼šç½‘ç«™å/ç±»å‹/å°è¯´å/
                novel_dir = os.path.join(self.output_base_dir, 'novels', site_name, category, safe_title)
                os.makedirs(novel_dir, exist_ok=True)
                
                # ç¡®å®šæ–‡ä»¶è·¯å¾„
                txt_file = os.path.join(novel_dir, f"{safe_title}.txt")
                json_file = os.path.join(novel_dir, f"{safe_title}.json")
                
                # æŸ¥æ‰¾NovelScraperä¿å­˜çš„æ–‡ä»¶
                source_txt = None
                source_json = None
                
                # æ£€æŸ¥NovelScraperåˆ›å»ºçš„å­æ–‡ä»¶å¤¹
                if hasattr(scraper, 'novel_output_dir') and scraper.novel_output_dir:
                    potential_txt = os.path.join(scraper.novel_output_dir, f"{safe_title}.txt")
                    potential_json = os.path.join(scraper.novel_output_dir, f"{safe_title}.json")
                    if os.path.exists(potential_txt):
                        source_txt = potential_txt
                    if os.path.exists(potential_json):
                        source_json = potential_json
                
                # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œåœ¨temp_dirä¸­æŸ¥æ‰¾
                if not source_txt:
                    for root, dirs, files in os.walk(temp_dir):
                        for file in files:
                            if file.endswith('.txt') and safe_title in file:
                                source_txt = os.path.join(root, file)
                                break
                        if source_txt:
                            break
                
                # ç§»åŠ¨æˆ–å¤åˆ¶æ–‡ä»¶åˆ°æœ€ç»ˆä½ç½®
                if source_txt and os.path.exists(source_txt) and source_txt != txt_file:
                    try:
                        shutil.move(source_txt, txt_file)
                    except (OSError, IOError, PermissionError):
                        # å¦‚æœç§»åŠ¨å¤±è´¥ï¼Œå°è¯•å¤åˆ¶
                        try:
                            shutil.copy2(source_txt, txt_file)
                        except (OSError, IOError, PermissionError):
                            print(f"âš ï¸  æ— æ³•ç§»åŠ¨æˆ–å¤åˆ¶æ–‡ä»¶: {source_txt}")
                elif not os.path.exists(txt_file):
                    # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œæ‰‹åŠ¨ä¿å­˜
                    content = self._extract_full_content(novel_info_dict)
                    if content:
                        with open(txt_file, 'w', encoding='utf-8') as f:
                            f.write(content)
                
                if source_json and os.path.exists(source_json) and source_json != json_file:
                    shutil.move(source_json, json_file)
                
                # ä¿å­˜TXTï¼ˆå¦‚æœè¿˜æ²¡æœ‰ä¿å­˜ï¼‰
                if not os.path.exists(txt_file):
                    try:
                        scraper.save_to_txt()
                        # ç§»åŠ¨æ–‡ä»¶åˆ°æ­£ç¡®ä½ç½®
                        if hasattr(scraper, 'novel_output_dir') and scraper.novel_output_dir:
                            source_file = os.path.join(scraper.novel_output_dir, f"{safe_title}.txt")
                            if os.path.exists(source_file) and source_file != txt_file:
                                try:
                                    shutil.move(source_file, txt_file)
                                except (OSError, IOError, PermissionError):
                                    try:
                                        shutil.copy2(source_file, txt_file)
                                    except (OSError, IOError, PermissionError):
                                        print(f"âš ï¸  æ— æ³•ç§»åŠ¨æˆ–å¤åˆ¶æ–‡ä»¶: {source_file}")
                    except (OSError, IOError, PermissionError) as e:
                        # æ‰‹åŠ¨ä¿å­˜
                        content = self._extract_full_content(novel_info_dict)
                        if content:
                            try:
                                with open(txt_file, 'w', encoding='utf-8') as f:
                                    f.write(content)
                            except (OSError, IOError, PermissionError):
                                print(f"âš ï¸  æ— æ³•ä¿å­˜æ–‡ä»¶: {txt_file}")
                
                # æ•°æ®è´¨é‡éªŒè¯
                is_valid, error_msg, validation_stats = DataValidator.validate_novel(novel_info_dict)
                
                if not is_valid:
                    print(f"âš ï¸  æ•°æ®è´¨é‡æ£€æŸ¥æœªé€šè¿‡: {error_msg}")
                    print(f"   ç»Ÿè®¡: æ€»ç« èŠ‚{validation_stats['total_chapters']}, "
                          f"æœ‰æ•ˆç« èŠ‚{validation_stats['valid_chapters']}, "
                          f"ç©ºç« èŠ‚{validation_stats['empty_chapters']}, "
                          f"æ€»å­—ç¬¦{validation_stats['total_chars']}, "
                          f"æœ‰æ•ˆå­—ç¬¦{validation_stats['valid_chars']}")
                    
                    # åˆ é™¤å·²åˆ›å»ºçš„æ–‡ä»¶å’Œç›®å½•
                    if os.path.exists(txt_file):
                        try:
                            os.remove(txt_file)
                        except (OSError, PermissionError):
                            pass
                    if os.path.exists(json_file):
                        try:
                            os.remove(json_file)
                        except (OSError, PermissionError):
                            pass
                    if os.path.exists(novel_dir):
                        try:
                            shutil.rmtree(novel_dir)
                        except (OSError, PermissionError):
                            pass  # é™é»˜å¤±è´¥ï¼Œä¸å½±å“ä¸»æµç¨‹
                    
                    self.failed_novels.append(novel_info)
                    self.stats['failed'] += 1
                    if site_name not in self.stats['sites']:
                        self.stats['sites'][site_name] = {'success': 0, 'failed': 0}
                    self.stats['sites'][site_name]['failed'] += 1
                    print(f"âŒ çˆ¬å–å¤±è´¥ï¼ˆæ•°æ®è´¨é‡ä¸åˆæ ¼ï¼‰: {title}")
                    return False
                
                # æ¸…ç†å†…å®¹ï¼ˆç§»é™¤ä¸ç›¸å…³æ•°æ®ï¼‰
                cleaned_chapters = []
                for chapter in novel_info_dict.get('chapters', []):
                    content = chapter.get('content', '')
                    cleaned_content = DataValidator.clean_content(content)
                    if cleaned_content:
                        cleaned_chapters.append({
                            'title': chapter.get('title', ''),
                            'url': chapter.get('url', ''),
                            'content': cleaned_content
                        })
                
                # æ›´æ–°å°è¯´ä¿¡æ¯
                novel_info_dict['chapters'] = cleaned_chapters
                
                # é‡æ–°ä¿å­˜æ¸…ç†åçš„å†…å®¹
                content = self._extract_full_content(novel_info_dict)
                if content:
                    with open(txt_file, 'w', encoding='utf-8') as f:
                        f.write(content)
                
                # ä¿å­˜å…ƒæ•°æ®
                metadata = {
                    'title': actual_title,
                    'author': novel_info_dict.get('author', 'æœªçŸ¥'),
                    'description': novel_info_dict.get('description', ''),
                    'url': url,
                    'site': site_name,
                    'category': category,
                    'scraped_time': time.strftime('%Y-%m-%d %H:%M:%S'),
                    'chapters': validation_stats['valid_chapters'],
                    'total_chapters': validation_stats['total_chapters'],
                    'total_chars': validation_stats['valid_chars'],
                    'validation_stats': validation_stats
                }
                
                with open(json_file, 'w', encoding='utf-8') as f:
                    json.dump(metadata, f, ensure_ascii=False, indent=2)
                
                # ç»Ÿè®¡
                self.stats['success'] += 1
                self.stats['total_chapters'] += validation_stats['valid_chapters']
                self.stats['total_chars'] += validation_stats['valid_chars']
                
                if site_name not in self.stats['sites']:
                    self.stats['sites'][site_name] = {'success': 0, 'failed': 0}
                self.stats['sites'][site_name]['success'] += 1
                
                self.scraped_novels.append({
                    **novel_info,
                    'title': actual_title,
                    'file': txt_file,
                    'site': site_name,
                    'category': category,
                    'novel_dir': novel_dir,
                    'metadata': metadata
                })
                
                print(f"âœ… çˆ¬å–æˆåŠŸ: {actual_title} ({validation_stats['valid_chapters']}/{validation_stats['total_chapters']}ç« æœ‰æ•ˆ, {validation_stats['valid_chars']}å­—ç¬¦)")
                
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶å’Œç›®å½•
                self._cleanup_temp_files(temp_dir, scraper)
                
                return True
            else:
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶ï¼ˆå³ä½¿å¤±è´¥ä¹Ÿè¦æ¸…ç†ï¼‰
                if 'temp_dir' in locals():
                    self._cleanup_temp_files(temp_dir, None)
                
                self.failed_novels.append(novel_info)
                self.stats['failed'] += 1
                if site_name not in self.stats['sites']:
                    self.stats['sites'][site_name] = {'success': 0, 'failed': 0}
                self.stats['sites'][site_name]['failed'] += 1
                print(f"âŒ çˆ¬å–å¤±è´¥: {title}")
                return False
                
        except Exception as e:
            import traceback
            print(f"âŒ çˆ¬å–å‡ºé”™: {e}")
            traceback.print_exc()
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶ï¼ˆå³ä½¿å‡ºé”™ä¹Ÿè¦æ¸…ç†ï¼‰
            if 'temp_dir' in locals():
                self._cleanup_temp_files(temp_dir, None)
            
            self.failed_novels.append(novel_info)
            self.stats['failed'] += 1
            if site_name not in self.stats['sites']:
                self.stats['sites'][site_name] = {'success': 0, 'failed': 0}
            self.stats['sites'][site_name]['failed'] += 1
            return False
    
    def batch_scrape(self, site_name: str, category: str, count: int = 10, 
                     filter_completed: bool = True) -> Dict:
        """
        æ‰¹é‡çˆ¬å–
        
        Args:
            site_name: ç½‘ç«™åç§°
            category: å°è¯´ç±»å‹
            count: çˆ¬å–æ•°é‡
            filter_completed: æ˜¯å¦åªçˆ¬å–å·²å®Œç»“çš„
        
        Returns:
            çˆ¬å–ç»Ÿè®¡
        """
        print(f"\nğŸš€ å¼€å§‹æ‰¹é‡çˆ¬å–")
        print(f"   ç½‘ç«™: {site_name}")
        print(f"   ç±»å‹: {category}")
        print(f"   æ•°é‡: {count} æœ¬")
        if filter_completed:
            print(f"   ç­›é€‰: ä»…å·²å®Œç»“")
        
        # è·å–å°è¯´åˆ—è¡¨
        novels = self.get_category_list(site_name, category, filter_completed)
        
        if not novels:
            print("âŒ æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„å°è¯´åˆ—è¡¨")
            return self.stats
        
        if len(novels) < count:
            print(f"âš ï¸  åªæ‰¾åˆ° {len(novels)} æœ¬ç¬¦åˆæ¡ä»¶çš„å°è¯´ï¼ˆè¯·æ±‚ {count} æœ¬ï¼‰")
        
        novels = novels[:count]
        self.stats['total'] = len(novels)
        
        print(f"\nğŸ“š å‡†å¤‡çˆ¬å–ä»¥ä¸‹ {len(novels)} æœ¬å°è¯´:")
        for i, novel in enumerate(novels[:10], 1):
            status = "âœ… å·²å®Œç»“" if novel.get('completed', False) else "â³ è¿è½½ä¸­"
            print(f"   {i}. {novel['title'][:50]:50s} - {status}")
        if len(novels) > 10:
            print(f"   ... è¿˜æœ‰ {len(novels) - 10} æœ¬")
        
        # é€ä¸ªçˆ¬å–
        for idx, novel_info in enumerate(novels, 1):
            print(f"\nğŸ“– è¿›åº¦: [{idx}/{len(novels)}]")
            self.scrape_novel(site_name, novel_info)
            if idx < len(novels):
                time.sleep(2)
        
        # æ‰¹é‡çˆ¬å–å®Œæˆåï¼Œæ¸…ç†æ‰€æœ‰ä¸´æ—¶ç›®å½•
        self._cleanup_all_temp_dirs(site_name, category)
        
        return self.stats
    
    def _cleanup_temp_files(self, temp_dir: str, scraper: Optional[NovelScraper] = None):
        """
        æ¸…ç†ä¸´æ—¶æ–‡ä»¶å’Œç›®å½•
        
        Args:
            temp_dir: ä¸´æ—¶ç›®å½•è·¯å¾„
            scraper: NovelScraperå®ä¾‹ï¼ˆå¯é€‰ï¼‰
        """
        try:
            # æ¸…ç†ä¸´æ—¶ç›®å½•
            if temp_dir and os.path.exists(temp_dir):
                # æ£€æŸ¥æ˜¯å¦æ˜¯.tempç›®å½•
                if temp_dir.endswith('.temp') or '.temp' in temp_dir:
                    try:
                        shutil.rmtree(temp_dir)
                        print(f"   ğŸ—‘ï¸  å·²æ¸…ç†ä¸´æ—¶ç›®å½•: {os.path.basename(temp_dir)}")
                    except (OSError, PermissionError) as e:
                        print(f"   âš ï¸  æ¸…ç†ä¸´æ—¶ç›®å½•å¤±è´¥: {e}")
            
            # æ¸…ç†scraperåˆ›å»ºçš„ä¸´æ—¶æ–‡ä»¶
            if scraper and hasattr(scraper, 'novel_output_dir') and scraper.novel_output_dir:
                # å¦‚æœnovel_output_diråœ¨temp_dirä¸­ï¼Œæ¸…ç†å®ƒ
                if temp_dir and temp_dir in scraper.novel_output_dir:
                    try:
                        if os.path.exists(scraper.novel_output_dir):
                            shutil.rmtree(scraper.novel_output_dir)
                    except (OSError, PermissionError):
                        pass
            
            # æ¸…ç†scraperçš„sessionï¼ˆé‡Šæ”¾è¿æ¥ï¼‰
            if scraper and hasattr(scraper, 'session'):
                try:
                    scraper.session.close()
                except:
                    pass
                    
        except Exception as e:
            # é™é»˜å¤±è´¥ï¼Œä¸å½±å“ä¸»æµç¨‹
            pass
    
    def _cleanup_all_temp_dirs(self, site_name: str, category: str):
        """
        æ¸…ç†æŒ‡å®šç½‘ç«™å’Œç±»å‹çš„æ‰€æœ‰ä¸´æ—¶ç›®å½•
        
        Args:
            site_name: ç½‘ç«™åç§°
            category: å°è¯´ç±»å‹
        """
        try:
            temp_base_dir = os.path.join(self.output_base_dir, 'novels', site_name, category)
            if os.path.exists(temp_base_dir):
                # æŸ¥æ‰¾æ‰€æœ‰.tempç›®å½•
                for root, dirs, files in os.walk(temp_base_dir):
                    if '.temp' in root:
                        try:
                            shutil.rmtree(root)
                            print(f"   ğŸ—‘ï¸  å·²æ¸…ç†ä¸´æ—¶ç›®å½•: {os.path.basename(root)}")
                        except (OSError, PermissionError):
                            pass
        except Exception as e:
            # é™é»˜å¤±è´¥
            pass
    
    def _extract_full_content(self, novel_info: Dict) -> str:
        """ä»å°è¯´ä¿¡æ¯ä¸­æå–å®Œæ•´å†…å®¹"""
        content_parts = []
        
        if novel_info.get('title'):
            content_parts.append(f"æ ‡é¢˜: {novel_info['title']}")
        if novel_info.get('author'):
            content_parts.append(f"ä½œè€…: {novel_info['author']}")
        if novel_info.get('description'):
            content_parts.append(f"\nç®€ä»‹:\n{novel_info['description']}")
        
        content_parts.append("\n" + "="*50 + "\n")
        
        chapters = novel_info.get('chapters', [])
        for chapter in chapters:
            if isinstance(chapter, dict):
                title = chapter.get('title', '')
                content = chapter.get('content', '')
                if title and content:
                    content_parts.append(f"\n{title}")
                    content_parts.append("="*50)
                    content_parts.append(f"\n{content}\n")
        
        return '\n'.join(content_parts)
    
    def generate_summary(self) -> Dict:
        """ç”Ÿæˆçˆ¬å–æ‘˜è¦"""
        summary = {
            'stats': self.stats,
            'successful_novels': [
                {
                    'title': n['title'],
                    'site': n.get('site', 'æœªçŸ¥'),
                    'category': n.get('category', 'æœªçŸ¥'),
                    'file': n.get('file', '')
                }
                for n in self.scraped_novels
            ],
            'failed_novels': [
                {
                    'title': n['title'],
                    'url': n['url']
                }
                for n in self.failed_novels
            ]
        }
        return summary


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description='å¤šç½‘ç«™æ‰¹é‡å°è¯´çˆ¬å–å·¥å…·',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # æ³¨å†Œç½‘ç«™å¹¶çˆ¬å–
  python3 multi_site_scraper.py --register https://m.shuhaige.net
  python3 multi_site_scraper.py --site m.shuhaige.net --category éƒ½å¸‚ --count 10
  
  # åˆ—å‡ºå·²æ³¨å†Œçš„ç½‘ç«™
  python3 multi_site_scraper.py --list-sites
  
  # çˆ¬å–å¤šä¸ªç½‘ç«™
  python3 multi_site_scraper.py --site m.shuhaige.net --category éƒ½å¸‚ --count 5
  python3 multi_site_scraper.py --site m.shuhaige.net --category ç„å¹» --count 5
        """
    )
    
    parser.add_argument('--register', type=str, metavar='URL',
                       help='æ³¨å†Œæ–°ç½‘ç«™ï¼ˆURLï¼‰')
    parser.add_argument('--list-sites', action='store_true',
                       help='åˆ—å‡ºæ‰€æœ‰å·²æ³¨å†Œçš„ç½‘ç«™')
    parser.add_argument('--site', type=str, metavar='SITE_NAME',
                       help='ç½‘ç«™åç§°ï¼ˆå¦‚ï¼šm.shuhaige.netï¼‰')
    parser.add_argument('--category', type=str, metavar='CATEGORY',
                       help='å°è¯´ç±»å‹ï¼ˆå¦‚ï¼šéƒ½å¸‚ã€ç„å¹»ã€è¨€æƒ…ç­‰ï¼‰')
    parser.add_argument('--count', type=int, default=10, metavar='N',
                       help='çˆ¬å–æ•°é‡ï¼ˆé»˜è®¤ï¼š10ï¼‰')
    parser.add_argument('--output', '-o', default='data/training',
                       help='è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤ï¼šdata/trainingï¼‰')
    parser.add_argument('--no-filter-completed', dest='filter_completed', action='store_false',
                       help='ä¸ç­›é€‰ï¼Œçˆ¬å–æ‰€æœ‰å°è¯´ï¼ˆåŒ…æ‹¬è¿è½½ä¸­çš„ï¼‰')
    parser.add_argument('--generate-data', '-g', action='store_true',
                       help='è‡ªåŠ¨ç”Ÿæˆè®­ç»ƒæ•°æ®æ–‡ä»¶')
    
    args = parser.parse_args()
    
    scraper = MultiSiteScraper(args.output)
    
    # æ³¨å†Œç½‘ç«™
    if args.register:
        config = scraper.register_site(args.register)
        if config:
            print(f"\nâœ… ç½‘ç«™æ³¨å†ŒæˆåŠŸ: {config.get('url')}")
            if config.get('adapter'):
                print(f"   é€‚é…å™¨: {config['adapter']}")
            else:
                print(f"   çŠ¶æ€: {config.get('status')}")
                print(f"   âš ï¸  éœ€è¦æ‰‹åŠ¨åˆ›å»ºé€‚é…å™¨")
        return
    
    # åˆ—å‡ºç½‘ç«™
    if args.list_sites:
        sites = scraper.site_manager.list_sites()
        print(f"\nğŸ“‹ å·²æ³¨å†Œçš„ç½‘ç«™ ({len(sites)} ä¸ª):")
        for site in sites:
            print(f"\n   {site['name']}")
            print(f"   URL: {site['url']}")
            print(f"   çŠ¶æ€: {site['status']}")
            if site.get('adapter'):
                print(f"   é€‚é…å™¨: {site['adapter']}")
            if site.get('categories'):
                print(f"   åˆ†ç±»: {', '.join(site['categories'])}")
        return
    
    # æ‰¹é‡çˆ¬å–
    if not args.site or not args.category:
        parser.print_help()
        print("\nâŒ é”™è¯¯: éœ€è¦æŒ‡å®š --site å’Œ --category")
        return
    
    stats = scraper.batch_scrape(args.site, args.category, args.count, args.filter_completed)
    
    # ç”Ÿæˆæ‘˜è¦
    summary = scraper.generate_summary()
    summary_file = os.path.join(args.output, 'novels', 'summary.json')
    os.makedirs(os.path.dirname(summary_file), exist_ok=True)
    with open(summary_file, 'w', encoding='utf-8') as f:
        json.dump(summary, f, ensure_ascii=False, indent=2)
    
    # ä¿å­˜çˆ¬å–çš„å°è¯´ä¿¡æ¯
    novels_info_file = os.path.join(args.output, 'novels', 'scraped_novels.json')
    with open(novels_info_file, 'w', encoding='utf-8') as f:
        json.dump(scraper.scraped_novels, f, ensure_ascii=False, indent=2)
    
    # æ‰“å°ç»Ÿè®¡
    print(f"\n{'='*60}")
    print("ğŸ“Š çˆ¬å–ç»Ÿè®¡")
    print(f"{'='*60}")
    print(f"  æ€»è®¡: {stats['total']} æœ¬")
    print(f"  æˆåŠŸ: {stats['success']} æœ¬")
    print(f"  å¤±è´¥: {stats['failed']} æœ¬")
    print(f"  æ€»ç« èŠ‚: {stats['total_chapters']} ç« ")
    print(f"  æ€»å­—ç¬¦: {stats['total_chars']:,} å­—ç¬¦")
    
    if stats['sites']:
        print(f"\n   æŒ‰ç½‘ç«™ç»Ÿè®¡:")
        for site_name, site_stats in stats['sites'].items():
            print(f"     {site_name}: æˆåŠŸ {site_stats['success']} æœ¬, å¤±è´¥ {site_stats['failed']} æœ¬")
    
    print(f"\nğŸ“ æ–‡ä»¶ä¿å­˜åœ¨: {args.output}/novels/")
    print(f"ğŸ“„ æ‘˜è¦æ–‡ä»¶: {summary_file}")
    
    # ç”Ÿæˆè®­ç»ƒæ•°æ®
    if args.generate_data:
        print(f"\nğŸ“ æ­£åœ¨ç”Ÿæˆè®­ç»ƒæ•°æ®...")
        sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'core'))
        from training_data_generator import TrainingDataGenerator
        
        generator = TrainingDataGenerator(args.output)
        training_file = generator.generate_from_novels(use_ai=args.use_ai)
        
        if training_file:
            print(f"   âœ… è®­ç»ƒæ•°æ®å·²ç”Ÿæˆ: {training_file}")
        else:
            print(f"   âŒ è®­ç»ƒæ•°æ®ç”Ÿæˆå¤±è´¥")


if __name__ == '__main__':
    main()

