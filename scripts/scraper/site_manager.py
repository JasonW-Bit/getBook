#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç½‘ç«™ç®¡ç†å™¨
è´Ÿè´£ç®¡ç†å¤šä¸ªç½‘ç«™çš„é€‚é…å™¨ï¼Œæ”¯æŒè‡ªåŠ¨å‘çŽ°å’Œè§£æž
"""

import os
import json
import re
from typing import Dict, List, Optional
from urllib.parse import urlparse
from bs4 import BeautifulSoup
import requests

from adapters import get_adapter, list_adapters, ADAPTERS
from adapters.base_adapter import BaseSiteAdapter


class SiteManager:
    """ç½‘ç«™ç®¡ç†å™¨"""
    
    def __init__(self, config_dir: str = "data/sites"):
        """
        åˆå§‹åŒ–ç½‘ç«™ç®¡ç†å™¨
        
        Args:
            config_dir: ç½‘ç«™é…ç½®ç›®å½•
        """
        self.config_dir = config_dir
        self.config_file = os.path.join(config_dir, 'sites.json')
        self.sites_config = self._load_config()
        
        # åˆ›å»ºé…ç½®ç›®å½•
        os.makedirs(config_dir, exist_ok=True)
    
    def _load_config(self) -> Dict:
        """åŠ è½½ç½‘ç«™é…ç½®"""
        if os.path.exists(self.config_file):
            try:
                with open(self.config_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except:
                pass
        return {}
    
    def _save_config(self):
        """ä¿å­˜ç½‘ç«™é…ç½®"""
        with open(self.config_file, 'w', encoding='utf-8') as f:
            json.dump(self.sites_config, f, ensure_ascii=False, indent=2)
    
    def get_site_name(self, url: str) -> str:
        """ä»ŽURLæå–ç½‘ç«™åç§°"""
        parsed = urlparse(url)
        domain = parsed.netloc or parsed.path.split('/')[0]
        # ç§»é™¤www.å‰ç¼€
        domain = re.sub(r'^www\.', '', domain)
        return domain
    
    def register_site(self, url: str, adapter_name: Optional[str] = None) -> Dict:
        """
        æ³¨å†Œç½‘ç«™ï¼ˆå¦‚æžœå·²æœ‰é€‚é…å™¨åˆ™ç›´æŽ¥ä½¿ç”¨ï¼Œå¦åˆ™å°è¯•è‡ªåŠ¨è§£æžï¼‰
        
        Args:
            url: ç½‘ç«™URL
            adapter_name: é€‚é…å™¨åç§°ï¼ˆå¯é€‰ï¼‰
        
        Returns:
            ç½‘ç«™é…ç½®ä¿¡æ¯
        """
        site_name = self.get_site_name(url)
        
        # æ£€æŸ¥æ˜¯å¦å·²æ³¨å†Œ
        if site_name in self.sites_config:
            print(f"âœ… ç½‘ç«™ {site_name} å·²æ³¨å†Œ")
            return self.sites_config[site_name]
        
        # æ£€æŸ¥æ˜¯å¦æœ‰çŽ°æˆçš„é€‚é…å™¨
        adapter_class = get_adapter(site_name)
        if adapter_class:
            print(f"âœ… æ‰¾åˆ°é€‚é…å™¨: {site_name}")
            config = {
                'url': url,
                'adapter': site_name,
                'status': 'ready',
                'categories': []
            }
            self.sites_config[site_name] = config
            self._save_config()
            return config
        
        # å¦‚æžœæ²¡æœ‰é€‚é…å™¨ï¼Œå°è¯•è‡ªåŠ¨è§£æž
        print(f"ðŸ” æœªæ‰¾åˆ°é€‚é…å™¨ï¼Œå°è¯•è‡ªåŠ¨è§£æžç½‘ç«™: {site_name}")
        config = self._auto_discover_site(url, site_name)
        
        if config:
            self.sites_config[site_name] = config
            self._save_config()
            return config
        else:
            print(f"âš ï¸  æ— æ³•è‡ªåŠ¨è§£æžç½‘ç«™ï¼Œè¯·æ‰‹åŠ¨åˆ›å»ºé€‚é…å™¨")
            return None
    
    def _auto_discover_site(self, url: str, site_name: str) -> Optional[Dict]:
        """
        è‡ªåŠ¨å‘çŽ°ç½‘ç«™ç»“æž„
        
        Args:
            url: ç½‘ç«™URL
            site_name: ç½‘ç«™åç§°
        
        Returns:
            ç½‘ç«™é…ç½®ä¿¡æ¯
        """
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            }
            
            response = requests.get(url, headers=headers, timeout=10)
            response.encoding = response.apparent_encoding or 'utf-8'
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # å°è¯•æŸ¥æ‰¾åˆ†ç±»é“¾æŽ¥
            categories = self._discover_categories(soup, url)
            
            # å°è¯•æŸ¥æ‰¾å°è¯´é“¾æŽ¥æ ¼å¼
            novel_pattern = self._discover_novel_pattern(soup)
            
            config = {
                'url': url,
                'adapter': None,  # éœ€è¦æ‰‹åŠ¨åˆ›å»ºé€‚é…å™¨
                'status': 'discovered',
                'categories': categories,
                'novel_pattern': novel_pattern,
                'discovery_info': {
                    'title': soup.title.string if soup.title else None,
                    'links_count': len(soup.find_all('a')),
                }
            }
            
            print(f"âœ… è‡ªåŠ¨å‘çŽ°å®Œæˆ:")
            print(f"   æ‰¾åˆ° {len(categories)} ä¸ªå¯èƒ½çš„åˆ†ç±»")
            print(f"   å°è¯´é“¾æŽ¥æ¨¡å¼: {novel_pattern}")
            
            return config
            
        except Exception as e:
            print(f"âŒ è‡ªåŠ¨å‘çŽ°å¤±è´¥: {e}")
            return None
    
    def _discover_categories(self, soup: BeautifulSoup, base_url: str) -> List[str]:
        """å‘çŽ°ç½‘ç«™çš„åˆ†ç±»"""
        categories = []
        
        # æŸ¥æ‰¾å¯èƒ½åŒ…å«åˆ†ç±»çš„é“¾æŽ¥
        links = soup.find_all('a', href=True)
        category_keywords = ['éƒ½å¸‚', 'çŽ„å¹»', 'è¨€æƒ…', 'æ­¦ä¾ ', 'ç§‘å¹»', 'æ‚¬ç–‘', 'åŽ†å²', 'å†›äº‹']
        
        for link in links:
            href = link.get('href', '')
            text = link.get_text(strip=True)
            
            # æ£€æŸ¥é“¾æŽ¥æ–‡æœ¬æˆ–URLä¸­æ˜¯å¦åŒ…å«åˆ†ç±»å…³é”®è¯
            for keyword in category_keywords:
                if keyword in text or keyword in href:
                    if href not in categories:
                        categories.append(keyword)
                        break
        
        return list(set(categories))  # åŽ»é‡
    
    def _discover_novel_pattern(self, soup: BeautifulSoup) -> str:
        """å‘çŽ°å°è¯´é“¾æŽ¥çš„æ¨¡å¼"""
        links = soup.find_all('a', href=True)
        
        # æŸ¥æ‰¾åŒ…å«æ•°å­—çš„é“¾æŽ¥ï¼ˆå¯èƒ½æ˜¯å°è¯´é“¾æŽ¥ï¼‰
        patterns = []
        for link in links[:50]:  # åªæ£€æŸ¥å‰50ä¸ªé“¾æŽ¥
            href = link.get('href', '')
            if re.search(r'/\d{4,}/', href):
                patterns.append('æ•°å­—IDæ ¼å¼: /æ•°å­—ID/')
                break
            elif re.search(r'/novel/', href):
                patterns.append('novelæ ¼å¼: /novel/...')
                break
            elif re.search(r'/book/', href):
                patterns.append('bookæ ¼å¼: /book/...')
                break
        
        return patterns[0] if patterns else 'æœªçŸ¥'
    
    def get_adapter_for_site(self, site_name: str) -> Optional[BaseSiteAdapter]:
        """
        èŽ·å–ç½‘ç«™çš„é€‚é…å™¨å®žä¾‹
        
        Args:
            site_name: ç½‘ç«™åç§°
        
        Returns:
            é€‚é…å™¨å®žä¾‹
        """
        config = self.sites_config.get(site_name)
        if not config:
            return None
        
        adapter_name = config.get('adapter')
        if not adapter_name:
            return None
        
        adapter_class = get_adapter(adapter_name)
        if not adapter_class:
            return None
        
        base_url = config.get('url')
        return adapter_class(base_url)
    
    def list_sites(self) -> List[Dict]:
        """åˆ—å‡ºæ‰€æœ‰å·²æ³¨å†Œçš„ç½‘ç«™"""
        return [
            {
                'name': name,
                'url': config.get('url'),
                'status': config.get('status', 'unknown'),
                'adapter': config.get('adapter'),
                'categories': config.get('categories', [])
            }
            for name, config in self.sites_config.items()
        ]

