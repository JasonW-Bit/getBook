#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¨¡å‹è¯„ä¼°æ¨¡å—
ç”¨äºè¯„ä¼°è®­ç»ƒå¥½çš„æ¨¡å‹æ€§èƒ½
"""

import os
import json
import numpy as np
from typing import List, Dict, Tuple
from collections import Counter

try:
    from .tensorflow_model import TensorFlowTextRewriter
except ImportError:
    from tensorflow_model import TensorFlowTextRewriter


class ModelEvaluator:
    """æ¨¡å‹è¯„ä¼°å™¨"""
    
    def __init__(self, model_path: str = "models/text_rewriter_model"):
        """
        åˆå§‹åŒ–è¯„ä¼°å™¨
        
        Args:
            model_path: æ¨¡å‹è·¯å¾„
        """
        self.model_path = model_path
        self.rewriter = TensorFlowTextRewriter(model_path=model_path)
        self.model_loaded = False
    
    def load_model(self) -> bool:
        """åŠ è½½æ¨¡å‹"""
        if self.rewriter.load_vocab() and self.rewriter.load_model():
            self.model_loaded = True
            return True
        return False
    
    def evaluate_accuracy(self, test_data: List[Tuple[str, str, int]]) -> Dict:
        """
        è¯„ä¼°æ¨¡å‹å‡†ç¡®ç‡
        
        Args:
            test_data: æµ‹è¯•æ•°æ®åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ä¸º (åŸå§‹æ–‡æœ¬, ç›®æ ‡æ–‡æœ¬, é£æ ¼ID)
        
        Returns:
            è¯„ä¼°ç»“æœå­—å…¸
        """
        if not self.model_loaded:
            if not self.load_model():
                return {'error': 'æ¨¡å‹æœªåŠ è½½'}
        
        print(f"\nğŸ“Š å¼€å§‹è¯„ä¼°æ¨¡å‹...")
        print(f"   æµ‹è¯•æ ·æœ¬æ•°: {len(test_data)}")
        
        results = {
            'total': len(test_data),
            'correct': 0,
            'partial_correct': 0,
            'failed': 0,
            'avg_length_ratio': 0.0,
            'style_accuracy': {},
        }
        
        length_ratios = []
        
        for i, (original, target, style) in enumerate(test_data):
            try:
                # ä½¿ç”¨æ¨¡å‹æ”¹å†™
                rewritten = self.rewriter.rewrite(original, style, temperature=0.7)
                
                # è®¡ç®—é•¿åº¦æ¯”ä¾‹
                if len(original) > 0:
                    length_ratio = len(rewritten) / len(original)
                    length_ratios.append(length_ratio)
                
                # ç®€å•çš„å‡†ç¡®ç‡è¯„ä¼°ï¼ˆåŸºäºå­—ç¬¦é‡å ï¼‰
                overlap = self._calculate_overlap(rewritten, target)
                
                if overlap > 0.8:
                    results['correct'] += 1
                elif overlap > 0.5:
                    results['partial_correct'] += 1
                else:
                    results['failed'] += 1
                
                # æŒ‰é£æ ¼ç»Ÿè®¡
                if style not in results['style_accuracy']:
                    results['style_accuracy'][style] = {'total': 0, 'correct': 0}
                results['style_accuracy'][style]['total'] += 1
                if overlap > 0.8:
                    results['style_accuracy'][style]['correct'] += 1
                
                if (i + 1) % 100 == 0:
                    print(f"   è¿›åº¦: {i+1}/{len(test_data)}")
            
            except Exception as e:
                results['failed'] += 1
                if results['failed'] <= 5:
                    print(f"   âš ï¸  è¯„ä¼°æ ·æœ¬ {i+1} æ—¶å‡ºé”™: {e}")
        
        # è®¡ç®—å¹³å‡é•¿åº¦æ¯”ä¾‹
        if length_ratios:
            results['avg_length_ratio'] = sum(length_ratios) / len(length_ratios)
        
        # è®¡ç®—æ€»ä½“å‡†ç¡®ç‡
        results['accuracy'] = results['correct'] / results['total'] if results['total'] > 0 else 0
        results['partial_accuracy'] = (results['correct'] + results['partial_correct']) / results['total'] if results['total'] > 0 else 0
        
        # è®¡ç®—å„é£æ ¼å‡†ç¡®ç‡
        for style in results['style_accuracy']:
            stats = results['style_accuracy'][style]
            stats['accuracy'] = stats['correct'] / stats['total'] if stats['total'] > 0 else 0
        
        return results
    
    def _calculate_overlap(self, text1: str, text2: str) -> float:
        """
        è®¡ç®—ä¸¤ä¸ªæ–‡æœ¬çš„é‡å åº¦
        
        Args:
            text1: æ–‡æœ¬1
            text2: æ–‡æœ¬2
        
        Returns:
            é‡å åº¦ï¼ˆ0-1ï¼‰
        """
        if not text1 or not text2:
            return 0.0
        
        # ä½¿ç”¨å­—ç¬¦çº§åˆ«çš„é‡å 
        chars1 = set(text1)
        chars2 = set(text2)
        
        if not chars1 or not chars2:
            return 0.0
        
        intersection = chars1 & chars2
        union = chars1 | chars2
        
        return len(intersection) / len(union) if union else 0.0
    
    def generate_report(self, results: Dict, output_file: str = None):
        """
        ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š
        
        Args:
            results: è¯„ä¼°ç»“æœ
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        print(f"\n{'='*60}")
        print("ğŸ“Š æ¨¡å‹è¯„ä¼°æŠ¥å‘Š")
        print(f"{'='*60}")
        print(f"\næ€»ä½“ç»Ÿè®¡:")
        print(f"  æ€»æ ·æœ¬æ•°: {results.get('total', 0)}")
        print(f"  å®Œå…¨æ­£ç¡®: {results.get('correct', 0)} ({results.get('accuracy', 0)*100:.1f}%)")
        print(f"  éƒ¨åˆ†æ­£ç¡®: {results.get('partial_correct', 0)}")
        print(f"  å¤±è´¥: {results.get('failed', 0)}")
        print(f"  æ€»ä½“å‡†ç¡®ç‡: {results.get('accuracy', 0)*100:.2f}%")
        print(f"  éƒ¨åˆ†å‡†ç¡®ç‡: {results.get('partial_accuracy', 0)*100:.2f}%")
        print(f"  å¹³å‡é•¿åº¦æ¯”ä¾‹: {results.get('avg_length_ratio', 0):.2f}")
        
        if results.get('style_accuracy'):
            print(f"\nå„é£æ ¼å‡†ç¡®ç‡:")
            for style, stats in sorted(results['style_accuracy'].items()):
                accuracy = stats.get('accuracy', 0)
                print(f"  é£æ ¼ {style}: {stats['correct']}/{stats['total']} ({accuracy*100:.1f}%)")
        
        if output_file:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            print(f"\nğŸ“„ æŠ¥å‘Šå·²ä¿å­˜: {output_file}")


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='æ¨¡å‹è¯„ä¼°å·¥å…·')
    parser.add_argument('test_data_file', help='æµ‹è¯•æ•°æ®æ–‡ä»¶ï¼ˆTSVæ ¼å¼ï¼‰')
    parser.add_argument('--model-path', default='models/text_rewriter_model',
                       help='æ¨¡å‹è·¯å¾„')
    parser.add_argument('--output', '-o', help='è¾“å‡ºæŠ¥å‘Šæ–‡ä»¶')
    
    args = parser.parse_args()
    
    # åŠ è½½æµ‹è¯•æ•°æ®
    test_data = []
    with open(args.test_data_file, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if not line or line.startswith('#'):
                continue
            parts = line.split('\t')
            if len(parts) >= 3:
                test_data.append((parts[0], parts[1], int(parts[2])))
    
    if not test_data:
        print("âŒ æ²¡æœ‰æµ‹è¯•æ•°æ®")
        return
    
    # è¯„ä¼°æ¨¡å‹
    evaluator = ModelEvaluator(args.model_path)
    results = evaluator.evaluate_accuracy(test_data)
    
    # ç”ŸæˆæŠ¥å‘Š
    evaluator.generate_report(results, args.output)


if __name__ == '__main__':
    main()

