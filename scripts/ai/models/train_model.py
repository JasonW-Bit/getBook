#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è®­ç»ƒTensorFlowæ–‡æœ¬æ”¹å†™æ¨¡å‹
"""

import os
import sys
import json
from typing import List, Tuple
import sys
import os
sys.path.insert(0, os.path.dirname(__file__))
from tensorflow_model import TensorFlowTextRewriter


def prepare_training_data(data_file: str) -> Tuple[List[str], List[str], List[int]]:
    """å‡†å¤‡è®­ç»ƒæ•°æ®"""
    print("ğŸ“š å‡†å¤‡è®­ç»ƒæ•°æ®...")
    
    original_texts = []
    rewritten_texts = []
    styles = []
    
    if os.path.exists(data_file):
        print(f"   ä»æ–‡ä»¶åŠ è½½: {data_file}")
        with open(data_file, 'r', encoding='utf-8') as f:
            line_count = 0
            error_count = 0
            for line_num, line in enumerate(f, 1):
                line = line.strip()
                if not line or line.startswith('#'):
                    continue
                
                # æ”¯æŒTSVæ ¼å¼ï¼šåŸå§‹æ–‡æœ¬<TAB>æ”¹å†™æ–‡æœ¬<TAB>é£æ ¼ID<TAB>ä¸Šä¸‹æ–‡JSONï¼ˆå¯é€‰ï¼‰
                parts = line.split('\t')
                if len(parts) >= 3:
                    try:
                        # æ•°æ®éªŒè¯ï¼šæ£€æŸ¥åŸå§‹æ–‡æœ¬å’Œæ”¹å†™æ–‡æœ¬é•¿åº¦
                        if len(parts[0].strip()) < 10 or len(parts[1].strip()) < 10:
                            error_count += 1
                            if error_count <= 5:
                                print(f"   âš ï¸  ç¬¬{line_num}è¡Œæ•°æ®è¿‡çŸ­ï¼Œè·³è¿‡")
                            continue
                        
                        # æ•°æ®éªŒè¯ï¼šæ£€æŸ¥é£æ ¼ID
                        style_id = int(parts[2])
                        if style_id < 0 or style_id > 20:
                            error_count += 1
                            if error_count <= 5:
                                print(f"   âš ï¸  ç¬¬{line_num}è¡Œé£æ ¼IDæ— æ•ˆ: {style_id}")
                            continue
                        
                        # è§£æä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                        context_info = {}
                        if len(parts) >= 4:
                            try:
                                context_info = json.loads(parts[3])
                            except json.JSONDecodeError:
                                pass  # ä¸Šä¸‹æ–‡è§£æå¤±è´¥ä¸å½±å“ä¸»æ•°æ®
                        
                        # æ•°æ®éªŒè¯é€šè¿‡ï¼Œæ·»åŠ åˆ°è®­ç»ƒæ•°æ®
                        orig = parts[0].strip()
                        rew = parts[1].strip()
                        style = int(parts[2].strip())
                        context = parts[3].strip() if len(parts) >= 4 else ""  # å¯é€‰çš„ä¸Šä¸‹æ–‡
                        
                        # éªŒè¯æ•°æ®ï¼ˆæé«˜æœ€å°é•¿åº¦è¦æ±‚ï¼‰
                        if orig and rew and len(orig) > 50 and len(rew) > 50:  # ä»10å¢åŠ åˆ°50
                            # å¦‚æœæœ‰ä¸Šä¸‹æ–‡ï¼Œå¯ä»¥åˆå¹¶åˆ°åŸå§‹æ–‡æœ¬ï¼ˆç”¨äºè®­ç»ƒï¼‰
                            if context:
                                # å°†ä¸Šä¸‹æ–‡ä¿¡æ¯æ·»åŠ åˆ°è®­ç»ƒæ•°æ®ä¸­
                                orig = f"[ä¸Šä¸‹æ–‡: {context[:200]}] {orig}"  # é™åˆ¶ä¸Šä¸‹æ–‡é•¿åº¦
                            
                            original_texts.append(orig)
                            rewritten_texts.append(rew)
                            styles.append(style)
                            line_count += 1
                        else:
                            error_count += 1
                            if error_count <= 5:  # åªæ˜¾ç¤ºå‰5ä¸ªé”™è¯¯
                                print(f"   âš ï¸  ç¬¬{line_num}è¡Œæ•°æ®å¤ªçŸ­ï¼Œè·³è¿‡ï¼ˆè¦æ±‚è‡³å°‘50å­—ç¬¦ï¼‰")
                    except (ValueError, IndexError) as e:
                        error_count += 1
                        if error_count <= 5:
                            print(f"   âš ï¸  ç¬¬{line_num}è¡Œæ ¼å¼é”™è¯¯ï¼Œè·³è¿‡: {e}")
                        continue
            
            if error_count > 5:
                print(f"   âš ï¸  è¿˜æœ‰ {error_count - 5} è¡Œæ•°æ®è¢«è·³è¿‡")
        
        print(f"   âœ… æˆåŠŸåŠ è½½ {line_count} æ¡æœ‰æ•ˆæ•°æ®")
    else:
        print(f"âš ï¸  è®­ç»ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_file}")
        print("   å°†ä½¿ç”¨ç¤ºä¾‹æ•°æ®ï¼ˆä»…ç”¨äºæµ‹è¯•ï¼‰")
        # ç¤ºä¾‹æ•°æ®
        examples = [
            ("é™ˆæ—­è¯´ï¼šå¥½çš„ï¼Œæˆ‘æ˜ç™½äº†ã€‚", "é™ˆæ—­åœ¨éƒ½å¸‚çš„å’–å•¡å…é‡Œï¼Œè½»æ¾åœ°ç¬‘ç€è¯´ï¼šå¥½çš„ï¼Œæˆ‘æ˜ç™½äº†ã€‚", 18),  # éƒ½å¸‚å¹½é»˜
            ("ä»–å¾ˆé«˜å…´ã€‚", "ä»–è¶…çº§é«˜å…´ã€‚", 6),  # å¹½é»˜
            ("å¥¹èµ°åœ¨è¡—ä¸Šã€‚", "å¥¹ç©¿æ¢­åœ¨éƒ½å¸‚çš„è¡—é“ä¸Šã€‚", 11),  # éƒ½å¸‚
            ("ä»Šå¤©å¤©æ°”å¾ˆå¥½ã€‚", "ä»Šå¤©å¤©æ°”è¶…çº§æ£’ã€‚", 6),  # å¹½é»˜
            ("ä»–åœ¨æ€è€ƒé—®é¢˜ã€‚", "ä»–åœ¨éƒ½å¸‚çš„å’–å•¡å…é‡Œæ€è€ƒé—®é¢˜ã€‚", 11),  # éƒ½å¸‚
        ]
        
        for orig, rew, style in examples:
            original_texts.append(orig)
            rewritten_texts.append(rew)
            styles.append(style)
        
        print(f"   ä½¿ç”¨ {len(examples)} æ¡ç¤ºä¾‹æ•°æ®")
    
    if len(original_texts) == 0:
        print("âŒ æ²¡æœ‰å¯ç”¨çš„è®­ç»ƒæ•°æ®")
        return [], [], []
    
    # æ•°æ®ç»Ÿè®¡
    style_counts = {}
    for style in styles:
        style_counts[style] = style_counts.get(style, 0) + 1
    
    print(f"\nğŸ“Š æ•°æ®ç»Ÿè®¡:")
    print(f"   æ€»æ•°æ®é‡: {len(original_texts)}")
    print(f"   é£æ ¼åˆ†å¸ƒ: {dict(sorted(style_counts.items()))}")
    
    return original_texts, rewritten_texts, styles


def main():
    """ä¸»å‡½æ•°"""
    if len(sys.argv) < 2:
        print("=" * 60)
        print("TensorFlowæ–‡æœ¬æ”¹å†™æ¨¡å‹è®­ç»ƒå·¥å…·")
        print("=" * 60)
        print("\nä½¿ç”¨æ–¹æ³•:")
        print("  python3 train_model.py <è®­ç»ƒæ•°æ®æ–‡ä»¶> [é€‰é¡¹]")
        print("\nå‚æ•°:")
        print("  è®­ç»ƒæ•°æ®æ–‡ä»¶: TSVæ ¼å¼ï¼Œæ¯è¡ŒåŒ…å« åŸå§‹æ–‡æœ¬<TAB>æ”¹å†™æ–‡æœ¬<TAB>é£æ ¼ID")
        print("\né€‰é¡¹:")
        print("  --model-path=è·¯å¾„     æ¨¡å‹ä¿å­˜è·¯å¾„ï¼ˆé»˜è®¤: models/text_rewriter_modelï¼‰")
        print("  --epochs=æ•°é‡         è®­ç»ƒè½®æ•°ï¼ˆé»˜è®¤: 20ï¼‰")
        print("  --batch-size=æ•°é‡     æ‰¹æ¬¡å¤§å°ï¼ˆé»˜è®¤: 16ï¼‰")
        print("  --validation-split=æ¯”ä¾‹  éªŒè¯é›†æ¯”ä¾‹ï¼ˆé»˜è®¤: 0.2ï¼‰")
        print("\nç¤ºä¾‹:")
        print("  python3 train_model.py data/training/training_data.txt")
        print("  python3 train_model.py data/training/training_data.txt --epochs=50 --batch-size=32")
        print("\næ•°æ®æ ¼å¼ç¤ºä¾‹:")
        print("  åŸå§‹æ–‡æœ¬\\tæ”¹å†™æ–‡æœ¬\\té£æ ¼ID")
        print("  é™ˆæ—­è¯´ï¼šå¥½çš„ã€‚\\té™ˆæ—­åœ¨éƒ½å¸‚çš„å’–å•¡å…é‡Œç¬‘ç€è¯´ï¼šå¥½çš„ã€‚\\t18")
        print("\nè¯¦ç»†è¯´æ˜è¯·æŸ¥çœ‹: data/training/README.md")
        sys.exit(1)
    
    data_file = sys.argv[1]
    model_path = "models/text_rewriter_model"
    epochs = 20
    batch_size = 16
    validation_split = 0.2
    
    # è§£æå‚æ•°
    for arg in sys.argv[2:]:
        if arg.startswith('--model-path='):
            model_path = arg.split('=')[1]
        elif arg.startswith('--epochs='):
            epochs = int(arg.split('=')[1])
        elif arg.startswith('--batch-size='):
            batch_size = int(arg.split('=')[1])
        elif arg.startswith('--validation-split='):
            validation_split = float(arg.split('=')[1])
    
    print("=" * 60)
    print("å¼€å§‹è®­ç»ƒTensorFlowæ–‡æœ¬æ”¹å†™æ¨¡å‹")
    print("=" * 60)
    print(f"\né…ç½®:")
    print(f"  è®­ç»ƒæ•°æ®: {data_file}")
    print(f"  æ¨¡å‹è·¯å¾„: {model_path}")
    print(f"  è®­ç»ƒè½®æ•°: {epochs}")
    print(f"  æ‰¹æ¬¡å¤§å°: {batch_size}")
    print(f"  éªŒè¯é›†æ¯”ä¾‹: {validation_split}")
    print()
    
    # å‡†å¤‡æ•°æ®
    original_texts, rewritten_texts, styles = prepare_training_data(data_file)
    
    if len(original_texts) == 0:
        print("\nâŒ æ²¡æœ‰è®­ç»ƒæ•°æ®ï¼Œé€€å‡º")
        sys.exit(1)
    
    # æ£€æŸ¥æ•°æ®é‡
    if len(original_texts) < 10:
        print("\nâš ï¸  è­¦å‘Š: è®­ç»ƒæ•°æ®é‡è¾ƒå°‘ï¼Œå»ºè®®è‡³å°‘100æ¡æ•°æ®ä»¥è·å¾—è¾ƒå¥½æ•ˆæœ")
        # éäº¤äº’æ¨¡å¼ä¸‹è‡ªåŠ¨ç»§ç»­
        if sys.stdin.isatty():
            response = input("æ˜¯å¦ç»§ç»­? (y/n): ")
            if response.lower() != 'y':
                sys.exit(0)
        else:
            print("   éäº¤äº’æ¨¡å¼ï¼Œè‡ªåŠ¨ç»§ç»­è®­ç»ƒ...")
    
    # åˆ›å»ºæ”¹å†™å™¨
    print("\n" + "=" * 60)
    rewriter = TensorFlowTextRewriter(model_path=model_path)
    
    # æ„å»ºè¯æ±‡è¡¨
    print("\nğŸ“š æ„å»ºè¯æ±‡è¡¨...")
    all_texts = original_texts + rewritten_texts
    rewriter.build_tokenizer(all_texts)
    
    # æ„å»ºæ¨¡å‹ï¼ˆå¯é…ç½®å‚æ•°ï¼‰
    print("\nğŸ—ï¸  æ„å»ºæ¨¡å‹...")
    # æ ¹æ®æ•°æ®é‡è°ƒæ•´æ¨¡å‹å¤æ‚åº¦å’Œå­¦ä¹ ç‡
    if len(original_texts) > 10000:
        num_layers = 4
        num_heads = 8
        learning_rate = 0.0008
    elif len(original_texts) > 1000:
        num_layers = 3
        num_heads = 6
        learning_rate = 0.001
    else:
        num_layers = 2
        num_heads = 4
        learning_rate = 0.002
    
    print(f"   æ¨¡å‹é…ç½®: {num_layers}å±‚, {num_heads}ä¸ªæ³¨æ„åŠ›å¤´")
    rewriter.build_model(num_layers=num_layers, num_heads=num_heads)
    
    # è®­ç»ƒæ¨¡å‹
    print("\nğŸš€ å¼€å§‹è®­ç»ƒ...")
    print("=" * 60)
    
    # å¦‚æœæä¾›äº†éªŒè¯æ•°æ®æ–‡ä»¶ï¼Œä½¿ç”¨å®ƒ
    validation_data_file = data_file.replace('training_data.txt', 'validation_data.txt')
    validation_texts = None
    validation_rewritten = None
    validation_styles = None
    
    if os.path.exists(validation_data_file):
        print(f"   å‘ç°éªŒè¯é›†æ–‡ä»¶: {validation_data_file}")
        validation_texts, validation_rewritten, validation_styles = prepare_training_data(validation_data_file)
        validation_split = 0.0  # ä½¿ç”¨ç‹¬ç«‹éªŒè¯é›†ï¼Œä¸ä½¿ç”¨åˆ†å‰²
    
    try:
        history = rewriter.train(
            original_texts=original_texts,
            rewritten_texts=rewritten_texts,
            styles=styles,
            epochs=epochs,
            batch_size=batch_size,
            validation_split=validation_split,
            validation_data=(validation_texts, validation_rewritten, validation_styles) if validation_texts else None,
            learning_rate=learning_rate
        )
        
        print("\n" + "=" * 60)
        print("âœ… æ¨¡å‹è®­ç»ƒå®Œæˆï¼")
        print("=" * 60)
        print(f"\nğŸ“ æ¨¡å‹æ–‡ä»¶:")
        print(f"   {model_path}/best_model.h5")
        print(f"   {model_path}/final_model.h5")
        print(f"   {model_path}/vocab.json")
        print(f"\nğŸ’¡ ä½¿ç”¨æ–¹æ³•:")
        print(f"   python3 scripts/creative/rewrite_novel.py novel.txt \\")
        print(f"     --use-ai --ai-type=tensorflow --style=éƒ½å¸‚å¹½é»˜")
        
    except KeyboardInterrupt:
        print("\n\nâš ï¸  è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
        print("   å·²ä¿å­˜çš„æ¨¡å‹æ–‡ä»¶å¯ä»¥ä½¿ç”¨")
        sys.exit(0)
    except Exception as e:
        print(f"\nâŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == '__main__':
    main()

