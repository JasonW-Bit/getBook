#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¨¡å‹ä¸‹è½½å™¨
ä»HuggingFaceç­‰å¹³å°ä¸‹è½½å’Œé›†æˆè¯­è¨€æ¨¡å‹
"""

import os
import sys
import json
from typing import Optional, Dict, List
from pathlib import Path


class ModelDownloader:
    """æ¨¡å‹ä¸‹è½½å™¨ - æ”¯æŒä»å¤šä¸ªå¹³å°ä¸‹è½½æ¨¡å‹"""
    
    # æ¨èçš„ä¸­æ–‡è¯­è¨€æ¨¡å‹åˆ—è¡¨
    RECOMMENDED_MODELS = {
        'chatglm3-6b': {
            'name': 'THUDM/chatglm3-6b',
            'type': 'huggingface',
            'size': '12GB',
            'description': 'ChatGLM3-6Bï¼Œæ¸…åå¤§å­¦å¼€æºçš„ä¸­æ–‡å¯¹è¯æ¨¡å‹',
            'suitable_for': ['å¯¹è¯', 'æ–‡æœ¬ç”Ÿæˆ', 'æ”¹å†™'],
            'min_memory': '16GB'
        },
        'qwen-7b-chat': {
            'name': 'Qwen/Qwen-7B-Chat',
            'type': 'huggingface',
            'size': '14GB',
            'description': 'é€šä¹‰åƒé—®7Bå¯¹è¯æ¨¡å‹ï¼Œé˜¿é‡Œäº‘å¼€æº',
            'suitable_for': ['å¯¹è¯', 'æ–‡æœ¬ç”Ÿæˆ', 'æ”¹å†™', 'åˆ›ä½œ'],
            'min_memory': '16GB'
        },
        'baichuan2-7b-chat': {
            'name': 'baichuan-inc/Baichuan2-7B-Chat',
            'type': 'huggingface',
            'size': '14GB',
            'description': 'ç™¾å·2-7Bå¯¹è¯æ¨¡å‹ï¼Œç™¾å·æ™ºèƒ½å¼€æº',
            'suitable_for': ['å¯¹è¯', 'æ–‡æœ¬ç”Ÿæˆ', 'æ”¹å†™'],
            'min_memory': '16GB'
        },
        'internlm-chat-7b': {
            'name': 'internlm/internlm-chat-7b',
            'type': 'huggingface',
            'size': '14GB',
            'description': 'ä¹¦ç”ŸÂ·æµ¦è¯­7Bå¯¹è¯æ¨¡å‹ï¼Œä¸Šæµ·AI Labå¼€æº',
            'suitable_for': ['å¯¹è¯', 'æ–‡æœ¬ç”Ÿæˆ', 'æ”¹å†™'],
            'min_memory': '16GB'
        },
        'qwen-1.8b-chat': {
            'name': 'Qwen/Qwen-1_8B-Chat',
            'type': 'huggingface',
            'size': '3.6GB',
            'description': 'é€šä¹‰åƒé—®1.8Bå¯¹è¯æ¨¡å‹ï¼ˆè½»é‡ç‰ˆï¼‰',
            'suitable_for': ['å¯¹è¯', 'æ–‡æœ¬ç”Ÿæˆ', 'æ”¹å†™'],
            'min_memory': '8GB'
        },
        'chatglm2-6b': {
            'name': 'THUDM/chatglm2-6b',
            'type': 'huggingface',
            'size': '12GB',
            'description': 'ChatGLM2-6Bï¼ŒChatGLMçš„å‡çº§ç‰ˆ',
            'suitable_for': ['å¯¹è¯', 'æ–‡æœ¬ç”Ÿæˆ', 'æ”¹å†™'],
            'min_memory': '16GB'
        }
    }
    
    def __init__(self, models_dir: str = "models/pretrained"):
        """
        åˆå§‹åŒ–æ¨¡å‹ä¸‹è½½å™¨
        
        Args:
            models_dir: æ¨¡å‹ä¿å­˜ç›®å½•
        """
        self.models_dir = models_dir
        os.makedirs(self.models_dir, exist_ok=True)
    
    def list_available_models(self) -> Dict:
        """åˆ—å‡ºå¯ç”¨çš„æ¨¡å‹"""
        return self.RECOMMENDED_MODELS
    
    def download_from_huggingface(self, model_name: str, local_dir: Optional[str] = None) -> bool:
        """
        ä»HuggingFaceä¸‹è½½æ¨¡å‹
        
        Args:
            model_name: æ¨¡å‹åç§°ï¼ˆå¦‚ 'THUDM/chatglm3-6b'ï¼‰
            local_dir: æœ¬åœ°ä¿å­˜ç›®å½•
        
        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        try:
            from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM
            from huggingface_hub import snapshot_download
        except ImportError:
            print("âŒ éœ€è¦å®‰è£… transformers å’Œ huggingface_hub")
            print("   è¿è¡Œ: pip install transformers huggingface_hub")
            return False
        
        if local_dir is None:
            # ä½¿ç”¨æ¨¡å‹åç§°ä½œä¸ºç›®å½•å
            safe_name = model_name.replace('/', '_')
            local_dir = os.path.join(self.models_dir, safe_name)
        
        print(f"\nğŸ“¥ å¼€å§‹ä¸‹è½½æ¨¡å‹: {model_name}")
        print(f"   ä¿å­˜åˆ°: {local_dir}")
        print(f"   è¿™å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´ï¼Œè¯·è€å¿ƒç­‰å¾…...")
        
        try:
            # ä¸‹è½½æ¨¡å‹
            snapshot_download(
                repo_id=model_name,
                local_dir=local_dir,
                local_dir_use_symlinks=False
            )
            
            print(f"\nâœ… æ¨¡å‹ä¸‹è½½å®Œæˆ: {local_dir}")
            return True
            
        except Exception as e:
            print(f"\nâŒ ä¸‹è½½å¤±è´¥: {e}")
            print(f"   æç¤º:")
            print(f"   1. æ£€æŸ¥ç½‘ç»œè¿æ¥")
            print(f"   2. ç¡®ä¿æœ‰è¶³å¤Ÿçš„ç£ç›˜ç©ºé—´")
            print(f"   3. å¦‚æœä½¿ç”¨HuggingFaceï¼Œå¯èƒ½éœ€è¦ç™»å½•: huggingface-cli login")
            return False
    
    def download_recommended_model(self, model_key: str) -> bool:
        """
        ä¸‹è½½æ¨èçš„æ¨¡å‹
        
        Args:
            model_key: æ¨¡å‹é”®åï¼ˆå¦‚ 'qwen-7b-chat'ï¼‰
        
        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        if model_key not in self.RECOMMENDED_MODELS:
            print(f"âŒ æœªçŸ¥çš„æ¨¡å‹: {model_key}")
            print(f"   å¯ç”¨æ¨¡å‹: {', '.join(self.RECOMMENDED_MODELS.keys())}")
            return False
        
        model_info = self.RECOMMENDED_MODELS[model_key]
        return self.download_from_huggingface(model_info['name'])
    
    def check_model_exists(self, model_name: str) -> bool:
        """æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²ä¸‹è½½"""
        safe_name = model_name.replace('/', '_')
        model_dir = os.path.join(self.models_dir, safe_name)
        return os.path.exists(model_dir) and os.path.isdir(model_dir)
    
    def get_model_info(self, model_key: str) -> Optional[Dict]:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        return self.RECOMMENDED_MODELS.get(model_key)
    
    def recommend_model(self, use_case: str = 'æ”¹å†™', memory_limit: Optional[int] = None) -> List[str]:
        """
        æ ¹æ®ä½¿ç”¨åœºæ™¯æ¨èæ¨¡å‹
        
        Args:
            use_case: ä½¿ç”¨åœºæ™¯ï¼ˆ'æ”¹å†™', 'ç”Ÿæˆ', 'å¯¹è¯'ç­‰ï¼‰
            memory_limit: å†…å­˜é™åˆ¶ï¼ˆGBï¼‰
        
        Returns:
            æ¨èçš„æ¨¡å‹åˆ—è¡¨
        """
        recommendations = []
        
        for key, info in self.RECOMMENDED_MODELS.items():
            # æ£€æŸ¥æ˜¯å¦é€‚åˆä½¿ç”¨åœºæ™¯
            if use_case in info.get('suitable_for', []):
                # æ£€æŸ¥å†…å­˜é™åˆ¶
                if memory_limit:
                    min_memory_str = info.get('min_memory', '16GB')
                    min_memory = int(min_memory_str.replace('GB', ''))
                    if memory_limit >= min_memory:
                        recommendations.append(key)
                else:
                    recommendations.append(key)
        
        return recommendations


def main():
    """å‘½ä»¤è¡Œå·¥å…·"""
    import argparse
    
    parser = argparse.ArgumentParser(description='æ¨¡å‹ä¸‹è½½å·¥å…·')
    parser.add_argument('action', choices=['list', 'download', 'recommend', 'check'],
                       help='æ“ä½œç±»å‹')
    parser.add_argument('--model', help='æ¨¡å‹åç§°æˆ–é”®å')
    parser.add_argument('--use-case', default='æ”¹å†™', help='ä½¿ç”¨åœºæ™¯')
    parser.add_argument('--memory', type=int, help='å†…å­˜é™åˆ¶ï¼ˆGBï¼‰')
    
    args = parser.parse_args()
    
    downloader = ModelDownloader()
    
    if args.action == 'list':
        print("\nğŸ“‹ å¯ç”¨çš„ä¸­æ–‡è¯­è¨€æ¨¡å‹:")
        print("=" * 60)
        for key, info in downloader.RECOMMENDED_MODELS.items():
            print(f"\n{key}:")
            print(f"  åç§°: {info['name']}")
            print(f"  å¤§å°: {info['size']}")
            print(f"  æè¿°: {info['description']}")
            print(f"  é€‚ç”¨: {', '.join(info['suitable_for'])}")
            print(f"  æœ€ä½å†…å­˜: {info['min_memory']}")
    
    elif args.action == 'recommend':
        recommendations = downloader.recommend_model(args.use_case, args.memory)
        if recommendations:
            print(f"\nğŸ’¡ æ¨èæ¨¡å‹ï¼ˆç”¨äº{args.use_case}ï¼‰:")
            for key in recommendations:
                info = downloader.get_model_info(key)
                print(f"  - {key}: {info['description']} ({info['size']})")
        else:
            print(f"\nâš ï¸  æœªæ‰¾åˆ°åˆé€‚çš„æ¨¡å‹")
    
    elif args.action == 'download':
        if not args.model:
            print("âŒ éœ€è¦æŒ‡å®š --model")
            return
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯æ¨èçš„æ¨¡å‹é”®å
        if args.model in downloader.RECOMMENDED_MODELS:
            downloader.download_recommended_model(args.model)
        else:
            # ç›´æ¥ä½¿ç”¨æ¨¡å‹åç§°
            downloader.download_from_huggingface(args.model)
    
    elif args.action == 'check':
        if not args.model:
            print("âŒ éœ€è¦æŒ‡å®š --model")
            return
        
        if args.model in downloader.RECOMMENDED_MODELS:
            model_name = downloader.RECOMMENDED_MODELS[args.model]['name']
        else:
            model_name = args.model
        
        exists = downloader.check_model_exists(model_name)
        if exists:
            safe_name = model_name.replace('/', '_')
            model_dir = os.path.join(downloader.models_dir, safe_name)
            print(f"âœ… æ¨¡å‹å·²ä¸‹è½½: {model_dir}")
        else:
            print(f"âŒ æ¨¡å‹æœªä¸‹è½½: {model_name}")


if __name__ == '__main__':
    main()

