#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åŸºäºTensorFlowçš„æœ¬åœ°è¯­è¨€æ¨¡å‹
ç”¨äºæ–‡æœ¬æ”¹å†™å’Œè¯­è¨€ä¼˜åŒ–
"""

import os
import re
import json
import numpy as np
from typing import Dict, List, Optional, Tuple
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, models, optimizers, callbacks


class TensorFlowTextRewriter:
    """åŸºäºTensorFlowçš„æ–‡æœ¬æ”¹å†™å™¨"""
    
    def __init__(self, model_path: Optional[str] = None, vocab_size: int = 10000, embedding_dim: int = 256):
        """
        åˆå§‹åŒ–TensorFlowæ–‡æœ¬æ”¹å†™å™¨
        
        Args:
            model_path: æ¨¡å‹ä¿å­˜è·¯å¾„
            vocab_size: è¯æ±‡è¡¨å¤§å°
            embedding_dim: è¯å‘é‡ç»´åº¦
        """
        self.model_path = model_path or "models/text_rewriter_model"
        self.vocab_size = vocab_size
        self.embedding_dim = embedding_dim
        self.model = None
        self.tokenizer = None
        self.vocab = {}
        self.reverse_vocab = {}
        self.max_length = 1024  # å¢åŠ æœ€å¤§é•¿åº¦ä»¥æ”¯æŒæ›´é•¿çš„ä¸Šä¸‹æ–‡
        
        # æ£€æŸ¥æ˜¯å¦æœ‰GPU
        self.use_gpu = len(tf.config.list_physical_devices('GPU')) > 0
        if self.use_gpu:
            print("âœ… æ£€æµ‹åˆ°GPUï¼Œå°†ä½¿ç”¨GPUåŠ é€Ÿ")
        else:
            print("âš ï¸  æœªæ£€æµ‹åˆ°GPUï¼Œå°†ä½¿ç”¨CPUï¼ˆé€Ÿåº¦è¾ƒæ…¢ï¼‰")
    
    def build_tokenizer(self, texts: List[str]):
        """æ„å»ºè¯æ±‡è¡¨å’Œåˆ†è¯å™¨"""
        print("ğŸ“š æ„å»ºè¯æ±‡è¡¨...")
        
        # ç®€å•çš„å­—ç¬¦çº§åˆ†è¯ï¼ˆä¸­æ–‡é€‚åˆå­—ç¬¦çº§ï¼‰
        all_chars = set()
        for text in texts:
            all_chars.update(text)
        
        # æ„å»ºè¯æ±‡è¡¨
        special_tokens = ['<PAD>', '<UNK>', '<START>', '<END>']
        vocab_list = special_tokens + sorted(list(all_chars))
        
        self.vocab = {char: idx for idx, char in enumerate(vocab_list)}
        self.reverse_vocab = {idx: char for char, idx in self.vocab.items()}
        self.vocab_size = len(self.vocab)
        
        print(f"âœ… è¯æ±‡è¡¨æ„å»ºå®Œæˆï¼Œå…± {self.vocab_size} ä¸ªå­—ç¬¦")
    
    def text_to_sequence(self, text: str, max_length: Optional[int] = None) -> List[int]:
        """å°†æ–‡æœ¬è½¬æ¢ä¸ºåºåˆ—"""
        if max_length is None:
            max_length = self.max_length
        
        sequence = [self.vocab.get('<START>', 1)]
        for char in text[:max_length-2]:
            sequence.append(self.vocab.get(char, self.vocab.get('<UNK>', 2)))
        sequence.append(self.vocab.get('<END>', 3))
        
        # å¡«å……æˆ–æˆªæ–­
        if len(sequence) < max_length:
            sequence.extend([self.vocab.get('<PAD>', 0)] * (max_length - len(sequence)))
        else:
            sequence = sequence[:max_length]
        
        return sequence
    
    def sequence_to_text(self, sequence: List[int]) -> str:
        """å°†åºåˆ—è½¬æ¢ä¸ºæ–‡æœ¬"""
        text = ""
        for idx in sequence:
            if idx == self.vocab.get('<PAD>', 0):
                continue
            if idx == self.vocab.get('<START>', 1):
                continue
            if idx == self.vocab.get('<END>', 3):
                break
            char = self.reverse_vocab.get(idx, '<UNK>')
            if char not in ['<PAD>', '<UNK>', '<START>', '<END>']:
                text += char
        return text
    
    def build_model(self, style_embedding_dim: int = 64, num_layers: int = 3, num_heads: int = 8):
        """
        æ„å»ºTransformeré£æ ¼çš„æ”¹å†™æ¨¡å‹
        
        Args:
            style_embedding_dim: é£æ ¼åµŒå…¥ç»´åº¦
            num_layers: Transformerå±‚æ•°
            num_heads: æ³¨æ„åŠ›å¤´æ•°
        """
        print("ğŸ—ï¸  æ„å»ºæ·±åº¦å­¦ä¹ æ¨¡å‹...")
        print(f"   é…ç½®: {num_layers}å±‚, {num_heads}ä¸ªæ³¨æ„åŠ›å¤´, åµŒå…¥ç»´åº¦{self.embedding_dim}")
        
        # è¾“å…¥å±‚
        input_text = layers.Input(shape=(self.max_length,), name='input_text')
        input_style = layers.Input(shape=(1,), name='input_style', dtype='int32')
        
        # æ–‡æœ¬åµŒå…¥ï¼ˆæ·»åŠ ä½ç½®ç¼–ç ï¼‰
        text_embedding = layers.Embedding(
            self.vocab_size, 
            self.embedding_dim,
            mask_zero=True,
            name='text_embedding'
        )(input_text)
        
        # ä½ç½®ç¼–ç ï¼ˆå¯å­¦ä¹ çš„ï¼‰
        position_encoding = layers.Embedding(
            self.max_length,
            self.embedding_dim,
            name='position_encoding'
        )(tf.range(self.max_length))
        text_embedding = text_embedding + position_encoding
        
        # é£æ ¼åµŒå…¥
        style_embedding = layers.Embedding(
            20,  # æ”¯æŒ20ç§é£æ ¼
            style_embedding_dim,
            name='style_embedding'
        )(input_style)
        # Embeddingè¾“å‡ºå½¢çŠ¶: (batch_size, 1, style_embedding_dim)
        # éœ€è¦å…ˆå±•å¹³ä¸º (batch_size, style_embedding_dim) æ‰èƒ½ç”¨RepeatVector
        style_embedding_flat = layers.Flatten()(style_embedding)  # å±•å¹³ä¸º (batch_size, style_embedding_dim)
        
        # æ‰©å±•é£æ ¼åµŒå…¥ä»¥åŒ¹é…æ–‡æœ¬é•¿åº¦
        # RepeatVectoréœ€è¦2ç»´è¾“å…¥ (batch_size, features)ï¼Œè¾“å‡º (batch_size, timesteps, features)
        style_embedding_expanded = layers.RepeatVector(self.max_length)(style_embedding_flat)
        # ç°åœ¨å½¢çŠ¶æ˜¯ (batch_size, max_length, style_embedding_dim)ï¼Œä¸éœ€è¦Reshapeäº†
        
        # èåˆæ–‡æœ¬å’Œé£æ ¼åµŒå…¥
        combined = layers.Concatenate()([text_embedding, style_embedding_expanded])
        
        # Transformerç¼–ç å™¨ï¼ˆå¯é…ç½®å±‚æ•°ï¼‰
        x = combined
        for i in range(num_layers):
            # å¤šå¤´æ³¨æ„åŠ›
            attention = layers.MultiHeadAttention(
                num_heads=num_heads,
                key_dim=self.embedding_dim + style_embedding_dim,
                name=f'attention_{i}'
            )(x, x)
            
            # æ®‹å·®è¿æ¥å’Œå±‚å½’ä¸€åŒ–
            x = layers.Add(name=f'add_{i}')([x, attention])
            x = layers.LayerNormalization(name=f'norm_{i}')(x)
            
            # å‰é¦ˆç½‘ç»œï¼ˆæ›´æ·±çš„ç½‘ç»œï¼‰
            ffn = layers.Dense((self.embedding_dim + style_embedding_dim) * 2, activation='relu', name=f'ffn_{i}_1')(x)
            ffn = layers.Dropout(0.1, name=f'dropout_{i}')(ffn)
            ffn = layers.Dense(self.embedding_dim + style_embedding_dim, name=f'ffn_{i}_2')(ffn)
            
            # æ®‹å·®è¿æ¥å’Œå±‚å½’ä¸€åŒ–
            x = layers.Add(name=f'add_ffn_{i}')([x, ffn])
            x = layers.LayerNormalization(name=f'norm_ffn_{i}')(x)
        
        # å…¨å±€æ± åŒ–ï¼ˆå¯é€‰ï¼‰
        # x = layers.GlobalAveragePooling1D()(x)
        
        # è¾“å‡ºå±‚
        output = layers.Dense(self.vocab_size, activation='softmax', name='output')(x)
        
        # æ„å»ºæ¨¡å‹
        self.model = models.Model(
            inputs=[input_text, input_style],
            outputs=output,
            name='text_rewriter'
        )
        
        # ç¼–è¯‘æ¨¡å‹ï¼ˆä½¿ç”¨æ›´ä¼˜åŒ–çš„å­¦ä¹ ç‡å’ŒæŒ‡æ ‡ï¼‰
        self.model.compile(
            optimizer=optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999),
            loss='sparse_categorical_crossentropy',
            metrics=[
                'accuracy', 
                'sparse_top_k_categorical_accuracy',
                'sparse_categorical_crossentropy'  # æ·»åŠ äº¤å‰ç†µä½œä¸ºæŒ‡æ ‡
            ]
        )
        
        print("âœ… æ¨¡å‹æ„å»ºå®Œæˆ")
        print(f"   æ¨¡å‹å‚æ•°æ•°é‡: {self.model.count_params():,}")
        
        # æ˜¾ç¤ºæ¨¡å‹ç»“æ„æ‘˜è¦
        if self.use_gpu:
            print("   GPUåŠ é€Ÿ: å·²å¯ç”¨")
    
    def prepare_training_data(self, original_texts: List[str], rewritten_texts: List[str], styles: List[int], 
                              augment: bool = False):
        """
        å‡†å¤‡è®­ç»ƒæ•°æ®
        
        Args:
            original_texts: åŸå§‹æ–‡æœ¬åˆ—è¡¨
            rewritten_texts: æ”¹å†™æ–‡æœ¬åˆ—è¡¨
            styles: é£æ ¼IDåˆ—è¡¨
            augment: æ˜¯å¦è¿›è¡Œæ•°æ®å¢å¼º
        """
        print("ğŸ“Š å‡†å¤‡è®­ç»ƒæ•°æ®...")
        
        X_text = []
        X_style = []
        y = []
        
        for orig, rew, style in zip(original_texts, rewritten_texts, styles):
            # æ•°æ®å¢å¼ºï¼ˆå¯é€‰ï¼‰
            if augment and len(orig) > 50:
                # è½»å¾®çš„æ•°æ®å¢å¼ºï¼šéšæœºæˆªå–æˆ–å¡«å……
                if len(orig) > self.max_length:
                    start = np.random.randint(0, len(orig) - self.max_length + 1)
                    orig = orig[start:start + self.max_length]
            
            orig_seq = self.text_to_sequence(orig)
            rew_seq = self.text_to_sequence(rew)
            
            X_text.append(orig_seq)
            X_style.append([style])
            
            # ä¸ºæ¯ä¸ªä½ç½®åˆ›å»ºæ ‡ç­¾ï¼ˆä¸‹ä¸€ä¸ªå­—ç¬¦ï¼‰
            y_seq = rew_seq[1:] + [self.vocab.get('<PAD>', 0)]
            y.append(y_seq)
        
        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        X_text = np.array(X_text)
        X_style = np.array(X_style)
        y = np.array(y)
        
        print(f"   æ•°æ®å½¢çŠ¶: X_text={X_text.shape}, X_style={X_style.shape}, y={y.shape}")
        
        return X_text, X_style, y
    
    def train(self, original_texts: List[str], rewritten_texts: List[str], styles: List[int], 
              epochs: int = 10, batch_size: int = 32, validation_split: float = 0.2,
              validation_data: Tuple[List[str], List[str], List[int]] = None,
              learning_rate: float = None):
        """
        è®­ç»ƒæ¨¡å‹ï¼ˆå¢å¼ºç‰ˆï¼Œæ”¯æŒè‡ªå®šä¹‰å­¦ä¹ ç‡ï¼‰
        
        Args:
            original_texts: åŸå§‹æ–‡æœ¬åˆ—è¡¨
            rewritten_texts: æ”¹å†™æ–‡æœ¬åˆ—è¡¨
            styles: é£æ ¼IDåˆ—è¡¨
            epochs: è®­ç»ƒè½®æ•°
            batch_size: æ‰¹æ¬¡å¤§å°
            validation_split: éªŒè¯é›†æ¯”ä¾‹ï¼ˆå¦‚æœæä¾›äº†validation_dataåˆ™å¿½ç•¥ï¼‰
            validation_data: ç‹¬ç«‹éªŒè¯é›† (åŸå§‹æ–‡æœ¬, æ”¹å†™æ–‡æœ¬, é£æ ¼ID)
            learning_rate: å­¦ä¹ ç‡ï¼ˆå¯é€‰ï¼Œå¦‚æœæä¾›ä¼šæ›´æ–°ä¼˜åŒ–å™¨ï¼‰
        """
        # å¦‚æœæä¾›äº†å­¦ä¹ ç‡ï¼Œæ›´æ–°ä¼˜åŒ–å™¨
        if learning_rate is not None:
            self.model.compile(
                optimizer=optimizers.Adam(learning_rate=learning_rate, beta_1=0.9, beta_2=0.999),
                loss='sparse_categorical_crossentropy',
                metrics=[
                    'accuracy', 
                    'sparse_top_k_categorical_accuracy',
                    'sparse_categorical_crossentropy'
                ]
            )
        print("ğŸš€ å¼€å§‹è®­ç»ƒæ¨¡å‹...")
        
        # å‡†å¤‡æ•°æ®
        X_text, X_style, y = self.prepare_training_data(original_texts, rewritten_texts, styles, augment=True)
        
        # å‡†å¤‡éªŒè¯æ•°æ®
        val_data = None
        if validation_data:
            val_orig, val_rew, val_styles = validation_data
            val_X_text, val_X_style, val_y = self.prepare_training_data(val_orig, val_rew, val_styles, augment=False)
            val_data = ([val_X_text, val_X_style], val_y)
            print(f"   ä½¿ç”¨ç‹¬ç«‹éªŒè¯é›†: {len(val_orig)} æ¡æ ·æœ¬")
        
        # åˆ›å»ºä¿å­˜ç›®å½•
        os.makedirs(self.model_path, exist_ok=True)
        
        # å›è°ƒå‡½æ•°ï¼ˆå¢å¼ºç‰ˆï¼‰
        callbacks_list = [
            callbacks.ModelCheckpoint(
                os.path.join(self.model_path, 'best_model.h5'),
                save_best_only=True,
                monitor='val_loss',
                verbose=1,
                save_weights_only=False
            ),
            callbacks.EarlyStopping(
                monitor='val_loss',
                patience=5,  # å¢åŠ è€å¿ƒå€¼
                restore_best_weights=True,
                verbose=1
            ),
            callbacks.ReduceLROnPlateau(
                monitor='val_loss',
                factor=0.5,
                patience=3,  # å¢åŠ è€å¿ƒå€¼
                min_lr=1e-6,
                verbose=1
            ),
            callbacks.TensorBoard(
                log_dir=os.path.join(self.model_path, 'logs'),
                histogram_freq=1,
                write_graph=True
            )
        ]
        
        # è®­ç»ƒ
        fit_kwargs = {
            'epochs': epochs,
            'batch_size': batch_size,
            'callbacks': callbacks_list,
            'verbose': 1
        }
        
        if val_data:
            fit_kwargs['validation_data'] = val_data
        else:
            fit_kwargs['validation_split'] = validation_split
        
        history = self.model.fit(
            [X_text, X_style],
            y,
            **fit_kwargs
        )
        
        # ä¿å­˜æ¨¡å‹
        self.model.save(os.path.join(self.model_path, 'final_model.h5'))
        # å¦‚æœbest_modelä¸å­˜åœ¨ï¼Œå¤åˆ¶final_model
        best_file = os.path.join(self.model_path, 'best_model.h5')
        if not os.path.exists(best_file):
            import shutil
            shutil.copy(os.path.join(self.model_path, 'final_model.h5'), best_file)
        self.save_vocab()
        
        print("âœ… æ¨¡å‹è®­ç»ƒå®Œæˆ")
        return history
    
    def save_vocab(self):
        """ä¿å­˜è¯æ±‡è¡¨"""
        vocab_file = os.path.join(self.model_path, 'vocab.json')
        with open(vocab_file, 'w', encoding='utf-8') as f:
            json.dump({
                'vocab': self.vocab,
                'reverse_vocab': {str(k): v for k, v in self.reverse_vocab.items()},
                'vocab_size': self.vocab_size,
                'max_length': self.max_length
            }, f, ensure_ascii=False, indent=2)
    
    def load_vocab(self):
        """åŠ è½½è¯æ±‡è¡¨"""
        vocab_file = os.path.join(self.model_path, 'vocab.json')
        if os.path.exists(vocab_file):
            with open(vocab_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                self.vocab = data['vocab']
                self.reverse_vocab = {int(k): v for k, v in data['reverse_vocab'].items()}
                self.vocab_size = data['vocab_size']
                self.max_length = data.get('max_length', 1024)  # æ”¯æŒæ›´é•¿çš„ä¸Šä¸‹æ–‡
            print("âœ… è¯æ±‡è¡¨åŠ è½½å®Œæˆ")
            return True
        return False
    
    def load_model(self):
        """åŠ è½½æ¨¡å‹"""
        model_file = os.path.join(self.model_path, 'best_model.h5')
        if not os.path.exists(model_file):
            model_file = os.path.join(self.model_path, 'final_model.h5')
        
        if os.path.exists(model_file):
            if not self.model:
                self.build_model()
            self.model.load_weights(model_file)
            print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
            return True
        return False
    
    def rewrite(self, text: str, style: int = 0, temperature: float = 0.7, max_length: Optional[int] = None) -> str:
        """
        ä½¿ç”¨æ¨¡å‹æ”¹å†™æ–‡æœ¬
        
        Args:
            text: åŸå§‹æ–‡æœ¬
            style: é£æ ¼ID
            temperature: é‡‡æ ·æ¸©åº¦ï¼ˆ0.1-1.0ï¼Œè¶Šå°è¶Šç¡®å®šï¼Œè¶Šå¤§è¶Šéšæœºï¼‰
            max_length: æœ€å¤§ç”Ÿæˆé•¿åº¦
        
        Returns:
            æ”¹å†™åçš„æ–‡æœ¬
        """
        if not self.model:
            if not self.load_model():
                raise ValueError("æ¨¡å‹æœªè®­ç»ƒæˆ–æœªåŠ è½½ï¼Œè¯·å…ˆè®­ç»ƒæ¨¡å‹")
        
        if max_length is None:
            max_length = min(len(text) * 2, self.max_length)
        
        # å‡†å¤‡è¾“å…¥
        input_seq = self.text_to_sequence(text)
        input_text = np.array([input_seq])
        input_style = np.array([[style]])
        
        # ç”Ÿæˆæ”¹å†™æ–‡æœ¬ï¼ˆä½¿ç”¨è´ªå¿ƒè§£ç æˆ–é‡‡æ ·ï¼‰
        output_seq = []
        current_seq = input_seq.copy()
        
        try:
            for step in range(max_length):
                # é¢„æµ‹ä¸‹ä¸€ä¸ªå­—ç¬¦
                predictions = self.model.predict([input_text, input_style], verbose=0, batch_size=1)
                
                if step >= predictions.shape[1]:
                    break
                
                next_char_probs = predictions[0, step, :]
                
                # åº”ç”¨æ”¹è¿›çš„é‡‡æ ·ç­–ç•¥ï¼ˆTop-k + Nucleusé‡‡æ ·ï¼‰
                if temperature > 0:
                    # Top-ké‡‡æ ·ï¼ˆé€‰æ‹©æ¦‚ç‡æœ€é«˜çš„kä¸ªå­—ç¬¦ï¼‰
                    top_k = 50
                    top_k_indices = np.argsort(next_char_probs)[-top_k:]
                    top_k_probs = next_char_probs[top_k_indices]
                    
                    # åº”ç”¨æ¸©åº¦
                    top_k_probs = np.log(top_k_probs + 1e-10) / temperature
                    top_k_probs = np.exp(top_k_probs)
                    top_k_probs = top_k_probs / np.sum(top_k_probs)
                    
                    # Nucleusé‡‡æ ·ï¼ˆå¯é€‰ï¼Œç´¯ç§¯æ¦‚ç‡é˜ˆå€¼ï¼‰
                    nucleus_threshold = 0.9
                    if nucleus_threshold < 1.0:
                        sorted_probs = np.sort(top_k_probs)[::-1]
                        cumsum_probs = np.cumsum(sorted_probs)
                        cutoff = np.searchsorted(cumsum_probs, nucleus_threshold)
                        if cutoff < len(top_k_probs):
                            top_k_probs = top_k_probs[:cutoff+1]
                            top_k_probs = top_k_probs / np.sum(top_k_probs)
                            top_k_indices = top_k_indices[:cutoff+1]
                    
                    next_char_idx = np.random.choice(top_k_indices, p=top_k_probs)
                else:
                    # è´ªå¿ƒè§£ç 
                    next_char_idx = np.argmax(next_char_probs)
                
                # æ£€æŸ¥ç»“æŸæ ‡è®°
                if next_char_idx == self.vocab.get('<END>', 3):
                    break
                
                # è·³è¿‡å¡«å……æ ‡è®°
                if next_char_idx != self.vocab.get('<PAD>', 0):
                    output_seq.append(int(next_char_idx))
                    
                    # æ›´æ–°è¾“å…¥åºåˆ—ï¼ˆç”¨äºä¸‹ä¸€æ­¥é¢„æµ‹ï¼‰
                    if step + 1 < len(current_seq):
                        current_seq[step + 1] = int(next_char_idx)
                    else:
                        # å¦‚æœè¶…å‡ºå½“å‰åºåˆ—é•¿åº¦ï¼Œéœ€è¦æ‰©å±•
                        current_seq = np.append(current_seq, int(next_char_idx))
                        if len(current_seq) > self.max_length:
                            current_seq = current_seq[:self.max_length]
                    
                    input_text = np.array([current_seq])
        
        except Exception as e:
            print(f"âš ï¸  ç”Ÿæˆè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
            # å¦‚æœç”Ÿæˆå¤±è´¥ï¼Œè¿”å›åŸå§‹æ–‡æœ¬çš„ç®€å•å¤„ç†
            return text
        
        # è½¬æ¢ä¸ºæ–‡æœ¬
        result = self.sequence_to_text(output_seq)
        
        # å¦‚æœç»“æœä¸ºç©ºæˆ–å¤ªçŸ­ï¼Œè¿”å›åŸå§‹æ–‡æœ¬
        if not result or len(result) < len(text) * 0.3:
            return text
        
        return result


class TensorFlowAnalyzer:
    """TensorFlowåˆ†æå™¨ï¼ˆé›†æˆåˆ°AIåˆ†æå™¨ï¼‰"""
    
    def __init__(self, model_path: Optional[str] = None):
        self.model_path = model_path or "models/text_rewriter_model"
        self.rewriter = TensorFlowTextRewriter(model_path=model_path)
        self.style_map = {
            'ç°ä»£': 0, 'å¤å…¸': 1, 'ç®€æ´': 2, 'åä¸½': 3,
            'æ‚¬ç–‘': 4, 'æµªæ¼«': 5, 'å¹½é»˜': 6, 'ä¸¥è‚ƒ': 7,
            'ç§‘å¹»': 8, 'æ­¦ä¾ ': 9, 'é’æ˜¥': 10, 'éƒ½å¸‚': 11,
            'å¤é£': 12, 'è¯—åŒ–': 13, 'å£è¯­': 14, 'æ­£å¼': 15,
            'ç½‘ç»œ': 16, 'æ–‡è‰º': 17, 'éƒ½å¸‚å¹½é»˜': 18
        }
        self.model_loaded = False
    
    def load_model(self) -> bool:
        """åŠ è½½æ¨¡å‹"""
        if self.rewriter.load_vocab() and self.rewriter.load_model():
            self.model_loaded = True
            return True
        return False
    
    def analyze_characters(self, content: str) -> Dict[str, Dict]:
        """åˆ†æäººç‰©ï¼ˆç®€åŒ–ç‰ˆï¼Œä½¿ç”¨è§„åˆ™ï¼‰"""
        # è¿™é‡Œå¯ä»¥ä½¿ç”¨ç®€å•çš„è§„åˆ™æˆ–è®­ç»ƒä¸€ä¸ªåˆ†ç±»æ¨¡å‹
        return {}
    
    def analyze_storyline(self, content: str) -> Dict:
        """åˆ†ææ•…äº‹è„‰ç»œï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        return {}
    
    def analyze_plot(self, content: str) -> Dict:
        """åˆ†ææƒ…èŠ‚ç»“æ„"""
        return {}
    
    def rewrite_text(self, text: str, style: str, perspective: Optional[str] = None, context: Optional[str] = None) -> str:
        """ä½¿ç”¨TensorFlowæ¨¡å‹æ”¹å†™æ–‡æœ¬"""
        if not self.model_loaded:
            if not self.load_model():
                print("âš ï¸  æ¨¡å‹æœªåŠ è½½ï¼Œä½¿ç”¨ç®€å•è§„åˆ™æ”¹å†™")
                return text
        
        # è·å–é£æ ¼ID
        style_id = self.style_map.get(style, 0)
        
        # åˆ†æ®µå¤„ç†
        if len(text) > 500:
            # åˆ†æ®µå¤„ç†é•¿æ–‡æœ¬
            chunks = []
            chunk_size = 500
            for i in range(0, len(text), chunk_size):
                chunk = text[i:i+chunk_size]
                rewritten_chunk = self.rewriter.rewrite(chunk, style_id)
                chunks.append(rewritten_chunk)
            return ''.join(chunks)
        else:
            return self.rewriter.rewrite(text, style_id)

