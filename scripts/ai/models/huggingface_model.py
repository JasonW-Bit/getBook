#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
HuggingFaceæ¨¡å‹é›†æˆ
å°†ä¸‹è½½çš„HuggingFaceæ¨¡å‹é›†æˆåˆ°é¡¹ç›®ä¸­ï¼Œç”¨äºæ–‡æœ¬æ”¹å†™
"""

import os
import sys
from typing import Optional, Dict, List
import torch


class HuggingFaceTextRewriter:
    """åŸºäºHuggingFaceæ¨¡å‹çš„æ–‡æœ¬æ”¹å†™å™¨"""
    
    def __init__(self, model_path: str, model_type: str = 'auto'):
        """
        åˆå§‹åŒ–HuggingFaceæ–‡æœ¬æ”¹å†™å™¨
        
        Args:
            model_path: æ¨¡å‹è·¯å¾„ï¼ˆæœ¬åœ°è·¯å¾„æˆ–HuggingFaceæ¨¡å‹IDï¼‰
            model_type: æ¨¡å‹ç±»å‹ï¼ˆ'auto', 'chatglm', 'qwen', 'baichuan'ç­‰ï¼‰
        """
        self.model_path = model_path
        self.model_type = model_type
        self.model = None
        self.tokenizer = None
        self.device = self._get_device()
        
        # åŠ è½½æ¨¡å‹
        self._load_model()
    
    def _get_device(self) -> str:
        """è·å–å¯ç”¨è®¾å¤‡"""
        if torch.cuda.is_available():
            return 'cuda'
        elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
            return 'mps'  # Apple Silicon
        else:
            return 'cpu'
    
    def _load_model(self):
        """åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨"""
        try:
            from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM
            
            print(f"ğŸ“¥ åŠ è½½æ¨¡å‹: {self.model_path}")
            print(f"   è®¾å¤‡: {self.device}")
            
            # æ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©åŠ è½½æ–¹å¼
            if 'chatglm' in self.model_path.lower() or 'chatglm' in self.model_type.lower():
                self._load_chatglm()
            elif 'qwen' in self.model_path.lower() or 'qwen' in self.model_type.lower():
                self._load_qwen()
            elif 'baichuan' in self.model_path.lower() or 'baichuan' in self.model_type.lower():
                self._load_baichuan()
            else:
                # è‡ªåŠ¨æ£€æµ‹
                self._load_auto()
            
            print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
            
        except ImportError:
            print("âŒ éœ€è¦å®‰è£… transformers å’Œ torch")
            print("   è¿è¡Œ: pip install transformers torch")
            raise
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise
    
    def _load_chatglm(self):
        """åŠ è½½ChatGLMæ¨¡å‹"""
        from transformers import AutoTokenizer, AutoModel
        
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.model_path,
            trust_remote_code=True
        )
        self.model = AutoModel.from_pretrained(
            self.model_path,
            trust_remote_code=True,
            device_map='auto' if self.device != 'cpu' else None
        )
        
        if self.device == 'cpu':
            self.model = self.model.float()
        else:
            self.model = self.model.half()
        
        self.model.eval()
    
    def _load_qwen(self):
        """åŠ è½½Qwenæ¨¡å‹"""
        from transformers import AutoTokenizer, AutoModelForCausalLM
        
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.model_path,
            trust_remote_code=True
        )
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_path,
            trust_remote_code=True,
            device_map='auto' if self.device != 'cpu' else None,
            torch_dtype=torch.float16 if self.device != 'cpu' else torch.float32
        )
        self.model.eval()
    
    def _load_baichuan(self):
        """åŠ è½½Baichuanæ¨¡å‹"""
        from transformers import AutoTokenizer, AutoModelForCausalLM
        
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.model_path,
            trust_remote_code=True
        )
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_path,
            trust_remote_code=True,
            device_map='auto' if self.device != 'cpu' else None,
            torch_dtype=torch.float16 if self.device != 'cpu' else torch.float32
        )
        self.model.eval()
    
    def _load_auto(self):
        """è‡ªåŠ¨åŠ è½½æ¨¡å‹"""
        from transformers import AutoTokenizer, AutoModelForCausalLM
        
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_path,
                trust_remote_code=True
            )
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_path,
                trust_remote_code=True,
                device_map='auto' if self.device != 'cpu' else None,
                torch_dtype=torch.float16 if self.device != 'cpu' else torch.float32
            )
            self.model.eval()
        except:
            # å°è¯•åŠ è½½ä¸ºAutoModel
            from transformers import AutoModel
            self.model = AutoModel.from_pretrained(
                self.model_path,
                trust_remote_code=True,
                device_map='auto' if self.device != 'cpu' else None
            )
            if self.device == 'cpu':
                self.model = self.model.float()
            else:
                self.model = self.model.half()
            self.model.eval()
    
    def rewrite(self, text: str, style: Optional[str] = None, 
                context: Optional[str] = None, max_length: int = 512) -> str:
        """
        æ”¹å†™æ–‡æœ¬
        
        Args:
            text: åŸå§‹æ–‡æœ¬
            style: é£æ ¼ï¼ˆå¯é€‰ï¼‰
            context: ä¸Šä¸‹æ–‡ï¼ˆå¯é€‰ï¼‰
            max_length: æœ€å¤§é•¿åº¦
        
        Returns:
            æ”¹å†™åçš„æ–‡æœ¬
        """
        # æ„å»ºæç¤ºè¯
        prompt = self._build_prompt(text, style, context)
        
        # æ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©ç”Ÿæˆæ–¹å¼
        if 'chatglm' in self.model_type.lower() or hasattr(self.model, 'chat'):
            return self._generate_chatglm(prompt, max_length)
        else:
            return self._generate_standard(prompt, max_length)
    
    def _build_prompt(self, text: str, style: Optional[str] = None, 
                     context: Optional[str] = None) -> str:
        """æ„å»ºæç¤ºè¯"""
        prompt_parts = []
        
        if context:
            prompt_parts.append(f"ä¸Šä¸‹æ–‡ï¼š{context}")
        
        if style:
            prompt_parts.append(f"é£æ ¼ï¼š{style}")
        
        prompt_parts.append(f"è¯·æ”¹å†™ä»¥ä¸‹æ–‡æœ¬ï¼Œä¿æŒåŸæ„ä½†ä½¿è¡¨è¾¾æ›´åŠ ç”ŸåŠ¨è‡ªç„¶ï¼š")
        prompt_parts.append(text)
        
        return "\n".join(prompt_parts)
    
    def _generate_chatglm(self, prompt: str, max_length: int) -> str:
        """ä½¿ç”¨ChatGLMç”Ÿæˆ"""
        if hasattr(self.model, 'chat'):
            response, _ = self.model.chat(
                self.tokenizer,
                prompt,
                history=[],
                max_length=max_length,
                temperature=0.7
            )
            return response
        else:
            return self._generate_standard(prompt, max_length)
    
    def _generate_standard(self, prompt: str, max_length: int) -> str:
        """æ ‡å‡†ç”Ÿæˆæ–¹å¼"""
        inputs = self.tokenizer(prompt, return_tensors="pt", padding=True, truncation=True)
        
        if self.device != 'cpu':
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_length=max_length,
                temperature=0.7,
                top_p=0.9,
                do_sample=True,
                pad_token_id=self.tokenizer.eos_token_id
            )
        
        generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # æå–ç”Ÿæˆçš„éƒ¨åˆ†ï¼ˆå»é™¤æç¤ºè¯ï¼‰
        if prompt in generated_text:
            generated_text = generated_text.replace(prompt, "").strip()
        
        return generated_text
    
    def analyze(self, text: str) -> Dict:
        """åˆ†ææ–‡æœ¬"""
        # å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ æ–‡æœ¬åˆ†æåŠŸèƒ½
        return {}


def main():
    """æµ‹è¯•å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='HuggingFaceæ¨¡å‹æµ‹è¯•')
    parser.add_argument('--model-path', required=True, help='æ¨¡å‹è·¯å¾„')
    parser.add_argument('--text', help='è¦æ”¹å†™çš„æ–‡æœ¬')
    parser.add_argument('--style', help='é£æ ¼')
    
    args = parser.parse_args()
    
    rewriter = HuggingFaceTextRewriter(args.model_path)
    
    if args.text:
        result = rewriter.rewrite(args.text, style=args.style)
        print(f"\næ”¹å†™ç»“æœ:\n{result}")
    else:
        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼Œå¯ä»¥ä½¿ç”¨ rewrite() æ–¹æ³•è¿›è¡Œæ”¹å†™")


if __name__ == '__main__':
    main()

