#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¢é‡è®­ç»ƒè„šæœ¬
åœ¨å·²æœ‰æ¨¡å‹åŸºç¡€ä¸Šç»§ç»­è®­ç»ƒï¼Œå®Œå–„æ¨¡å‹æ•°æ®
"""

import os
import sys
import json
import argparse
from typing import List, Tuple

sys.path.insert(0, os.path.dirname(__file__))
from tensorflow_model import TensorFlowTextRewriter
from train_model import prepare_training_data


class IncrementalTrainer:
    """å¢é‡è®­ç»ƒå™¨"""
    
    def __init__(self, model_path: str = "models/text_rewriter_model"):
        """
        åˆå§‹åŒ–å¢é‡è®­ç»ƒå™¨
        
        Args:
            model_path: æ¨¡å‹è·¯å¾„
        """
        self.model_path = model_path
        self.rewriter = TensorFlowTextRewriter(model_path=model_path)
    
    def load_existing_model(self) -> bool:
        """åŠ è½½å·²æœ‰æ¨¡å‹"""
        print(f"\nğŸ“‚ åŠ è½½å·²æœ‰æ¨¡å‹: {self.model_path}")
        
        if not self.rewriter.load_vocab():
            print("âŒ æ— æ³•åŠ è½½è¯æ±‡è¡¨ï¼Œè¯·å…ˆè®­ç»ƒåŸºç¡€æ¨¡å‹")
            return False
        
        if not self.rewriter.load_model():
            print("âŒ æ— æ³•åŠ è½½æ¨¡å‹ï¼Œè¯·å…ˆè®­ç»ƒåŸºç¡€æ¨¡å‹")
            return False
        
        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
        print(f"   è¯æ±‡è¡¨å¤§å°: {self.rewriter.vocab_size}")
        print(f"   æœ€å¤§é•¿åº¦: {self.rewriter.max_length}")
        
        return True
    
    def merge_vocab(self, new_texts: List[str]) -> bool:
        """
        åˆå¹¶æ–°æ•°æ®çš„è¯æ±‡è¡¨
        
        Args:
            new_texts: æ–°æ–‡æœ¬åˆ—è¡¨
        
        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        print(f"\nğŸ“š åˆå¹¶è¯æ±‡è¡¨...")
        
        # æ”¶é›†æ–°è¯æ±‡
        new_chars = set()
        for text in new_texts:
            new_chars.update(text)
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ–°å­—ç¬¦
        existing_chars = set(self.rewriter.vocab.keys())
        new_chars = new_chars - existing_chars
        
        if not new_chars:
            print("âœ… æ²¡æœ‰æ–°è¯æ±‡éœ€è¦æ·»åŠ ")
            return True
        
        print(f"   å‘ç° {len(new_chars)} ä¸ªæ–°å­—ç¬¦")
        
        # æ·»åŠ æ–°å­—ç¬¦åˆ°è¯æ±‡è¡¨
        current_size = len(self.rewriter.vocab)
        for char in new_chars:
            if char not in self.rewriter.vocab:
                idx = current_size
                self.rewriter.vocab[char] = idx
                self.rewriter.reverse_vocab[idx] = char
                current_size += 1
        
        self.rewriter.vocab_size = current_size
        
        print(f"   è¯æ±‡è¡¨å·²æ›´æ–°: {len(existing_chars)} â†’ {current_size}")
        
        # éœ€è¦é‡æ–°æ„å»ºæ¨¡å‹ï¼ˆå› ä¸ºè¯æ±‡è¡¨å¤§å°å˜åŒ–ï¼‰
        print("   é‡æ–°æ„å»ºæ¨¡å‹ä»¥é€‚åº”æ–°è¯æ±‡è¡¨...")
        self.rewriter.build_model()
        
        # åŠ è½½ä¹‹å‰çš„æƒé‡ï¼ˆå¦‚æœå¯èƒ½ï¼‰
        try:
            old_weights_file = os.path.join(self.model_path, 'best_model.h5')
            if os.path.exists(old_weights_file):
                # å°è¯•åŠ è½½å…¼å®¹çš„æƒé‡
                print("   å°è¯•åŠ è½½ä¹‹å‰çš„æ¨¡å‹æƒé‡...")
                # æ³¨æ„ï¼šå¦‚æœè¯æ±‡è¡¨å¤§å°å˜åŒ–ï¼Œå¯èƒ½éœ€è¦ç‰¹æ®Šå¤„ç†
                try:
                    # å°è¯•åŠ è½½æƒé‡ï¼ˆå¦‚æœç»“æ„å…¼å®¹ï¼‰
                    self.rewriter.model.load_weights(old_weights_file, by_name=True, skip_mismatch=True)
                    print("   âœ… æˆåŠŸåŠ è½½éƒ¨åˆ†æƒé‡")
                except Exception as e:
                    print(f"   âš ï¸  æ— æ³•åŠ è½½ä¹‹å‰çš„æƒé‡: {e}")
                    print("   å°†ä»å¤´è®­ç»ƒ")
        except Exception as e:
            print(f"   âš ï¸  æ— æ³•åŠ è½½ä¹‹å‰çš„æƒé‡: {e}")
            print("   å°†ä»å¤´è®­ç»ƒ")
        
        return True
    
    def incremental_train(self, new_data_file: str, 
                         epochs: int = 10,
                         batch_size: int = 16,
                         learning_rate: float = 0.0001) -> bool:
        """
        å¢é‡è®­ç»ƒ
        
        Args:
            new_data_file: æ–°è®­ç»ƒæ•°æ®æ–‡ä»¶
            epochs: è®­ç»ƒè½®æ•°
            batch_size: æ‰¹æ¬¡å¤§å°
            learning_rate: å­¦ä¹ ç‡ï¼ˆå¢é‡è®­ç»ƒä½¿ç”¨è¾ƒå°çš„å­¦ä¹ ç‡ï¼‰
        
        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        print(f"\nğŸš€ å¼€å§‹å¢é‡è®­ç»ƒ...")
        print(f"   æ–°æ•°æ®æ–‡ä»¶: {new_data_file}")
        print(f"   è®­ç»ƒè½®æ•°: {epochs}")
        print(f"   æ‰¹æ¬¡å¤§å°: {batch_size}")
        print(f"   å­¦ä¹ ç‡: {learning_rate}")
        
        # å‡†å¤‡æ–°æ•°æ®
        original_texts, rewritten_texts, styles = prepare_training_data(new_data_file)
        
        if len(original_texts) == 0:
            print("âŒ æ²¡æœ‰å¯ç”¨çš„æ–°è®­ç»ƒæ•°æ®")
            return False
        
        print(f"\nğŸ“Š æ–°æ•°æ®ç»Ÿè®¡:")
        print(f"   æ ·æœ¬æ•°: {len(original_texts)}")
        
        # åˆå¹¶è¯æ±‡è¡¨
        all_texts = original_texts + rewritten_texts
        self.merge_vocab(all_texts)
        
        # å‡†å¤‡è®­ç»ƒæ•°æ®
        X_text, X_style, y = self.rewriter.prepare_training_data(
            original_texts, rewritten_texts, styles
        )
        
        # è°ƒæ•´å­¦ä¹ ç‡
        self.rewriter.model.compile(
            optimizer=self.rewriter.model.optimizer.__class__(learning_rate=learning_rate),
            loss='sparse_categorical_crossentropy',
            metrics=['accuracy']
        )
        
        # å¢é‡è®­ç»ƒ
        print(f"\nğŸ¯ å¼€å§‹è®­ç»ƒï¼ˆå¢é‡æ¨¡å¼ï¼‰...")
        history = self.rewriter.model.fit(
            [X_text, X_style],
            y,
            epochs=epochs,
            batch_size=batch_size,
            validation_split=0.2,
            verbose=1
        )
        
        # ä¿å­˜æ¨¡å‹
        self.rewriter.model.save(os.path.join(self.model_path, 'incremental_model.h5'))
        self.rewriter.save_vocab()
        
        print(f"\nâœ… å¢é‡è®­ç»ƒå®Œæˆï¼")
        print(f"   æ¨¡å‹å·²ä¿å­˜: {self.model_path}/incremental_model.h5")
        
        return True
    
    def merge_models(self, keep_best: bool = True):
        """
        åˆå¹¶æ¨¡å‹ï¼ˆå°†å¢é‡è®­ç»ƒçš„æ¨¡å‹ä¸åŸºç¡€æ¨¡å‹åˆå¹¶ï¼‰
        
        Args:
            keep_best: æ˜¯å¦ä¿ç•™æœ€ä½³æ¨¡å‹
        """
        incremental_file = os.path.join(self.model_path, 'incremental_model.h5')
        best_file = os.path.join(self.model_path, 'best_model.h5')
        final_file = os.path.join(self.model_path, 'final_model.h5')
        
        if os.path.exists(incremental_file):
            if keep_best and os.path.exists(best_file):
                # æ¯”è¾ƒæ¨¡å‹æ€§èƒ½ï¼Œä¿ç•™æ›´å¥½çš„
                print("ğŸ“Š æ¯”è¾ƒæ¨¡å‹æ€§èƒ½...")
                # è¿™é‡Œå¯ä»¥æ·»åŠ æ¨¡å‹è¯„ä¼°é€»è¾‘
                # æš‚æ—¶ç›´æ¥ä½¿ç”¨å¢é‡æ¨¡å‹
                shutil.copy(incremental_file, final_file)
                print("âœ… å·²æ›´æ–°æœ€ç»ˆæ¨¡å‹")
            else:
                shutil.copy(incremental_file, final_file)
                print("âœ… å·²æ›´æ–°æœ€ç»ˆæ¨¡å‹")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description='å¢é‡è®­ç»ƒå·¥å…· - åœ¨å·²æœ‰æ¨¡å‹åŸºç¡€ä¸Šç»§ç»­è®­ç»ƒ',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # åœ¨å·²æœ‰æ¨¡å‹åŸºç¡€ä¸Šç»§ç»­è®­ç»ƒ
  python3 incremental_train.py data/training/novels/training_data.txt
  
  # æŒ‡å®šæ¨¡å‹è·¯å¾„å’Œè®­ç»ƒå‚æ•°
  python3 incremental_train.py data/training/novels/training_data.txt \
    --model-path models/my_model --epochs=20 --learning-rate=0.0001
        """
    )
    
    parser.add_argument('new_data_file', help='æ–°çš„è®­ç»ƒæ•°æ®æ–‡ä»¶ï¼ˆTSVæ ¼å¼ï¼‰')
    parser.add_argument('--model-path', default='models/text_rewriter_model',
                       help='æ¨¡å‹è·¯å¾„ï¼ˆé»˜è®¤: models/text_rewriter_modelï¼‰')
    parser.add_argument('--epochs', type=int, default=10,
                       help='è®­ç»ƒè½®æ•°ï¼ˆé»˜è®¤: 10ï¼Œå¢é‡è®­ç»ƒé€šå¸¸è¾ƒå°‘ï¼‰')
    parser.add_argument('--batch-size', type=int, default=16,
                       help='æ‰¹æ¬¡å¤§å°ï¼ˆé»˜è®¤: 16ï¼‰')
    parser.add_argument('--learning-rate', type=float, default=0.0001,
                       help='å­¦ä¹ ç‡ï¼ˆé»˜è®¤: 0.0001ï¼Œå¢é‡è®­ç»ƒä½¿ç”¨è¾ƒå°å­¦ä¹ ç‡ï¼‰')
    parser.add_argument('--merge', action='store_true',
                       help='è®­ç»ƒååˆå¹¶æ¨¡å‹')
    
    args = parser.parse_args()
    
    # åˆ›å»ºå¢é‡è®­ç»ƒå™¨
    trainer = IncrementalTrainer(args.model_path)
    
    # åŠ è½½å·²æœ‰æ¨¡å‹
    if not trainer.load_existing_model():
        print("\nâŒ æ— æ³•åŠ è½½å·²æœ‰æ¨¡å‹")
        print("   è¯·å…ˆè¿è¡ŒåŸºç¡€è®­ç»ƒ: python3 train_model.py <æ•°æ®æ–‡ä»¶>")
        sys.exit(1)
    
    # å¢é‡è®­ç»ƒ
    success = trainer.incremental_train(
        args.new_data_file,
        epochs=args.epochs,
        batch_size=args.batch_size,
        learning_rate=args.learning_rate
    )
    
    if success and args.merge:
        trainer.merge_models()
    
    if success:
        print("\nâœ… å¢é‡è®­ç»ƒå®Œæˆï¼")
        print(f"ğŸ’¡ ä½¿ç”¨æ–¹æ³•:")
        print(f"   python3 scripts/creative/rewrite_novel.py novel.txt \\")
        print(f"     --use-ai --ai-type=tensorflow --style=éƒ½å¸‚å¹½é»˜")
    else:
        print("\nâŒ å¢é‡è®­ç»ƒå¤±è´¥")
        sys.exit(1)


if __name__ == '__main__':
    main()

