#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è‡ªåŠ¨æ‰§è¡Œå®Œæ•´æµç¨‹
åŒ…æ‹¬ï¼šæ•°æ®æ£€æŸ¥ã€ç»“æ„åŒ–å¤„ç†ã€è®­ç»ƒæ•°æ®ç”Ÿæˆã€æ¨¡å‹è®­ç»ƒã€æ¸…ç†ç­‰
"""

import os
import sys
import json
import shutil
from pathlib import Path
from typing import Dict, List, Optional

# å¯¼å…¥æ ¸å¿ƒæ¨¡å—
import sys
import os
sys.path.insert(0, os.path.dirname(__file__))
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

try:
    from .pipeline import DataPipeline
    from .config_center import ConfigCenter
except ImportError:
    from pipeline import DataPipeline
    from config_center import ConfigCenter


class AutoExecutor:
    """è‡ªåŠ¨æ‰§è¡Œå™¨ - æ‰§è¡Œå®Œæ•´çš„å·¥ä½œæµç¨‹"""
    
    def __init__(self, output_dir: str = "data/training"):
        """
        åˆå§‹åŒ–è‡ªåŠ¨æ‰§è¡Œå™¨
        
        Args:
            output_dir: è¾“å‡ºç›®å½•
        """
        self.output_dir = output_dir
        self.pipeline = DataPipeline(output_dir)
        self.config = ConfigCenter()
        
        # æ‰§è¡Œç»Ÿè®¡
        self.stats = {
            'novels_found': 0,
            'structured': 0,
            'training_samples': 0,
            'model_trained': False,
            'errors': []
        }
    
    def check_existing_data(self) -> Dict:
        """
        æ£€æŸ¥ç°æœ‰æ•°æ®
        
        Returns:
            æ•°æ®çŠ¶æ€å­—å…¸
        """
        print("\n" + "="*60)
        print("ğŸ“‹ æ£€æŸ¥ç°æœ‰æ•°æ®")
        print("="*60)
        
        status = {
            'novels_exist': False,
            'novels_count': 0,
            'structured_exist': False,
            'structured_count': 0,
            'training_data_exist': False,
            'model_exist': False
        }
        
        # æ£€æŸ¥å°è¯´æ•°æ®
        novels_dir = os.path.join(self.output_dir, 'novels')
        if os.path.exists(novels_dir):
            novel_files = []
            for root, dirs, files in os.walk(novels_dir):
                for file in files:
                    if file.endswith('.txt'):
                        novel_files.append(os.path.join(root, file))
            
            status['novels_exist'] = len(novel_files) > 0
            status['novels_count'] = len(novel_files)
            self.stats['novels_found'] = len(novel_files)
            
            print(f"âœ… æ‰¾åˆ° {len(novel_files)} æœ¬å°è¯´")
        
        # æ£€æŸ¥ç»“æ„åŒ–æ•°æ®
        structured_dir = os.path.join(self.output_dir, 'structured')
        if os.path.exists(structured_dir):
            structured_files = [f for f in os.listdir(structured_dir) 
                              if f.endswith('_structured.json')]
            status['structured_exist'] = len(structured_files) > 0
            status['structured_count'] = len(structured_files)
            self.stats['structured'] = len(structured_files)
            
            if structured_files:
                print(f"âœ… æ‰¾åˆ° {len(structured_files)} ä¸ªç»“æ„åŒ–æ•°æ®æ–‡ä»¶")
        
        # æ£€æŸ¥è®­ç»ƒæ•°æ®
        training_file = os.path.join(self.output_dir, 'processed', 'training_data.txt')
        if os.path.exists(training_file):
            with open(training_file, 'r', encoding='utf-8') as f:
                lines = [line for line in f if line.strip() and '\t' in line]
            status['training_data_exist'] = len(lines) > 0
            self.stats['training_samples'] = len(lines)
            
            if lines:
                print(f"âœ… æ‰¾åˆ°è®­ç»ƒæ•°æ®: {len(lines)} æ¡æ ·æœ¬")
        
        # æ£€æŸ¥æ¨¡å‹
        model_path = self.pipeline.model_path
        if os.path.exists(model_path):
            model_files = [
                os.path.join(model_path, 'best_model.h5'),
                os.path.join(model_path, 'final_model.h5'),
                os.path.join(model_path, 'vocab.json')
            ]
            status['model_exist'] = any(os.path.exists(f) for f in model_files)
            
            if status['model_exist']:
                print(f"âœ… æ‰¾åˆ°å·²æœ‰æ¨¡å‹: {model_path}")
        
        return status
    
    def execute_full_workflow(self, 
                             epochs: int = 10,
                             batch_size: int = 8,
                             use_ai: bool = False,
                             auto_fix: bool = True,
                             max_retries: int = 3) -> bool:
        """
        æ‰§è¡Œå®Œæ•´å·¥ä½œæµç¨‹
        
        Args:
            epochs: è®­ç»ƒè½®æ•°
            batch_size: æ‰¹æ¬¡å¤§å°
            use_ai: æ˜¯å¦ä½¿ç”¨AI
            auto_fix: æ˜¯å¦è‡ªåŠ¨ä¿®å¤é—®é¢˜
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        
        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        print("\n" + "="*60)
        print("ğŸš€ å¼€å§‹æ‰§è¡Œå®Œæ•´å·¥ä½œæµç¨‹")
        print("="*60)
        
        retry_count = 0
        
        while retry_count < max_retries:
            try:
                # 1. æ£€æŸ¥ç°æœ‰æ•°æ®
                data_status = self.check_existing_data()
                
                if not data_status['novels_exist']:
                    print("\nâŒ æœªæ‰¾åˆ°å°è¯´æ•°æ®ï¼Œæ— æ³•ç»§ç»­")
                    return False
                
                # 2. ç»“æ„åŒ–å¤„ç†ï¼ˆå¦‚æœéœ€è¦ï¼‰
                if not data_status['structured_exist'] or retry_count > 0:
                    print("\n" + "="*60)
                    print("æ­¥éª¤ 1.5: ç»“æ„åŒ–æ•°æ®å¤„ç†")
                    print("="*60)
                    
                    # æ£€æŸ¥data_processoræ˜¯å¦å¯ç”¨
                    if hasattr(self.pipeline, 'data_processor') and self.pipeline.data_processor:
                        # æŸ¥æ‰¾å°è¯´ç›®å½•
                        novels_dir = self.pipeline.novels_dir
                        if os.path.exists(novels_dir):
                            # å°è¯•ä»ç›®å½•ç»“æ„æ¨æ–­category
                            category = self._infer_category(novels_dir)
                            site = "m.shuhaige.net"  # é»˜è®¤
                            
                            stats = self.pipeline.data_processor.process_batch(
                                novels_dir, category, site
                            )
                            
                            if stats['success'] > 0:
                                print(f"âœ… ç»“æ„åŒ–å¤„ç†å®Œæˆ: {stats['success']} æœ¬")
                            else:
                                print("âš ï¸  ç»“æ„åŒ–å¤„ç†å¤±è´¥ï¼Œç»§ç»­æ‰§è¡Œ")
                    else:
                        print("âš ï¸  æ•°æ®å¤„ç†å™¨ä¸å¯ç”¨ï¼Œè·³è¿‡ç»“æ„åŒ–å¤„ç†")
                
                # 3. ç”Ÿæˆè®­ç»ƒæ•°æ®
                print("\n" + "="*60)
                print("æ­¥éª¤ 3: ç”Ÿæˆè®­ç»ƒæ•°æ®")
                print("="*60)
                
                # ä¼˜å…ˆä½¿ç”¨å¢å¼ºç‰ˆç”Ÿæˆå™¨
                training_file = None
                if hasattr(self.pipeline, 'enhanced_generator') and self.pipeline.enhanced_generator:
                    structured_dir = os.path.join(self.output_dir, 'structured')
                    if os.path.exists(structured_dir):
                        training_file = self.pipeline.enhanced_generator.generate_from_structured_data(
                            use_ai=use_ai
                        )
                
                # å¦‚æœå¢å¼ºç‰ˆå¤±è´¥ï¼Œä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•
                if not training_file or not os.path.exists(training_file):
                    print("âš ï¸  å¢å¼ºç‰ˆç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•")
                    if not self.pipeline.step3_generate_training_data(use_ai=use_ai):
                        if auto_fix:
                            print("âš ï¸  è®­ç»ƒæ•°æ®ç”Ÿæˆå¤±è´¥ï¼Œå°è¯•ä¿®å¤...")
                            # åˆ é™¤å¯èƒ½æœ‰é—®é¢˜çš„æ–‡ä»¶
                            old_file = os.path.join(self.output_dir, 'processed', 'training_data.txt')
                            if os.path.exists(old_file):
                                os.remove(old_file)
                                print("   å·²åˆ é™¤æ—§è®­ç»ƒæ•°æ®æ–‡ä»¶")
                            
                            # é‡è¯•
                            if retry_count < max_retries - 1:
                                retry_count += 1
                                print(f"   é‡è¯• {retry_count}/{max_retries}")
                                continue
                        
                        print("âŒ è®­ç»ƒæ•°æ®ç”Ÿæˆå¤±è´¥")
                        return False
                    else:
                        training_file = os.path.join(self.output_dir, 'processed', 'training_data.txt')
                
                # éªŒè¯è®­ç»ƒæ•°æ®
                if training_file and os.path.exists(training_file):
                    valid_samples = self._validate_training_data(training_file)
                    if valid_samples == 0:
                        print("âŒ è®­ç»ƒæ•°æ®éªŒè¯å¤±è´¥ï¼šæ²¡æœ‰æœ‰æ•ˆæ ·æœ¬")
                        if auto_fix and retry_count < max_retries - 1:
                            retry_count += 1
                            print(f"   é‡è¯• {retry_count}/{max_retries}")
                            continue
                        return False
                    
                    self.stats['training_samples'] = valid_samples
                    print(f"âœ… è®­ç»ƒæ•°æ®éªŒè¯é€šè¿‡: {valid_samples} æ¡æœ‰æ•ˆæ ·æœ¬")
                
                # 4. è®­ç»ƒæ¨¡å‹
                print("\n" + "="*60)
                print("æ­¥éª¤ 4: è®­ç»ƒæ¨¡å‹")
                print("="*60)
                
                if not self.pipeline.step4_train(epochs=epochs, batch_size=batch_size, incremental=False):
                    print("âŒ æ¨¡å‹è®­ç»ƒå¤±è´¥")
                    if auto_fix and retry_count < max_retries - 1:
                        retry_count += 1
                        print(f"   é‡è¯• {retry_count}/{max_retries}")
                        continue
                    return False
                
                self.stats['model_trained'] = True
                
                # 5. éªŒè¯æ¨¡å‹
                print("\n" + "="*60)
                print("ğŸ“‹ æœ€ç»ˆéªŒè¯")
                print("="*60)
                
                model_files = [
                    os.path.join(self.pipeline.model_path, 'best_model.h5'),
                    os.path.join(self.pipeline.model_path, 'final_model.h5'),
                    os.path.join(self.pipeline.model_path, 'vocab.json')
                ]
                
                existing = [f for f in model_files if os.path.exists(f)]
                if existing:
                    print(f"âœ… æ¨¡å‹æ–‡ä»¶: {len(existing)}/{len(model_files)} ä¸ªæ–‡ä»¶å·²ç”Ÿæˆ")
                    for f in existing:
                        size = os.path.getsize(f) / 1024 / 1024
                        print(f"   - {os.path.basename(f)}: {size:.2f} MB")
                else:
                    print("âŒ æ¨¡å‹æ–‡ä»¶æœªç”Ÿæˆ")
                    return False
                
                # 6. æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                self.cleanup_temp_files()
                
                print("\n" + "="*60)
                print("âœ… å®Œæ•´å·¥ä½œæµç¨‹æ‰§è¡ŒæˆåŠŸï¼")
                print("="*60)
                print(f"\nğŸ“Š æ‰§è¡Œç»Ÿè®¡:")
                print(f"   å°è¯´æ•°é‡: {self.stats['novels_found']}")
                print(f"   ç»“æ„åŒ–æ•°æ®: {self.stats['structured']}")
                print(f"   è®­ç»ƒæ ·æœ¬: {self.stats['training_samples']}")
                print(f"   æ¨¡å‹è®­ç»ƒ: {'æˆåŠŸ' if self.stats['model_trained'] else 'å¤±è´¥'}")
                
                return True
                
            except Exception as e:
                print(f"\nâŒ æ‰§è¡Œè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
                import traceback
                traceback.print_exc()
                
                self.stats['errors'].append(str(e))
                
                if auto_fix and retry_count < max_retries - 1:
                    retry_count += 1
                    print(f"\nğŸ”„ è‡ªåŠ¨ä¿®å¤å¹¶é‡è¯• {retry_count}/{max_retries}")
                    continue
                else:
                    return False
        
        return False
    
    def _infer_category(self, novels_dir: str) -> str:
        """ä»ç›®å½•ç»“æ„æ¨æ–­å°è¯´ç±»å‹"""
        # æ£€æŸ¥ç¬¬ä¸€å±‚ç›®å½•
        if os.path.exists(novels_dir):
            items = os.listdir(novels_dir)
            for item in items:
                item_path = os.path.join(novels_dir, item)
                if os.path.isdir(item_path):
                    # æ£€æŸ¥æ˜¯å¦æ˜¯ç±»å‹ç›®å½•
                    common_categories = ['éƒ½å¸‚', 'ç„å¹»', 'è¨€æƒ…', 'æ­¦ä¾ ', 'ç§‘å¹»', 'æ‚¬ç–‘']
                    if item in common_categories:
                        return item
            
            # æ£€æŸ¥ç¬¬äºŒå±‚ç›®å½•
            for item in items:
                item_path = os.path.join(novels_dir, item)
                if os.path.isdir(item_path):
                    sub_items = os.listdir(item_path)
                    for sub_item in sub_items:
                        if sub_item in common_categories:
                            return sub_item
        
        return 'éƒ½å¸‚'  # é»˜è®¤
    
    def _validate_training_data(self, training_file: str) -> int:
        """éªŒè¯è®­ç»ƒæ•°æ®"""
        valid_count = 0
        
        try:
            with open(training_file, 'r', encoding='utf-8') as f:
                for line in f:
                    line = line.strip()
                    if not line:
                        continue
                    
                    parts = line.split('\t')
                    if len(parts) >= 3:
                        try:
                            orig = parts[0].strip()
                            rew = parts[1].strip()
                            style_id = int(parts[2])
                            
                            if len(orig) >= 10 and len(rew) >= 10 and 0 <= style_id <= 20:
                                valid_count += 1
                        except (ValueError, IndexError):
                            continue
        except Exception as e:
            print(f"âš ï¸  éªŒè¯è®­ç»ƒæ•°æ®æ—¶å‡ºé”™: {e}")
        
        return valid_count
    
    def cleanup_temp_files(self):
        """æ¸…ç†ä¸´æ—¶æ–‡ä»¶"""
        print("\n" + "="*60)
        print("ğŸ§¹ æ¸…ç†ä¸´æ—¶æ–‡ä»¶")
        print("="*60)
        
        cleaned_count = 0
        cleaned_size = 0
        
        # æ¸…ç†.tempç›®å½•
        temp_dirs = []
        for root, dirs, files in os.walk(self.output_dir):
            if '.temp' in root:
                temp_dirs.append(root)
        
        for temp_dir in temp_dirs:
            try:
                size = sum(os.path.getsize(os.path.join(dirpath, filename))
                          for dirpath, dirnames, filenames in os.walk(temp_dir)
                          for filename in filenames)
                shutil.rmtree(temp_dir)
                cleaned_count += 1
                cleaned_size += size
                print(f"   ğŸ—‘ï¸  å·²æ¸…ç†: {os.path.basename(temp_dir)} ({size/1024/1024:.2f} MB)")
            except Exception as e:
                print(f"   âš ï¸  æ¸…ç†å¤±è´¥ {temp_dir}: {e}")
        
        # æ¸…ç†è¿›åº¦æ–‡ä»¶
        progress_files = []
        for root, dirs, files in os.walk(self.output_dir):
            for file in files:
                if file.endswith('_progress.json') or file.endswith('.tmp'):
                    progress_files.append(os.path.join(root, file))
        
        for progress_file in progress_files:
            try:
                size = os.path.getsize(progress_file)
                os.remove(progress_file)
                cleaned_count += 1
                cleaned_size += size
            except Exception as e:
                pass
        
        if cleaned_count > 0:
            print(f"\nâœ… æ¸…ç†å®Œæˆ: {cleaned_count} ä¸ªæ–‡ä»¶/ç›®å½•ï¼Œé‡Šæ”¾ {cleaned_size/1024/1024:.2f} MB")
        else:
            print("â„¹ï¸  æ²¡æœ‰éœ€è¦æ¸…ç†çš„ä¸´æ—¶æ–‡ä»¶")


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='è‡ªåŠ¨æ‰§è¡Œå®Œæ•´å·¥ä½œæµç¨‹')
    parser.add_argument('--epochs', type=int, default=10, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--batch-size', type=int, default=8, help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--use-ai', action='store_true', help='ä½¿ç”¨AIç”Ÿæˆæ”¹å†™æ ·æœ¬')
    parser.add_argument('--no-auto-fix', action='store_true', help='ç¦ç”¨è‡ªåŠ¨ä¿®å¤')
    parser.add_argument('--max-retries', type=int, default=3, help='æœ€å¤§é‡è¯•æ¬¡æ•°')
    
    args = parser.parse_args()
    
    executor = AutoExecutor()
    
    success = executor.execute_full_workflow(
        epochs=args.epochs,
        batch_size=args.batch_size,
        use_ai=args.use_ai,
        auto_fix=not args.no_auto_fix,
        max_retries=args.max_retries
    )
    
    if success:
        print("\nğŸ‰ æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼")
        sys.exit(0)
    else:
        print("\nâŒ æ‰§è¡Œå¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")
        sys.exit(1)


if __name__ == '__main__':
    main()

