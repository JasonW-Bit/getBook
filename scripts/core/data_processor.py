#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•°æ®å¤„ç†å™¨
åœ¨çˆ¬å–åç«‹å³å¯¹æ•°æ®è¿›è¡Œç»“æ„åŒ–å¤„ç†ï¼Œæ‹†åˆ†æˆé€‚åˆå­¦ä¹ çš„æ ¼å¼
"""

import os
import re
import json
from typing import Dict, List, Optional, Tuple
from pathlib import Path
from .intelligent_analyzer import IntelligentAnalyzer
from .config_center import ConfigCenter


class DataProcessor:
    """æ•°æ®å¤„ç†å™¨ - å°†çˆ¬å–çš„æ•°æ®è½¬æ¢ä¸ºå­¦ä¹ æ•°æ®ç»“æ„"""
    
    def __init__(self, output_dir: str = "data/training"):
        """
        åˆå§‹åŒ–æ•°æ®å¤„ç†å™¨
        
        Args:
            output_dir: è¾“å‡ºç›®å½•
        """
        self.output_dir = output_dir
        self.structured_dir = os.path.join(output_dir, 'structured')
        
        # åˆå§‹åŒ–é…ç½®ä¸­å¿ƒå’Œåˆ†æå™¨
        self.config = ConfigCenter()
        self.analyzer = IntelligentAnalyzer(config_center=self.config)
        
        os.makedirs(self.structured_dir, exist_ok=True)
    
    def process_novel(self, novel_file: str, category: str, site: str = "m.shuhaige.net") -> Optional[Dict]:
        """
        å¤„ç†å•æœ¬å°è¯´ï¼Œè½¬æ¢ä¸ºç»“æ„åŒ–æ•°æ®
        
        Args:
            novel_file: å°è¯´æ–‡ä»¶è·¯å¾„
            category: å°è¯´ç±»å‹
            site: æ¥æºç½‘ç«™
        
        Returns:
            ç»“æ„åŒ–æ•°æ®å­—å…¸
        """
        if not os.path.exists(novel_file):
            return None
        
        print(f"  ğŸ“– å¤„ç†: {Path(novel_file).name}")
        
        # è¯»å–å°è¯´å†…å®¹
        with open(novel_file, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # 1. æ™ºèƒ½åˆ†æ
        analysis = self.analyzer.analyze_novel_structure(content)
        
        # 1.5. ä»åˆ†æç»“æœä¸­å­¦ä¹ æ–°å…³é”®è¯ï¼ˆè‡ªåŠ¨æ›´æ–°é…ç½®ï¼‰
        self.config.learn_from_analysis(analysis)
        
        # 2. ç»“æ„åŒ–æ‹†åˆ†
        structured_data = self._structure_content(content, analysis, category, site)
        
        # 3. æ•°æ®éªŒè¯
        if not self._validate_structured_data(structured_data):
            print(f"    âš ï¸  æ•°æ®éªŒè¯å¤±è´¥ï¼Œè·³è¿‡")
            return None
        
        # 4. ä¿å­˜ç»“æ„åŒ–æ•°æ®
        novel_name = Path(novel_file).stem
        output_file = os.path.join(self.structured_dir, f"{novel_name}_structured.json")
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(structured_data, f, ensure_ascii=False, indent=2)
        
        print(f"    âœ… å·²ä¿å­˜ç»“æ„åŒ–æ•°æ®: {len(structured_data['chapters'])} ç« ")
        
        return structured_data
    
    def _structure_content(self, content: str, analysis: Dict, category: str, site: str) -> Dict:
        """
        å°†å†…å®¹æ‹†åˆ†ä¸ºç»“æ„åŒ–æ•°æ®
        
        Args:
            content: å°è¯´å†…å®¹
            analysis: åˆ†æç»“æœ
            category: ç±»å‹
            site: æ¥æºç½‘ç«™
        
        Returns:
            ç»“æ„åŒ–æ•°æ®
        """
        # æå–åŸºæœ¬ä¿¡æ¯
        title_match = re.search(r'æ ‡é¢˜[ï¼š:ï¼š]?\s*(.+)', content)
        author_match = re.search(r'ä½œè€…[ï¼š:ï¼š]?\s*(.+)', content)
        
        title = title_match.group(1).strip() if title_match else Path(content[:100]).stem
        author = author_match.group(1).strip() if author_match else 'æœªçŸ¥'
        
        # æŒ‰ç« èŠ‚æ‹†åˆ†
        chapters = self._split_into_chapters(content)
        
        # æ„å»ºç»“æ„åŒ–æ•°æ®
        structured_data = {
            'metadata': {
                'title': title,
                'author': author,
                'category': category,
                'site': site,
                'total_chapters': len(chapters),
                'total_chars': len(content)
            },
            'analysis': analysis,
            'chapters': []
        }
        
        # å¤„ç†æ¯ä¸ªç« èŠ‚
        for i, chapter_content in enumerate(chapters, 1):
            chapter_data = self._process_chapter(
                chapter_content, 
                i, 
                analysis,
                structured_data['chapters'][-1] if structured_data['chapters'] else None
            )
            
            if chapter_data:
                structured_data['chapters'].append(chapter_data)
        
        return structured_data
    
    def _split_into_chapters(self, content: str) -> List[str]:
        """å°†å†…å®¹æ‹†åˆ†ä¸ºç« èŠ‚"""
        # æŸ¥æ‰¾ç« èŠ‚æ ‡è®°
        chapter_pattern = r'ç¬¬\s*\d+\s*ç« [ï¼š:ï¼š]?\s*.+?\n'
        chapter_matches = list(re.finditer(chapter_pattern, content))
        
        if not chapter_matches:
            # å¦‚æœæ²¡æœ‰ç« èŠ‚æ ‡è®°ï¼ŒæŒ‰æ®µè½åˆ†å‰²
            paragraphs = re.split(r'\n\s*\n', content)
            return [p.strip() for p in paragraphs if len(p.strip()) > 500]
        
        chapters = []
        for i, match in enumerate(chapter_matches):
            start = match.end()
            end = chapter_matches[i + 1].start() if i + 1 < len(chapter_matches) else len(content)
            chapter_content = content[start:end].strip()
            
            if len(chapter_content) > 100:  # è‡³å°‘100å­—ç¬¦
                chapters.append(chapter_content)
        
        return chapters
    
    def _process_chapter(self, chapter_content: str, chapter_num: int, 
                         novel_analysis: Dict, prev_chapter: Optional[Dict]) -> Optional[Dict]:
        """
        å¤„ç†å•ä¸ªç« èŠ‚
        
        Args:
            chapter_content: ç« èŠ‚å†…å®¹
            chapter_num: ç« èŠ‚å·
            novel_analysis: æ•´æœ¬å°è¯´çš„åˆ†æç»“æœ
            prev_chapter: å‰ä¸€ç« çš„æ•°æ®ï¼ˆç”¨äºä¸Šä¸‹æ–‡ï¼‰
        
        Returns:
            ç« èŠ‚ç»“æ„åŒ–æ•°æ®
        """
        if len(chapter_content) < 200:  # å¤ªçŸ­çš„ç« èŠ‚è·³è¿‡
            return None
        
        # æ‹†åˆ†ä¸ºæ®µè½
        paragraphs = re.split(r'\n\s*\n', chapter_content)
        paragraphs = [p.strip() for p in paragraphs if len(p.strip()) > 50]
        
        # æ‹†åˆ†ä¸ºå¥å­
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ]', chapter_content)
        sentences = [s.strip() for s in sentences if len(s.strip()) > 10]
        
        # æå–å¯¹è¯
        dialogues = re.findall(r'["""]([^"""]+)["""]', chapter_content)
        
        # æå–åœºæ™¯
        scenes = self._extract_scenes(paragraphs)
        
        # æå–æƒ…èŠ‚è¦ç‚¹
        plot_points = self._extract_plot_points(chapter_content)
        
        # æå–äººç‰©å‡ºåœº
        characters_in_chapter = self._extract_characters_in_chapter(chapter_content, novel_analysis.get('characters', {}))
        
        # æå–æƒ…æ„Ÿå˜åŒ–
        emotional_flow = self._extract_emotional_flow(sentences)
        
        # æ„å»ºç« èŠ‚æ•°æ®
        chapter_data = {
            'chapter_num': chapter_num,
            'length': len(chapter_content),
            'paragraphs': paragraphs,
            'sentences': sentences[:50],  # æœ€å¤š50å¥
            'dialogues': dialogues[:20],  # æœ€å¤š20æ®µå¯¹è¯
            'scenes': scenes,
            'plot_points': plot_points,
            'characters': characters_in_chapter,
            'emotional_flow': emotional_flow,
            'writing_features': {
                'dialogue_ratio': len(dialogues) / len(sentences) if sentences else 0,
                'description_ratio': self._calculate_description_ratio(chapter_content),
                'action_ratio': self._calculate_action_ratio(chapter_content)
            },
            'context': {
                'prev_summary': prev_chapter.get('plot_points', [])[-1] if prev_chapter else None,
                'characters_continuity': self._check_character_continuity(
                    characters_in_chapter,
                    prev_chapter.get('characters', {}) if prev_chapter else {}
                )
            }
        }
        
        return chapter_data
    
    def _extract_scenes(self, paragraphs: List[str]) -> List[Dict]:
        """æå–åœºæ™¯ï¼ˆä½¿ç”¨é…ç½®ä¸­å¿ƒï¼‰"""
        scenes = []
        scene_keywords = set(self.config.get_scene_keywords())  # è½¬æ¢ä¸ºsetæé«˜æŸ¥æ‰¾æ•ˆç‡
        
        for para in paragraphs:
            # æŸ¥æ‰¾åœºæ™¯å…³é”®è¯
            found = False
            for keyword in scene_keywords:
                if keyword in para:
                    scenes.append({
                        'location': keyword,
                        'description': para[:100]  # å‰100å­—ç¬¦
                    })
                    found = True
                    break
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å·²çŸ¥åœºæ™¯ï¼Œå°è¯•æå–æ–°åœºæ™¯ï¼ˆç®€å•æ¨¡å¼ï¼‰
            if not found:
                # å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ æ–°åœºæ™¯æå–é€»è¾‘
                pass
        
        return scenes[:10]  # æœ€å¤š10ä¸ªåœºæ™¯
    
    def _extract_plot_points(self, content: str) -> List[str]:
        """æå–æƒ…èŠ‚è¦ç‚¹"""
        # æå–å…³é”®åŠ¨ä½œå’Œäº‹ä»¶
        action_patterns = [
            r'([^ã€‚ï¼ï¼Ÿ]+(?:æ‰“|æ€|æ•‘|é€ƒ|è¿½|æ‰¾|å‘ç°|é‡åˆ°|å†³å®š|å¼€å§‹|ç»“æŸ)[^ã€‚ï¼ï¼Ÿ]+[ã€‚ï¼ï¼Ÿ])',
            r'([^ã€‚ï¼ï¼Ÿ]+(?:çªç„¶|å¿½ç„¶|ç¬é—´|ç«‹åˆ»|é©¬ä¸Š)[^ã€‚ï¼ï¼Ÿ]+[ã€‚ï¼ï¼Ÿ])'
        ]
        
        plot_points = []
        for pattern in action_patterns:
            matches = re.findall(pattern, content)
            plot_points.extend(matches[:5])  # æ¯ç« æœ€å¤š5ä¸ªè¦ç‚¹
        
        return plot_points[:5]
    
    def _extract_characters_in_chapter(self, content: str, novel_characters: Dict) -> Dict:
        """æå–ç« èŠ‚ä¸­å‡ºç°çš„äººç‰©"""
        characters_in_chapter = {}
        
        for char_name, char_info in novel_characters.items():
            # æ£€æŸ¥äººç‰©æ˜¯å¦åœ¨æœ¬ç« å‡ºç°
            if char_name in content:
                # ç»Ÿè®¡å‡ºç°æ¬¡æ•°
                count = content.count(char_name)
                if count > 0:
                    characters_in_chapter[char_name] = {
                        'mention_count': count,
                        'personality': char_info.get('personality', {}),
                        'speaking_style': char_info.get('speaking_style', {}),
                        'key_actions': self._extract_character_actions(content, char_name)
                    }
        
        return characters_in_chapter
    
    def _extract_character_actions(self, content: str, char_name: str) -> List[str]:
        """æå–äººç‰©çš„å…³é”®åŠ¨ä½œ"""
        # æŸ¥æ‰¾åŒ…å«è¯¥äººç‰©çš„å¥å­
        pattern = f'{char_name}[ï¼Œ,ã€‚ï¼ï¼Ÿï¼›ï¼š:""""]?[^ã€‚ï¼ï¼Ÿï¼›]*[ã€‚ï¼ï¼Ÿï¼›]'
        sentences = re.findall(pattern, content)
        
        # æå–åŠ¨ä½œ
        actions = []
        action_keywords = ['è¯´', 'é“', 'çœ‹', 'ç¬‘', 'èµ°', 'æ¥', 'å»', 'åš', 'æƒ³', 'å†³å®š', 'å¼€å§‹', 'ç»“æŸ']
        
        for sent in sentences[:10]:  # æœ€å¤š10å¥
            for keyword in action_keywords:
                if keyword in sent:
                    actions.append(sent[:50])  # å‰50å­—ç¬¦
                    break
        
        return actions[:5]  # æœ€å¤š5ä¸ªåŠ¨ä½œ
    
    def _extract_emotional_flow(self, sentences: List[str]) -> List[Dict]:
        """æå–æƒ…æ„ŸæµåŠ¨"""
        emotional_flow = []
        
        emotion_keywords = {
            'ç§¯æ': ['å¼€å¿ƒ', 'é«˜å…´', 'å¿«ä¹', 'å…´å¥‹', 'æ»¡è¶³', 'æ»¡æ„', 'å–œæ¬¢', 'çˆ±'],
            'æ¶ˆæ': ['éš¾è¿‡', 'æ‚²ä¼¤', 'ç—›è‹¦', 'æ„¤æ€’', 'å¤±æœ›', 'æ²®ä¸§', 'è®¨åŒ', 'æ¨'],
            'ç´§å¼ ': ['ç´§å¼ ', 'ç„¦è™‘', 'æ‹…å¿ƒ', 'å®³æ€•', 'ææƒ§', 'ä¸å®‰'],
            'å¹³é™': ['å¹³é™', 'å†·é™', 'æ·¡å®š', 'ä»å®¹', 'é•‡å®š']
        }
        
        for i, sentence in enumerate(sentences[:30]):  # åˆ†æå‰30å¥
            for emotion, keywords in emotion_keywords.items():
                score = sum(sentence.count(kw) for kw in keywords)
                if score > 0:
                    emotional_flow.append({
                        'position': i,
                        'emotion': emotion,
                        'intensity': score
                    })
                    break
        
        return emotional_flow[:10]  # æœ€å¤š10ä¸ªæƒ…æ„Ÿç‚¹
    
    def _calculate_description_ratio(self, content: str) -> float:
        """è®¡ç®—æå†™æ¯”ä¾‹"""
        description_keywords = ['çš„', 'åœ°', 'å¾—', 'å¾ˆ', 'éå¸¸', 'ç‰¹åˆ«', 'ååˆ†']
        description_count = sum(content.count(kw) for kw in description_keywords)
        return description_count / len(content) if content else 0
    
    def _calculate_action_ratio(self, content: str) -> float:
        """è®¡ç®—åŠ¨ä½œæ¯”ä¾‹"""
        action_keywords = ['èµ°', 'è·‘', 'è·³', 'æ‰“', 'çœ‹', 'è¯´', 'åš', 'æ¥', 'å»', 'åŠ¨']
        action_count = sum(content.count(kw) for kw in action_keywords)
        return action_count / len(content) if content else 0
    
    def _check_character_continuity(self, current_chars: Dict, prev_chars: Dict) -> Dict:
        """æ£€æŸ¥äººç‰©è¿ç»­æ€§"""
        continuity = {
            'continued': [],  # ç»§ç»­å‡ºç°çš„äººç‰©
            'new': [],  # æ–°å‡ºç°çš„äººç‰©
            'disappeared': []  # æ¶ˆå¤±çš„äººç‰©
        }
        
        for char_name in current_chars.keys():
            if char_name in prev_chars:
                continuity['continued'].append(char_name)
            else:
                continuity['new'].append(char_name)
        
        for char_name in prev_chars.keys():
            if char_name not in current_chars:
                continuity['disappeared'].append(char_name)
        
        return continuity
    
    def _validate_structured_data(self, data: Dict) -> bool:
        """éªŒè¯ç»“æ„åŒ–æ•°æ®"""
        # æ£€æŸ¥å¿…éœ€å­—æ®µ
        if 'metadata' not in data:
            return False
        
        if 'chapters' not in data or len(data['chapters']) == 0:
            return False
        
        # æ£€æŸ¥ç« èŠ‚æ•°æ®å®Œæ•´æ€§
        for chapter in data['chapters']:
            if 'chapter_num' not in chapter or 'paragraphs' not in chapter:
                return False
            
            if len(chapter['paragraphs']) == 0:
                return False
        
        return True
    
    def process_batch(self, novels_dir: str, category: str, site: str = "m.shuhaige.net") -> Dict:
        """
        æ‰¹é‡å¤„ç†å°è¯´
        
        Args:
            novels_dir: å°è¯´ç›®å½•
            category: ç±»å‹
            site: æ¥æºç½‘ç«™
        
        Returns:
            å¤„ç†ç»Ÿè®¡
        """
        print(f"\nğŸ“š å¼€å§‹æ‰¹é‡å¤„ç†ç»“æ„åŒ–æ•°æ®...")
        print(f"   ç›®å½•: {novels_dir}")
        print(f"   ç±»å‹: {category}")
        
        stats = {
            'total': 0,
            'success': 0,
            'failed': 0
        }
        
        # æŸ¥æ‰¾æ‰€æœ‰å°è¯´æ–‡ä»¶
        novel_files = []
        for root, dirs, files in os.walk(novels_dir):
            for file in files:
                if file.endswith('.txt'):
                    novel_files.append(os.path.join(root, file))
        
        stats['total'] = len(novel_files)
        
        # å¤„ç†æ¯æœ¬å°è¯´
        for novel_file in novel_files:
            try:
                result = self.process_novel(novel_file, category, site)
                if result:
                    stats['success'] += 1
                else:
                    stats['failed'] += 1
            except Exception as e:
                print(f"    âŒ å¤„ç†å¤±è´¥: {e}")
                stats['failed'] += 1
        
        print(f"\nâœ… æ‰¹é‡å¤„ç†å®Œæˆ:")
        print(f"   æ€»è®¡: {stats['total']}")
        print(f"   æˆåŠŸ: {stats['success']}")
        print(f"   å¤±è´¥: {stats['failed']}")
        
        return stats

