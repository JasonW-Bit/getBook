#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è®­ç»ƒæ•°æ®ç”Ÿæˆå™¨ï¼ˆç»Ÿä¸€ç‰ˆæœ¬ï¼‰
ä»çˆ¬å–çš„å°è¯´ç”Ÿæˆè®­ç»ƒæ•°æ®ï¼Œæ”¯æŒå¤šç½‘ç«™ã€å¤šç±»å‹
"""

import os
import sys
import re
import json
from typing import List, Dict, Optional
from pathlib import Path

# å¯¼å…¥æ•°æ®å¢å¼ºæ¨¡å—
try:
    from ..utils.data_enhancer import DataEnhancer
except ImportError:
    import sys
    sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'utils'))
    from data_enhancer import DataEnhancer


class TrainingDataGenerator:
    """è®­ç»ƒæ•°æ®ç”Ÿæˆå™¨"""
    
    # é…ç½®å‚æ•°ï¼ˆä¼˜åŒ–ç‰ˆï¼‰
    MIN_CHUNK_LENGTH = 300        # æœ€å°æ–‡æœ¬å—é•¿åº¦ï¼ˆå¢åŠ ä»¥ä¿æŒæ›´å¤šä¸Šä¸‹æ–‡ï¼‰
    MAX_CHUNK_LENGTH = 3000       # æœ€å¤§æ–‡æœ¬å—é•¿åº¦ï¼ˆå¢åŠ ä»¥ä¿æŒæ›´å¤šä¸Šä¸‹æ–‡ï¼‰
    CHUNK_OVERLAP = 200           # æ–‡æœ¬å—é‡å é•¿åº¦ï¼ˆå¢åŠ ä»¥ä¿æŒè¿ç»­æ€§ï¼‰
    MAX_SAMPLES_PER_NOVEL = 200   # æ¯æœ¬å°è¯´æœ€å¤šç”Ÿæˆæ ·æœ¬æ•°ï¼ˆå¤§å¹…å¢åŠ ï¼‰
    MAX_TOTAL_SAMPLES = 500000    # æ€»æ ·æœ¬æ•°é™åˆ¶ï¼ˆå¢åŠ ï¼‰
    CONTEXT_WINDOW = 500          # ä¸Šä¸‹æ–‡çª—å£å¤§å°ï¼ˆç”¨äºä¿æŒå‰åæ–‡è¿è´¯ï¼‰
    
    def __init__(self, output_dir: str = "data/training"):
        """
        åˆå§‹åŒ–ç”Ÿæˆå™¨
        
        Args:
            output_dir: è¾“å‡ºç›®å½•
        """
        self.output_dir = output_dir
        self.novels_dir = os.path.join(output_dir, 'novels')
        self.processed_dir = os.path.join(output_dir, 'processed')
        os.makedirs(self.processed_dir, exist_ok=True)
    
    def _detect_directory_structure(self, base_dir: str) -> str:
        """
        æ£€æµ‹ç›®å½•ç»“æ„ç±»å‹
        
        Args:
            base_dir: åŸºç¡€ç›®å½•
        
        Returns:
            'novels' æˆ– 'processed' æˆ– 'unknown'
        """
        if not os.path.exists(base_dir):
            return 'unknown'
        
        # æ£€æŸ¥ç¬¬ä¸€å±‚ç›®å½•
        first_level_items = [item for item in os.listdir(base_dir) 
                           if os.path.isdir(os.path.join(base_dir, item))]
        
        if not first_level_items:
            return 'unknown'
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯novelsç»“æ„ï¼ˆç½‘ç«™/ç±»å‹/å°è¯´å/ï¼‰
        # æˆ–processedç»“æ„ï¼ˆç±»å‹/å°è¯´å.txtï¼‰
        for item in first_level_items[:3]:  # æ£€æŸ¥å‰3ä¸ª
            item_path = os.path.join(base_dir, item)
            # æ£€æŸ¥ç¬¬äºŒå±‚
            second_level_items = [sub_item for sub_item in os.listdir(item_path)
                                if os.path.isdir(os.path.join(item_path, sub_item))]
            
            if second_level_items:
                # æ£€æŸ¥ç¬¬ä¸‰å±‚ï¼ˆnovelsç»“æ„ï¼šç±»å‹/å°è¯´å/ï¼‰
                for sub_item in second_level_items[:2]:
                    sub_path = os.path.join(item_path, sub_item)
                    third_level_items = [t_item for t_item in os.listdir(sub_path)
                                       if os.path.isdir(os.path.join(sub_path, t_item))]
                    if third_level_items:
                        return 'novels'
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯processedç»“æ„ï¼ˆç±»å‹/å°è¯´å.txtï¼‰
            txt_files = [f for f in os.listdir(item_path) if f.endswith('.txt')]
            if txt_files:
                return 'processed'
        
        return 'unknown'
    
    def _find_novel_files(self, base_dir: str) -> List[tuple]:
        """
        æŸ¥æ‰¾æ‰€æœ‰å°è¯´æ–‡ä»¶ï¼ˆæ”¯æŒä¸¤ç§ç›®å½•ç»“æ„ï¼‰
        
        Args:
            base_dir: åŸºç¡€ç›®å½•
        
        Returns:
            [(æ–‡ä»¶è·¯å¾„, å°è¯´å, ç±»å‹), ...]
        """
        structure = self._detect_directory_structure(base_dir)
        files = []
        
        if structure == 'novels':
            # novelsç»“æ„ï¼šç½‘ç«™/ç±»å‹/å°è¯´å/å°è¯´å.txt
            for site_name in os.listdir(base_dir):
                site_dir = os.path.join(base_dir, site_name)
                if not os.path.isdir(site_dir):
                    continue
                
                for category in os.listdir(site_dir):
                    category_dir = os.path.join(site_dir, category)
                    if not os.path.isdir(category_dir):
                        continue
                    
                    for novel_name in os.listdir(category_dir):
                        novel_dir = os.path.join(category_dir, novel_name)
                        if not os.path.isdir(novel_dir):
                            continue
                        
                        # åœ¨å°è¯´ç›®å½•ä¸­æŸ¥æ‰¾txtæ–‡ä»¶
                        for file_name in os.listdir(novel_dir):
                            if file_name.endswith('.txt'):
                                files.append((
                                    os.path.join(novel_dir, file_name),
                                    novel_name,
                                    category
                                ))
                                break
        
        elif structure == 'processed':
            # processedç»“æ„ï¼šç±»å‹/å°è¯´å.txt
            for category in os.listdir(base_dir):
                category_dir = os.path.join(base_dir, category)
                if not os.path.isdir(category_dir):
                    continue
                
                for file_name in os.listdir(category_dir):
                    if file_name.endswith('.txt'):
                        novel_name = file_name[:-4]  # ç§»é™¤.txtåç¼€
                        files.append((
                            os.path.join(category_dir, file_name),
                            novel_name,
                            category
                        ))
        
        return files
    
    def generate_from_novels(self, use_ai: bool = False, enhance: bool = True, balance: bool = True, fallback_dir: Optional[str] = None) -> str:
        """
        ä»çˆ¬å–çš„å°è¯´ç”Ÿæˆè®­ç»ƒæ•°æ®ï¼ˆæ”¯æŒä¸¤ç§ç›®å½•ç»“æ„ï¼‰
        
        Args:
            use_ai: æ˜¯å¦ä½¿ç”¨AIç”Ÿæˆæ”¹å†™æ ·æœ¬
            enhance: æ˜¯å¦ä½¿ç”¨æ•°æ®å¢å¼º
            balance: æ˜¯å¦å¹³è¡¡æ•°æ®é›†
            fallback_dir: å›é€€ç›®å½•ï¼ˆå¦‚æœå½“å‰ç›®å½•ç”Ÿæˆå¤±è´¥ï¼‰
        
        Returns:
            è®­ç»ƒæ•°æ®æ–‡ä»¶è·¯å¾„
        """
        print(f"\nğŸ“ å¼€å§‹ç”Ÿæˆè®­ç»ƒæ•°æ®...")
        print(f"   æºç›®å½•: {self.novels_dir}")
        
        if not os.path.exists(self.novels_dir):
            print(f"âŒ æºç›®å½•ä¸å­˜åœ¨: {self.novels_dir}")
            # å°è¯•å›é€€ç›®å½•
            if fallback_dir and os.path.exists(fallback_dir):
                print(f"   âš ï¸  å°è¯•å›é€€åˆ°: {fallback_dir}")
                self.novels_dir = fallback_dir
            else:
                return None
        
        # æ£€æµ‹ç›®å½•ç»“æ„
        structure = self._detect_directory_structure(self.novels_dir)
        print(f"   æ£€æµ‹åˆ°ç›®å½•ç»“æ„: {structure}")
        
        if structure == 'unknown':
            print(f"   âš ï¸  æ— æ³•è¯†åˆ«ç›®å½•ç»“æ„ï¼Œå°è¯•å›é€€...")
            if fallback_dir and os.path.exists(fallback_dir):
                print(f"   å›é€€åˆ°: {fallback_dir}")
                original_dir = self.novels_dir
                self.novels_dir = fallback_dir
                structure = self._detect_directory_structure(self.novels_dir)
                if structure == 'unknown':
                    self.novels_dir = original_dir
                    print(f"   âŒ å›é€€ç›®å½•ä¹Ÿæ— æ³•è¯†åˆ«ç»“æ„")
                    return None
            else:
                print(f"   âŒ æ— æ³•è¯†åˆ«ç›®å½•ç»“æ„ï¼Œä¸”æ— å›é€€ç›®å½•")
                return None
        
        training_samples = []
        
        # æŸ¥æ‰¾æ‰€æœ‰å°è¯´æ–‡ä»¶
        novel_files = self._find_novel_files(self.novels_dir)
        
        if not novel_files:
            print(f"   âŒ æœªæ‰¾åˆ°ä»»ä½•å°è¯´æ–‡ä»¶")
            # å°è¯•å›é€€
            if fallback_dir and os.path.exists(fallback_dir) and fallback_dir != self.novels_dir:
                print(f"   âš ï¸  å°è¯•å›é€€åˆ°: {fallback_dir}")
                original_dir = self.novels_dir
                self.novels_dir = fallback_dir
                novel_files = self._find_novel_files(self.novels_dir)
                if not novel_files:
                    self.novels_dir = original_dir
                    return None
        
        print(f"   æ‰¾åˆ° {len(novel_files)} æœ¬å°è¯´")
        
        # æŒ‰ç±»å‹åˆ†ç»„å¤„ç†
        novels_by_category = {}
        for file_path, novel_name, category in novel_files:
            if category not in novels_by_category:
                novels_by_category[category] = []
            novels_by_category[category].append((file_path, novel_name))
        
        for category, novels in novels_by_category.items():
            print(f"\n   å¤„ç†ç±»å‹: {category} ({len(novels)} æœ¬)")
            
            for txt_file, novel_name in novels:
                try:
                    with open(txt_file, 'r', encoding='utf-8') as f:
                        content = f.read()
                    
                    # æå–ç« èŠ‚ï¼ˆæ”¯æŒå¤šç§æ ¼å¼ï¼‰
                    chapter_patterns = [
                        r'ç¬¬\s*(\d+)\s*ç« [ï¼š:ï¼š]?\s*(.*?)\n',
                        r'ç¬¬\s*[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸‡]+\s*ç« [ï¼š:ï¼š]?\s*(.*?)\n',
                        r'Chapter\s*\d+[ï¼š:ï¼š]?\s*(.*?)\n',
                    ]
                    
                    chapter_matches = []
                    for pattern in chapter_patterns:
                        matches = list(re.finditer(pattern, content))
                        if matches:
                            chapter_matches = matches
                            break
                    
                    if not chapter_matches:
                        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç« èŠ‚æ ‡è®°ï¼ŒæŒ‰æ®µè½åˆ†å‰²
                        paragraphs = re.split(r'\n\s*\n', content)
                        chapter_matches = [None] * len(paragraphs)
                    
                    # ä¸ºæ¯ä¸ªç« èŠ‚ç”Ÿæˆè®­ç»ƒæ ·æœ¬ï¼ˆå¢å¼ºç‰ˆï¼ŒåŒ…å«ä¸Šä¸‹æ–‡ï¼‰
                    samples_generated = 0
                    novel_context = self._extract_novel_context(content)  # æå–æ•´æœ¬å°è¯´çš„ä¸Šä¸‹æ–‡ä¿¡æ¯
                    
                    for i, match in enumerate(chapter_matches):
                        if samples_generated >= self.MAX_SAMPLES_PER_NOVEL:
                            break
                        
                        if match:
                            chapter_start = match.end()
                            next_match = chapter_matches[i + 1] if i + 1 < len(chapter_matches) else None
                            chapter_end = next_match.start() if next_match else min(chapter_start + self.MAX_CHUNK_LENGTH, len(content))
                        else:
                            # æ®µè½æ¨¡å¼
                            chapter_start = 0
                            chapter_end = len(content)
                        
                        chapter_text = content[chapter_start:chapter_end].strip()
                        
                        if len(chapter_text) < self.MIN_CHUNK_LENGTH:
                            continue
                        
                        # è·å–å‰åç« èŠ‚çš„ä¸Šä¸‹æ–‡ï¼ˆç”¨äºä¿æŒè¿è´¯æ€§ï¼‰
                        prev_context = ""
                        if i > 0:
                            prev_start = max(0, chapter_start - self.CONTEXT_WINDOW)
                            prev_context = content[prev_start:chapter_start].strip()
                        
                        next_context = ""
                        if next_match:
                            next_end = min(len(content), chapter_end + self.CONTEXT_WINDOW)
                            next_context = content[chapter_end:next_end].strip()
                        
                        # å°†ç« èŠ‚åˆ†å‰²æˆå¤šä¸ªæ–‡æœ¬å—ï¼ˆæ•°æ®å¢å¼ºï¼ŒåŒ…å«ä¸Šä¸‹æ–‡ï¼‰
                        chunks = self._split_into_chunks_with_context(
                            chapter_text, 
                            prev_context, 
                            next_context,
                            novel_context
                        )
                        
                        for chunk_data in chunks:
                            chunk = chunk_data['text']
                            if len(chunk) < self.MIN_CHUNK_LENGTH:
                                continue
                            
                            # é™åˆ¶é•¿åº¦
                            original = chunk[:self.MAX_CHUNK_LENGTH].strip()
                            
                            # é£æ ¼IDï¼ˆæ ¹æ®categoryæ˜ å°„ï¼‰
                            style_id = self._get_style_id(category)
                            
                            # ç”Ÿæˆæ”¹å†™æ–‡æœ¬ï¼ˆä½¿ç”¨AIæ”¹å†™ï¼Œå¦‚æœå¯ç”¨ï¼‰
                            rewritten = self._generate_rewritten_text(
                                original, 
                                style_id, 
                                chunk_data.get('context', ''),
                                use_ai=use_ai
                            )
                            
                            # åŒ…å«ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆç”¨äºè®­ç»ƒæ—¶ä¿æŒè¿è´¯æ€§ï¼‰
                            training_samples.append({
                                'original': original,
                                'rewritten': rewritten,
                                'style': style_id,
                                'source': novel_name,
                                'site': 'm.shuhaige.net',  # æ·»åŠ siteå­—æ®µï¼ˆä»ç›®å½•ç»“æ„æ¨æ–­ï¼‰
                                'category': category,
                                'context': chunk_data.get('context', ''),  # ä¸Šä¸‹æ–‡ä¿¡æ¯
                                'chapter_num': i + 1,  # ç« èŠ‚å·
                            })
                            
                            samples_generated += 1
                            
                            if len(training_samples) >= self.MAX_TOTAL_SAMPLES:
                                break
                        
                        if len(training_samples) >= self.MAX_TOTAL_SAMPLES:
                            break
                
                except (OSError, IOError, UnicodeDecodeError) as e:
                    print(f"      âš ï¸  å¤„ç† {novel_name} æ—¶å‡ºé”™: {e}")
                    continue
                
                if len(training_samples) >= self.MAX_TOTAL_SAMPLES:
                    break
            
            if len(training_samples) >= self.MAX_TOTAL_SAMPLES:
                break
        
        # æ•°æ®å¢å¼º
        if enhance and training_samples:
            print(f"\nğŸ”„ è¿›è¡Œæ•°æ®å¢å¼º...")
            original_count = len(training_samples)
            
            # ä¸ºéƒ¨åˆ†æ ·æœ¬ç”Ÿæˆå˜ä½“
            enhanced_samples = []
            for sample in training_samples[:min(1000, len(training_samples))]:  # æœ€å¤šå¢å¼º1000ä¸ªæ ·æœ¬
                variations = DataEnhancer.generate_variations(
                    sample['original'],
                    sample['rewritten'],
                    count=1
                )
                for orig_var, rew_var in variations:
                    enhanced_samples.append({
                        **sample,
                        'original': orig_var,
                        'rewritten': rew_var
                    })
            
            training_samples.extend(enhanced_samples)
            print(f"   å¢å¼ºåæ ·æœ¬æ•°: {len(training_samples)} (å¢åŠ äº† {len(training_samples) - original_count} ä¸ª)")
        
        # æ•°æ®é›†å¹³è¡¡
        if balance and training_samples:
            print(f"\nâš–ï¸  å¹³è¡¡æ•°æ®é›†...")
            original_count = len(training_samples)
            training_samples = DataEnhancer.balance_dataset(training_samples)
            print(f"   å¹³è¡¡åæ ·æœ¬æ•°: {len(training_samples)}")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰è®­ç»ƒæ ·æœ¬
        if not training_samples:
            print(f"\nâŒ è­¦å‘Š: æ²¡æœ‰ç”Ÿæˆä»»ä½•è®­ç»ƒæ ·æœ¬ï¼")
            print(f"   è¯·æ£€æŸ¥:")
            print(f"   1. å°è¯´æ–‡ä»¶æ˜¯å¦å­˜åœ¨")
            print(f"   2. å°è¯´å†…å®¹æ˜¯å¦æœ‰æ•ˆ")
            print(f"   3. æ–‡æœ¬å—é•¿åº¦æ˜¯å¦æ»¡è¶³è¦æ±‚ï¼ˆæœ€å°{self.MIN_CHUNK_LENGTH}å­—ç¬¦ï¼‰")
            return None
        
        # ä¿å­˜è®­ç»ƒæ•°æ®ï¼ˆTSVæ ¼å¼ï¼‰
        training_data_file = os.path.join(self.processed_dir, 'training_data.txt')
        
        # éªŒè¯æ ·æœ¬æ ¼å¼
        valid_samples = []
        invalid_count = 0
        for sample in training_samples:
            if not isinstance(sample, dict):
                invalid_count += 1
                continue
            
            orig = sample.get('original', '').strip()
            rew = sample.get('rewritten', '').strip()
            style = sample.get('style', 11)
            
            # éªŒè¯å¿…éœ€å­—æ®µ
            if not orig or not rew:
                invalid_count += 1
                continue
            
            # éªŒè¯é•¿åº¦
            if len(orig) < 10 or len(rew) < 10:
                invalid_count += 1
                continue
            
            # éªŒè¯é£æ ¼ID
            if not isinstance(style, int):
                try:
                    style = int(style)
                except (ValueError, TypeError):
                    invalid_count += 1
                    continue
            
            valid_samples.append(sample)
        
        if invalid_count > 0:
            print(f"   âš ï¸  è¿‡æ»¤äº† {invalid_count} ä¸ªæ— æ•ˆæ ·æœ¬")
        
        if not valid_samples:
            print(f"\nâŒ é”™è¯¯: æ‰€æœ‰è®­ç»ƒæ ·æœ¬éƒ½æ— æ•ˆï¼")
            return None
        
        # ä¿å­˜æœ‰æ•ˆçš„è®­ç»ƒæ ·æœ¬
        with open(training_data_file, 'w', encoding='utf-8') as f:
            for sample in valid_samples:
                orig = sample['original'].replace('\t', ' ').replace('\r', '').replace('\n', ' ').strip()
                rew = sample['rewritten'].replace('\t', ' ').replace('\r', '').replace('\n', ' ').strip()
                
                # é™åˆ¶é•¿åº¦
                if len(orig) > 2000:
                    orig = orig[:2000]
                if len(rew) > 2000:
                    rew = rew[:2000]
                
                # ç¡®ä¿origå’Œrewä¸ä¸ºç©º
                if not orig or not rew:
                    continue
                
                # å†™å…¥TSVæ ¼å¼ï¼šåŸå§‹æ–‡æœ¬<TAB>æ”¹å†™æ–‡æœ¬<TAB>é£æ ¼ID<TAB>ä¸Šä¸‹æ–‡ï¼ˆå¯é€‰ï¼‰
                context = sample.get('context', '')
                if context:
                    context = context.replace('\t', ' ').replace('\r', '').replace('\n', ' ')[:500]  # é™åˆ¶ä¸Šä¸‹æ–‡é•¿åº¦
                    f.write(f"{orig}\t{rew}\t{sample['style']}\t{context}\n")
                else:
                    f.write(f"{orig}\t{rew}\t{sample['style']}\n")
        
        # éªŒè¯ä¿å­˜çš„æ–‡ä»¶æ ¼å¼
        valid_lines = 0
        with open(training_data_file, 'r', encoding='utf-8') as f:
            for line in f:
                parts = line.strip().split('\t')
                if len(parts) >= 3:
                    try:
                        int(parts[2])  # éªŒè¯é£æ ¼IDæ˜¯æ•´æ•°
                        if len(parts[0]) > 10 and len(parts[1]) > 10:
                            valid_lines += 1
                    except ValueError:
                        pass
        
        print(f"\nâœ… ç”Ÿæˆå®Œæˆï¼Œå…± {len(valid_samples)} æ¡æœ‰æ•ˆè®­ç»ƒæ ·æœ¬")
        print(f"   æ–‡ä»¶: {training_data_file}")
        print(f"   éªŒè¯: {valid_lines}/{len(valid_samples)} æ¡æ ¼å¼æ­£ç¡®")
        
        if valid_lines < len(valid_samples) * 0.9:
            print(f"   âš ï¸  è­¦å‘Š: éƒ¨åˆ†æ•°æ®æ ¼å¼å¯èƒ½æœ‰é—®é¢˜")
        
        # ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯
        stats = {
            'total_samples': len(training_samples),
            'sites': {},
            'categories': {}
        }
        
        for sample in training_samples:
            site = sample['site']
            category = sample['category']
            
            if site not in stats['sites']:
                stats['sites'][site] = 0
            stats['sites'][site] += 1
            
            if category not in stats['categories']:
                stats['categories'][category] = 0
            stats['categories'][category] += 1
        
        stats_file = os.path.join(self.processed_dir, 'training_stats.json')
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(stats, f, ensure_ascii=False, indent=2)
        
        print(f"   ç»Ÿè®¡: {stats_file}")
        
        return training_data_file
    
    def _split_into_chunks(self, text: str) -> List[str]:
        """
        å°†æ–‡æœ¬åˆ†å‰²æˆå¤šä¸ªå—ï¼ˆç”¨äºæ•°æ®å¢å¼ºï¼‰
        
        Args:
            text: åŸå§‹æ–‡æœ¬
        
        Returns:
            æ–‡æœ¬å—åˆ—è¡¨
        """
        chunks = []
        
        # æŒ‰æ®µè½åˆ†å‰²
        paragraphs = re.split(r'\n\s*\n', text)
        
        current_chunk = ""
        for para in paragraphs:
            para = para.strip()
            if not para:
                continue
            
            # å¦‚æœå½“å‰å—åŠ ä¸Šæ–°æ®µè½è¶…è¿‡æœ€å¤§é•¿åº¦ï¼Œä¿å­˜å½“å‰å—
            if len(current_chunk) + len(para) > self.MAX_CHUNK_LENGTH:
                if len(current_chunk) >= self.MIN_CHUNK_LENGTH:
                    chunks.append(current_chunk)
                # å¼€å§‹æ–°å—ï¼ˆä¿ç•™é‡å éƒ¨åˆ†ï¼‰
                if self.CHUNK_OVERLAP > 0 and len(current_chunk) > self.CHUNK_OVERLAP:
                    current_chunk = current_chunk[-self.CHUNK_OVERLAP:] + "\n\n" + para
                else:
                    current_chunk = para
            else:
                if current_chunk:
                    current_chunk += "\n\n" + para
                else:
                    current_chunk = para
        
        # æ·»åŠ æœ€åä¸€ä¸ªå—
        if len(current_chunk) >= self.MIN_CHUNK_LENGTH:
            chunks.append(current_chunk)
        
        # å¦‚æœåªæœ‰ä¸€ä¸ªå—ï¼Œå°è¯•è¿›ä¸€æ­¥åˆ†å‰²
        if len(chunks) == 1 and len(chunks[0]) > self.MAX_CHUNK_LENGTH:
            # æŒ‰å¥å­åˆ†å‰²
            sentences = re.split(r'[ã€‚ï¼ï¼Ÿ\n]', chunks[0])
            chunks = []
            current_chunk = ""
            for sent in sentences:
                sent = sent.strip()
                if not sent:
                    continue
                if len(current_chunk) + len(sent) > self.MAX_CHUNK_LENGTH:
                    if len(current_chunk) >= self.MIN_CHUNK_LENGTH:
                        chunks.append(current_chunk)
                    current_chunk = sent
                else:
                    current_chunk += sent
        
        return chunks if chunks else [text]
    
    def _split_into_chunks_with_context(self, text: str, prev_context: str, 
                                        next_context: str, novel_context: Dict) -> List[Dict]:
        """
        å°†æ–‡æœ¬åˆ†å‰²æˆå¤šä¸ªå—ï¼Œå¹¶åŒ…å«ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆå¢å¼ºç‰ˆï¼‰
        
        Args:
            text: åŸå§‹æ–‡æœ¬
            prev_context: å‰æ–‡ä¸Šä¸‹æ–‡
            next_context: åæ–‡ä¸Šä¸‹æ–‡
            novel_context: æ•´æœ¬å°è¯´çš„ä¸Šä¸‹æ–‡ä¿¡æ¯
        
        Returns:
            åŒ…å«æ–‡æœ¬å’Œä¸Šä¸‹æ–‡çš„å­—å…¸åˆ—è¡¨
        """
        chunks = []
        
        # æŒ‰æ®µè½åˆ†å‰²
        paragraphs = re.split(r'\n\s*\n', text)
        
        current_chunk = ""
        for para in paragraphs:
            para = para.strip()
            if not para:
                continue
            
            # å¦‚æœå½“å‰å—åŠ ä¸Šæ–°æ®µè½è¶…è¿‡æœ€å¤§é•¿åº¦ï¼Œä¿å­˜å½“å‰å—
            if len(current_chunk) + len(para) > self.MAX_CHUNK_LENGTH:
                if len(current_chunk) >= self.MIN_CHUNK_LENGTH:
                    # æ„å»ºä¸Šä¸‹æ–‡ä¿¡æ¯
                    context_info = self._build_context_info(
                        current_chunk, prev_context, next_context, novel_context
                    )
                    chunks.append({
                        'text': current_chunk,
                        'context': context_info
                    })
                # å¼€å§‹æ–°å—ï¼ˆä¿ç•™é‡å éƒ¨åˆ†ï¼‰
                if self.CHUNK_OVERLAP > 0 and len(current_chunk) > self.CHUNK_OVERLAP:
                    current_chunk = current_chunk[-self.CHUNK_OVERLAP:] + "\n\n" + para
                else:
                    current_chunk = para
            else:
                if current_chunk:
                    current_chunk += "\n\n" + para
                else:
                    current_chunk = para
        
        # æ·»åŠ æœ€åä¸€ä¸ªå—
        if len(current_chunk) >= self.MIN_CHUNK_LENGTH:
            context_info = self._build_context_info(
                current_chunk, prev_context, next_context, novel_context
            )
            chunks.append({
                'text': current_chunk,
                'context': context_info
            })
        
        # å¦‚æœåªæœ‰ä¸€ä¸ªå—ï¼Œå°è¯•è¿›ä¸€æ­¥åˆ†å‰²
        if len(chunks) == 1 and len(chunks[0]['text']) > self.MAX_CHUNK_LENGTH:
            # æŒ‰å¥å­åˆ†å‰²
            sentences = re.split(r'[ã€‚ï¼ï¼Ÿ\n]', chunks[0]['text'])
            chunks = []
            current_chunk = ""
            for sent in sentences:
                sent = sent.strip()
                if not sent:
                    continue
                if len(current_chunk) + len(sent) > self.MAX_CHUNK_LENGTH:
                    if len(current_chunk) >= self.MIN_CHUNK_LENGTH:
                        context_info = self._build_context_info(
                            current_chunk, prev_context, next_context, novel_context
                        )
                        chunks.append({
                            'text': current_chunk,
                            'context': context_info
                        })
                    current_chunk = sent
                else:
                    current_chunk += sent
        
        return chunks if chunks else [{'text': text, 'context': ''}]
    
    def _extract_novel_context(self, content: str) -> Dict:
        """
        æå–æ•´æœ¬å°è¯´çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆäººç‰©ã€æƒ…èŠ‚ã€ä¸»é¢˜ç­‰ï¼‰
        
        Args:
            content: å°è¯´å†…å®¹
        
        Returns:
            ä¸Šä¸‹æ–‡ä¿¡æ¯å­—å…¸
        """
        context = {
            'characters': [],
            'main_plot': '',
            'themes': [],
            'setting': ''
        }
        
        # æå–ä¸»è¦äººç‰©ï¼ˆç®€å•æ–¹æ³•ï¼šé«˜é¢‘å‡ºç°çš„2-3å­—è¯ï¼‰
        import re
        from collections import Counter
        
        # æå–å¯èƒ½çš„å§“åï¼ˆ2-3ä¸ªä¸­æ–‡å­—ç¬¦ï¼‰
        name_pattern = r'[\u4e00-\u9fa5]{2,3}'
        potential_names = re.findall(name_pattern, content[:10000])  # åªåˆ†æå‰10000å­—ç¬¦
        name_counter = Counter(potential_names)
        
        # æ’é™¤å¸¸è§è¯
        exclude_words = {'å¤§å®¶', 'è‡ªå·±', 'ä»–ä»¬', 'æˆ‘ä»¬', 'ä½ ä»¬', 'å¥¹ä»¬', 'å®ƒä»¬', 
                        'ä»€ä¹ˆ', 'æ€ä¹ˆ', 'è¿™æ ·', 'é‚£æ ·', 'è¿™ä¸ª', 'é‚£ä¸ª', 'è¿™äº›', 'é‚£äº›',
                        'ä»Šå¤©', 'æ˜å¤©', 'æ˜¨å¤©', 'ç°åœ¨', 'ä»¥å', 'ä¹‹å‰', 'ä¹‹å'}
        
        # è·å–å‰10ä¸ªé«˜é¢‘è¯ä½œä¸ºä¸»è¦äººç‰©
        for name, count in name_counter.most_common(20):
            if name not in exclude_words and count >= 5:
                context['characters'].append(name)
                if len(context['characters']) >= 10:
                    break
        
        # æå–ä¸»é¢˜å…³é”®è¯ï¼ˆç®€åŒ–ç‰ˆï¼‰
        theme_keywords = {
            'éƒ½å¸‚': ['éƒ½å¸‚', 'åŸå¸‚', 'å…¬å¸', 'èŒåœº'],
            'ç„å¹»': ['ä¿®ç‚¼', 'å¢ƒç•Œ', 'åŠŸæ³•', 'ä¸¹è¯'],
            'è¨€æƒ…': ['çˆ±æƒ…', 'æ‹çˆ±', 'ç»“å©š', 'æ„Ÿæƒ…'],
            'æ­¦ä¾ ': ['æ­¦åŠŸ', 'æ±Ÿæ¹–', 'é—¨æ´¾', 'å‰‘æ³•'],
        }
        
        # æ£€æµ‹ä¸»é¢˜
        for theme, keywords in theme_keywords.items():
            if any(kw in content[:5000] for kw in keywords):
                context['themes'].append(theme)
        
        return context
    
    def _build_context_info(self, text: str, prev_context: str, 
                           next_context: str, novel_context: Dict) -> str:
        """
        æ„å»ºä¸Šä¸‹æ–‡ä¿¡æ¯å­—ç¬¦ä¸²
        
        Args:
            text: å½“å‰æ–‡æœ¬
            prev_context: å‰æ–‡
            next_context: åæ–‡
            novel_context: å°è¯´ä¸Šä¸‹æ–‡
        
        Returns:
            ä¸Šä¸‹æ–‡ä¿¡æ¯å­—ç¬¦ä¸²
        """
        context_parts = []
        
        # æ·»åŠ å°è¯´çº§åˆ«ä¸Šä¸‹æ–‡
        if novel_context.get('characters'):
            context_parts.append(f"ä¸»è¦äººç‰©: {', '.join(novel_context['characters'][:5])}")
        if novel_context.get('themes'):
            context_parts.append(f"ä¸»é¢˜: {', '.join(novel_context['themes'][:3])}")
        
        # æ·»åŠ å‰æ–‡ä¸Šä¸‹æ–‡ï¼ˆæ‘˜è¦ï¼‰
        if prev_context:
            prev_summary = prev_context[-200:] if len(prev_context) > 200 else prev_context
            context_parts.append(f"å‰æ–‡: {prev_summary}")
        
        # æ·»åŠ åæ–‡ä¸Šä¸‹æ–‡ï¼ˆé¢„è§ˆï¼‰
        if next_context:
            next_preview = next_context[:200] if len(next_context) > 200 else next_context
            context_parts.append(f"åæ–‡: {next_preview}")
        
        return " | ".join(context_parts)
    
    def _get_style_id(self, category: str) -> int:
        """
        æ ¹æ®åˆ†ç±»è·å–é£æ ¼ID
        
        Args:
            category: å°è¯´åˆ†ç±»
        
        Returns:
            é£æ ¼ID
        """
        style_map = {
            'éƒ½å¸‚': 11,
            'ç„å¹»': 8,
            'è¨€æƒ…': 5,
            'æ­¦ä¾ ': 9,
            'ç§‘å¹»': 8,
            'æ‚¬ç–‘': 4,
            'å†å²': 1,
            'å†›äº‹': 7,
            'æ¸¸æˆ': 12,
            'ç«æŠ€': 13,
            'ä»™ä¾ ': 10,
            'å…¶ä»–': 11,
            'æœªçŸ¥': 11,
        }
        return style_map.get(category, 11)
    
    def _generate_rewritten_text(self, 
                                 original: str, 
                                 style_id: int,
                                 context: str = "",
                                 use_ai: bool = False) -> str:
        """
        ç”Ÿæˆæ”¹å†™æ–‡æœ¬ï¼ˆæ”¯æŒAIæ”¹å†™ï¼‰
        
        Args:
            original: åŸå§‹æ–‡æœ¬
            style_id: é£æ ¼ID
            context: ä¸Šä¸‹æ–‡ä¿¡æ¯
            use_ai: æ˜¯å¦ä½¿ç”¨AIæ”¹å†™
        
        Returns:
            æ”¹å†™åçš„æ–‡æœ¬
        """
        if not use_ai:
            # å¦‚æœä¸ä½¿ç”¨AIï¼Œä½¿ç”¨è§„åˆ™æ”¹å†™
            return self._rule_based_rewrite(original, style_id)
        
        # å°è¯•ä½¿ç”¨AIæ”¹å†™
        try:
            # å¯¼å…¥AIæ¨¡å—
            import sys
            import os
            ai_path = os.path.join(os.path.dirname(__file__), '..', 'ai', 'models')
            if ai_path not in sys.path:
                sys.path.insert(0, ai_path)
            
            from tensorflow_model import TensorFlowTextRewriter
            
            # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨
            model_path = "models/text_rewriter_model"
            if os.path.exists(os.path.join(model_path, 'vocab.json')):
                try:
                    rewriter = TensorFlowTextRewriter(model_path=model_path)
                    if rewriter.load_vocab() and rewriter.load_model():
                        # ä½¿ç”¨æ¨¡å‹æ”¹å†™
                        rewritten = rewriter.rewrite(
                            original, 
                            style=style_id,
                            context=context[:200] if context else None,
                            temperature=0.7
                        )
                        if rewritten and rewritten != original and len(rewritten) > len(original) * 0.5:
                            return rewritten
                except Exception:
                    # AIæ”¹å†™å¤±è´¥ï¼Œä½¿ç”¨é™çº§æ–¹æ¡ˆ
                    pass
        except ImportError:
            # AIæ¨¡å—ä¸å¯ç”¨
            pass
        except Exception:
            # å…¶ä»–é”™è¯¯
            pass
        
        # é™çº§æ–¹æ¡ˆï¼šä½¿ç”¨ç®€å•çš„è§„åˆ™æ”¹å†™ï¼ˆåŸºäºé£æ ¼ï¼‰
        return self._rule_based_rewrite(original, style_id)
    
    def _rule_based_rewrite(self, text: str, style_id: int) -> str:
        """
        åŸºäºè§„åˆ™çš„ç®€å•æ”¹å†™ï¼ˆé™çº§æ–¹æ¡ˆï¼‰
        
        Args:
            text: åŸå§‹æ–‡æœ¬
            style_id: é£æ ¼ID
        
        Returns:
            æ”¹å†™åçš„æ–‡æœ¬
        """
        import re
        
        # é£æ ¼æ˜ å°„åˆ°è§„åˆ™
        style_rules = {
            11: {'åŸå¸‚': 'éƒ½å¸‚', 'åœ°æ–¹': 'éƒ½å¸‚'},  # éƒ½å¸‚
            6: {'å¾ˆ': 'è¶…çº§', 'éå¸¸': 'è¶…çº§', 'å¥½': 'æ£’æäº†'},  # å¹½é»˜
            18: {'åŸå¸‚': 'éƒ½å¸‚', 'å¾ˆ': 'è¶…çº§', 'éå¸¸': 'è¶…çº§'},  # éƒ½å¸‚å¹½é»˜
        }
        
        rules = style_rules.get(style_id, {})
        result = text
        
        # åº”ç”¨è§„åˆ™ï¼ˆé€‚åº¦ï¼Œé¿å…è¿‡åº¦æ›¿æ¢ï¼‰
        for old, new in rules.items():
            # ä½¿ç”¨å•è¯è¾¹ç•Œï¼Œé¿å…éƒ¨åˆ†åŒ¹é…
            pattern = r'\b' + re.escape(old) + r'\b'
            # åªæ›¿æ¢å‰å‡ æ¬¡ï¼Œé¿å…è¿‡åº¦
            count = min(3, text.count(old) // 3) if text.count(old) > 0 else 0
            if count > 0:
                result = re.sub(pattern, new, result, count=count)
        
        return result if result != text else text
    
    def _escape_tsv(self, text: str) -> str:
        """
        è½¬ä¹‰TSVæ ¼å¼ä¸­çš„ç‰¹æ®Šå­—ç¬¦
        
        Args:
            text: åŸå§‹æ–‡æœ¬
        
        Returns:
            è½¬ä¹‰åçš„æ–‡æœ¬
        """
        # æ›¿æ¢åˆ¶è¡¨ç¬¦å’Œæ¢è¡Œç¬¦
        text = text.replace('\t', ' ').replace('\r', '').replace('\n', ' ')
        # ç§»é™¤å¤šä½™ç©ºç™½
        text = re.sub(r' +', ' ', text)
        return text.strip()

