#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç»Ÿä¸€çš„æ•°æ®å¤„ç†æµæ°´çº¿
æ•´åˆçˆ¬å–ã€æ•´ç†ã€ç”Ÿæˆè®­ç»ƒæ•°æ®ã€è®­ç»ƒæ¨¡å‹ç­‰å®Œæ•´æµç¨‹
"""

import os
import sys
import argparse
from typing import Optional, List

# æ·»åŠ è·¯å¾„
sys.path.insert(0, os.path.dirname(__file__))
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

from scraper.multi_site_scraper import MultiSiteScraper
from scraper.novel_analyzer import NovelAnalyzer
from core.training_data_generator import TrainingDataGenerator
from ai.models.train_model import main as train_main
from ai.models.incremental_train import IncrementalTrainer

# å¯é€‰å¯¼å…¥ï¼šæ•°æ®æ•´ç†åŠŸèƒ½
try:
    from utils.data_organizer import DataOrganizer
    HAS_DATA_ORGANIZER = True
except ImportError:
    HAS_DATA_ORGANIZER = False


class DataPipeline:
    """ç»Ÿä¸€çš„æ•°æ®å¤„ç†æµæ°´çº¿"""
    
    def __init__(self, output_dir: str = "data/training"):
        """
        åˆå§‹åŒ–æµæ°´çº¿
        
        Args:
            output_dir: è¾“å‡ºç›®å½•
        """
        self.output_dir = os.path.abspath(output_dir)
        self.novels_dir = os.path.join(self.output_dir, 'novels')
        self.processed_dir = os.path.join(self.output_dir, 'processed')
        # ç»Ÿä¸€æ¨¡å‹è·¯å¾„ï¼šä½¿ç”¨ç»å¯¹è·¯å¾„ï¼Œç¡®ä¿è·¯å¾„æ­£ç¡®
        model_path_relative = os.path.join(self.output_dir, '..', 'models', 'text_rewriter_model')
        self.model_path = os.path.abspath(os.path.normpath(model_path_relative))
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.scraper = MultiSiteScraper(output_dir)
        self.analyzer = NovelAnalyzer()
        self.data_generator = TrainingDataGenerator(output_dir)
        
        # å¯¼å…¥æ–°çš„æ™ºèƒ½å¤„ç†æ¨¡å—
        try:
            import sys as sys_module
            core_path = os.path.dirname(__file__)
            if core_path not in sys_module.path:
                sys_module.path.insert(0, core_path)
            
            from data_processor import DataProcessor
            from enhanced_training_data_generator import EnhancedTrainingDataGenerator
            self.data_processor = DataProcessor(output_dir)
            self.enhanced_generator = EnhancedTrainingDataGenerator(output_dir)
        except ImportError:
            self.data_processor = None
            self.enhanced_generator = None
            # é™é»˜å¤±è´¥ï¼Œä¸å½±å“ä¸»æµç¨‹
            pass
    
    def step1_scrape(self, site_name: str, category: str, count: int, 
                     filter_completed: bool = True) -> bool:
        """
        æ­¥éª¤1: çˆ¬å–å°è¯´
        
        Args:
            site_name: ç½‘ç«™åç§°
            category: å°è¯´ç±»å‹
            count: çˆ¬å–æ•°é‡
            filter_completed: æ˜¯å¦åªçˆ¬å–å·²å®Œç»“çš„
        
        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        print("\n" + "="*60)
        print("æ­¥éª¤ 1/5: çˆ¬å–å°è¯´")
        print("="*60)
        
        stats = self.scraper.batch_scrape(site_name, category, count, filter_completed)
        
        if stats['success'] > 0:
            print(f"\nâœ… çˆ¬å–å®Œæˆ: æˆåŠŸ {stats['success']} æœ¬ï¼Œå¤±è´¥ {stats['failed']} æœ¬")
            return True
        else:
            print(f"\nâŒ çˆ¬å–å¤±è´¥: æ²¡æœ‰æˆåŠŸçˆ¬å–ä»»ä½•å°è¯´")
            return False
    
    def step2_organize(self, organize_data: bool = False) -> bool:
        """
        æ­¥éª¤2: æ•´ç†æ•°æ®ï¼ˆå¯é€‰ï¼‰æˆ–åˆ†æå°è¯´
        
        Args:
            organize_data: æ˜¯å¦å…ˆæ•´ç†æ•°æ®ï¼ˆæ¸…ç†ã€åˆ†ç±»ç­‰ï¼‰
        
        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        if organize_data and HAS_DATA_ORGANIZER:
            print("\n" + "="*60)
            print("æ­¥éª¤ 2/6: æ•´ç†æ•°æ®")
            print("="*60)
            
            organizer = DataOrganizer(self.novels_dir, self.processed_dir)
            summary = organizer.organize()
            
            if summary['stats']['processed_files'] > 0:
                print(f"\nâœ… æ•°æ®æ•´ç†å®Œæˆ")
                # æ•´ç†åï¼Œä½¿ç”¨processed_dirä½œä¸ºæ•°æ®æº
                data_source = self.processed_dir
            else:
                print(f"\nâš ï¸  æ•°æ®æ•´ç†å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æ•°æ®")
                data_source = self.novels_dir
        else:
            data_source = self.novels_dir
        
        print("\n" + "="*60)
        print("æ­¥éª¤ 2/6: åˆ†æå°è¯´ç‰¹å¾")
        print("="*60)
        
        results = self.analyzer.analyze_batch(data_source)
        
        if results:
            summary = self.analyzer.generate_summary()
            analysis_file = os.path.join(self.processed_dir, 'analysis.json')
            self.analyzer.save_analysis(analysis_file)
            
            print(f"\nâœ… åˆ†æå®Œæˆ:")
            print(f"   æ€»å°è¯´æ•°: {summary.get('total_novels', 0)}")
            print(f"   æ€»å­—ç¬¦æ•°: {summary.get('total_chars', 0):,}")
            return True
        else:
            print(f"\nâš ï¸  åˆ†æå¤±è´¥: æ²¡æœ‰æ‰¾åˆ°å¯åˆ†æçš„å°è¯´")
            return False
    
    def step3_generate_training_data(self, use_ai: bool = False, 
                                     source_dir: Optional[str] = None) -> bool:
        """
        æ­¥éª¤3: ç”Ÿæˆè®­ç»ƒæ•°æ®
        
        Args:
            use_ai: æ˜¯å¦ä½¿ç”¨AIç”Ÿæˆæ”¹å†™æ ·æœ¬
        
        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        print("\n" + "="*60)
        print("æ­¥éª¤ 3/6: ç”Ÿæˆè®­ç»ƒæ•°æ®")
        print("="*60)
        
        # å¦‚æœæŒ‡å®šäº†æºç›®å½•ï¼Œä¸´æ—¶ä¿®æ”¹ç”Ÿæˆå™¨çš„novels_dir
        # å¦‚æœä½¿ç”¨processedç›®å½•ï¼Œæä¾›novelsç›®å½•ä½œä¸ºå›é€€
        if source_dir:
            original_novels_dir = self.data_generator.novels_dir
            self.data_generator.novels_dir = source_dir
            # å¦‚æœsource_diræ˜¯processedï¼Œæä¾›novelsä½œä¸ºå›é€€
            fallback_dir = self.novels_dir if 'processed' in source_dir else None
        else:
            fallback_dir = None
        
        training_file = self.data_generator.generate_from_novels(use_ai=use_ai, fallback_dir=fallback_dir)
        
        # æ¢å¤åŸå§‹ç›®å½•
        if source_dir:
            self.data_generator.novels_dir = original_novels_dir
        
        if training_file and os.path.exists(training_file):
            with open(training_file, 'r', encoding='utf-8') as f:
                lines = [line for line in f if line.strip()]
            
            print(f"\nâœ… è®­ç»ƒæ•°æ®ç”Ÿæˆå®Œæˆ: {len(lines)} æ¡æ ·æœ¬")
            return len(lines) > 0
        else:
            print(f"\nâŒ è®­ç»ƒæ•°æ®ç”Ÿæˆå¤±è´¥")
            return False
    
    def step4_train(self, epochs: int = 20, batch_size: int = 16, 
                    incremental: bool = False) -> bool:
        """
        æ­¥éª¤4: è®­ç»ƒæ¨¡å‹
        
        Args:
            epochs: è®­ç»ƒè½®æ•°
            batch_size: æ‰¹æ¬¡å¤§å°
            incremental: æ˜¯å¦å¢é‡è®­ç»ƒ
        
        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        print("\n" + "="*60)
        print(f"æ­¥éª¤ 4/5: {'å¢é‡' if incremental else 'åŸºç¡€'}è®­ç»ƒæ¨¡å‹")
        print("="*60)
        
        training_file = os.path.join(self.processed_dir, 'training_data.txt')
        
        if not os.path.exists(training_file):
            print(f"âŒ è®­ç»ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {training_file}")
            return False
        
        # æ£€æŸ¥æ•°æ®é‡
        with open(training_file, 'r', encoding='utf-8') as f:
            lines = [line for line in f if line.strip()]
        
        if len(lines) < 10:
            print(f"âš ï¸  è­¦å‘Š: è®­ç»ƒæ•°æ®é‡è¾ƒå°‘ ({len(lines)} æ¡)")
            return False
        
        if incremental:
            if IncrementalTrainer is None:
                print(f"âŒ æ— æ³•å¯¼å…¥IncrementalTrainerï¼Œè¯·æ£€æŸ¥ä¾èµ–")
                return False
            
            # å¢é‡è®­ç»ƒ
            trainer = IncrementalTrainer(self.model_path)
            if trainer.load_existing_model():
                success = trainer.incremental_train(training_file, epochs=epochs)
                if success:
                    trainer.merge_models(keep_best=True)
                return success
            else:
                print(f"âš ï¸  æœªæ‰¾åˆ°å·²æœ‰æ¨¡å‹ï¼Œå°†è¿›è¡ŒåŸºç¡€è®­ç»ƒ")
                incremental = False
        
        if not incremental:
            # åŸºç¡€è®­ç»ƒ
            if train_main is None:
                print(f"âŒ æ— æ³•å¯¼å…¥train_modelï¼Œè¯·æ£€æŸ¥ä¾èµ–")
                return False
            
            old_argv = sys.argv
            try:
                sys.argv = [
                    'train_model.py',
                    training_file,
                    '--model-path', self.model_path,
                    '--epochs', str(epochs),
                    '--batch-size', str(batch_size)
                ]
                train_main()
                
                # éªŒè¯æ¨¡å‹æ˜¯å¦æˆåŠŸç”Ÿæˆ
                model_files = [
                    os.path.join(self.model_path, 'best_model.h5'),
                    os.path.join(self.model_path, 'final_model.h5'),
                    os.path.join(self.model_path, 'vocab.json')
                ]
                
                all_exist = all(os.path.exists(f) for f in model_files)
                if all_exist:
                    print(f"\nâœ… æ¨¡å‹å·²æˆåŠŸç”Ÿæˆ:")
                    print(f"   æ¨¡å‹è·¯å¾„: {self.model_path}")
                    for f in model_files:
                        size = os.path.getsize(f) / 1024 / 1024
                        print(f"   - {os.path.basename(f)} ({size:.2f} MB)")
                    return True
                else:
                    print(f"\nâš ï¸  è­¦å‘Š: éƒ¨åˆ†æ¨¡å‹æ–‡ä»¶æœªç”Ÿæˆ")
                    print(f"   æœŸæœ›è·¯å¾„: {self.model_path}")
                    missing = [f for f in model_files if not os.path.exists(f)]
                    print(f"   ç¼ºå¤±æ–‡ä»¶: {[os.path.basename(f) for f in missing]}")
                    return False
                    
            except Exception as e:
                print(f"\nâŒ è®­ç»ƒå¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
                return False
            finally:
                sys.argv = old_argv
        
        return False
    
    def run_full_pipeline(self, site_name: Optional[str] = None, 
                         category: Optional[str] = None, 
                         count: int = 10,
                         use_ai: bool = False,
                         epochs: int = 20,
                         batch_size: int = 16,
                         incremental: bool = False,
                         organize_data: bool = False,
                         skip_steps: Optional[List[str]] = None) -> bool:
        """
        è¿è¡Œå®Œæ•´æµæ°´çº¿
        
        Args:
            site_name: ç½‘ç«™åç§°
            category: å°è¯´ç±»å‹
            count: çˆ¬å–æ•°é‡
            use_ai: æ˜¯å¦ä½¿ç”¨AIç”Ÿæˆæ”¹å†™æ ·æœ¬
            epochs: è®­ç»ƒè½®æ•°
            batch_size: æ‰¹æ¬¡å¤§å°
            incremental: æ˜¯å¦å¢é‡è®­ç»ƒ
            skip_steps: è·³è¿‡çš„æ­¥éª¤åˆ—è¡¨
        
        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        skip_steps = skip_steps or []
        
        print("\n" + "="*60)
        print("ğŸš€ å®Œæ•´æ•°æ®å¤„ç†æµæ°´çº¿")
        print("="*60)
        print(f"\né…ç½®:")
        if site_name and category:
            print(f"  ç½‘ç«™: {site_name}")
            print(f"  ç±»å‹: {category}")
            print(f"  æ•°é‡: {count} æœ¬")
        else:
            print(f"  æ¨¡å¼: æ•°æ®æ•´ç†æ¨¡å¼ï¼ˆå·²æœ‰æ•°æ®ï¼‰")
        print(f"  ä½¿ç”¨AI: {'æ˜¯' if use_ai else 'å¦'}")
        print(f"  è®­ç»ƒæ¨¡å¼: {'å¢é‡' if incremental else 'åŸºç¡€'}")
        print(f"  è®­ç»ƒè½®æ•°: {epochs}")
        if organize_data:
            print(f"  æ•°æ®æ•´ç†: æ˜¯")
        
        # æ­¥éª¤1: çˆ¬å–ï¼ˆä»…å½“æä¾›äº†site_nameå’Œcategoryæ—¶ï¼‰
        if site_name and category and 'scrape' not in skip_steps:
            if not self.step1_scrape(site_name, category, count):
                print("\nâš ï¸  çˆ¬å–æ­¥éª¤å¤±è´¥ï¼Œä½†ç»§ç»­åç»­æ­¥éª¤...")
        elif site_name and category:
            print("\nâ­ï¸  è·³è¿‡çˆ¬å–æ­¥éª¤")
        
        # æ­¥éª¤1.5: ç»“æ„åŒ–æ•°æ®å¤„ç†ï¼ˆåœ¨çˆ¬å–åç«‹å³æ‰§è¡Œï¼‰
        if 'structure' not in skip_steps and 'scrape' not in skip_steps and site_name and category:
            if not self.step1_5_structure_data(category=category, site=site_name):
                print("\nâš ï¸  ç»“æ„åŒ–å¤„ç†å¤±è´¥ï¼Œç»§ç»­æ‰§è¡Œåç»­æ­¥éª¤")
        else:
            print("\nâ­ï¸  è·³è¿‡ç»“æ„åŒ–å¤„ç†æ­¥éª¤")
        
        # æ­¥éª¤2: æ•´ç†æ•°æ®æˆ–åˆ†æ
        if 'organize' not in skip_steps and 'analyze' not in skip_steps:
            # å¦‚æœå¯ç”¨äº†æ•°æ®æ•´ç†ï¼Œå…ˆæ•´ç†å†åˆ†æ
            if organize_data:
                self.step2_organize(organize_data=True)
            else:
                self.step2_organize(organize_data=False)
        else:
            if 'organize' in skip_steps:
                print("\nâ­ï¸  è·³è¿‡æ•°æ®æ•´ç†æ­¥éª¤")
            if 'analyze' in skip_steps:
                print("\nâ­ï¸  è·³è¿‡åˆ†ææ­¥éª¤")
        
        # æ­¥éª¤3: ç”Ÿæˆè®­ç»ƒæ•°æ®
        if 'generate' not in skip_steps:
            # ä¼˜å…ˆä½¿ç”¨å¢å¼ºç‰ˆç”Ÿæˆå™¨ï¼ˆå¦‚æœç»“æ„åŒ–æ•°æ®å­˜åœ¨ï¼‰
            if self.enhanced_generator and os.path.exists(os.path.join(self.output_dir, 'structured')):
                print("\n" + "="*60)
                print("æ­¥éª¤ 3/6: ç”Ÿæˆè®­ç»ƒæ•°æ®ï¼ˆå¢å¼ºç‰ˆï¼‰")
                print("="*60)
                
                training_file = self.enhanced_generator.generate_from_structured_data(use_ai=use_ai)
                if training_file and os.path.exists(training_file):
                    with open(training_file, 'r', encoding='utf-8') as f:
                        lines = [line for line in f if line.strip()]
                    print(f"\nâœ… è®­ç»ƒæ•°æ®ç”Ÿæˆå®Œæˆ: {len(lines)} æ¡æ ·æœ¬")
                else:
                    print("\nâš ï¸  å¢å¼ºç‰ˆç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•")
                    # å›é€€åˆ°ä¼ ç»Ÿæ–¹æ³•
                    source_dir = self.processed_dir if organize_data and HAS_DATA_ORGANIZER else None
                    if not self.step3_generate_training_data(use_ai=use_ai, source_dir=source_dir):
                        print("\nâŒ ç”Ÿæˆè®­ç»ƒæ•°æ®å¤±è´¥ï¼Œæ— æ³•ç»§ç»­è®­ç»ƒ")
                        return False
            else:
                # ä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•
                source_dir = self.processed_dir if organize_data and HAS_DATA_ORGANIZER else None
                if not self.step3_generate_training_data(use_ai=use_ai, source_dir=source_dir):
                    print("\nâŒ ç”Ÿæˆè®­ç»ƒæ•°æ®å¤±è´¥ï¼Œæ— æ³•ç»§ç»­è®­ç»ƒ")
                    return False
        else:
            print("\nâ­ï¸  è·³è¿‡ç”Ÿæˆæ­¥éª¤")
        
        # æ­¥éª¤4: è®­ç»ƒ
        if 'train' not in skip_steps:
            if not self.step4_train(epochs=epochs, batch_size=batch_size, incremental=incremental):
                print("\nâŒ è®­ç»ƒå¤±è´¥")
                return False
        else:
            print("\nâ­ï¸  è·³è¿‡è®­ç»ƒæ­¥éª¤")
        
        # æœ€ç»ˆéªŒè¯ï¼šæ£€æŸ¥å…³é”®è¾“å‡º
        print("\n" + "="*60)
        print("ğŸ“‹ æœ€ç»ˆéªŒè¯")
        print("="*60)
        
        # æ£€æŸ¥è®­ç»ƒæ•°æ®
        training_file = os.path.join(self.processed_dir, 'training_data.txt')
        if os.path.exists(training_file):
            with open(training_file, 'r', encoding='utf-8') as f:
                lines = [line for line in f if line.strip()]
            print(f"âœ… è®­ç»ƒæ•°æ®: {len(lines)} æ¡æ ·æœ¬")
        else:
            print(f"âš ï¸  è®­ç»ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨")
        
        # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
        if 'train' not in skip_steps:
            model_files = [
                os.path.join(self.model_path, 'best_model.h5'),
                os.path.join(self.model_path, 'final_model.h5'),
                os.path.join(self.model_path, 'vocab.json')
            ]
            existing = [f for f in model_files if os.path.exists(f)]
            if existing:
                print(f"âœ… æ¨¡å‹æ–‡ä»¶: {len(existing)}/{len(model_files)} ä¸ªæ–‡ä»¶å·²ç”Ÿæˆ")
                print(f"   æ¨¡å‹è·¯å¾„: {self.model_path}")
            else:
                print(f"âŒ æ¨¡å‹æ–‡ä»¶: æœªæ‰¾åˆ°æ¨¡å‹æ–‡ä»¶")
                print(f"   æœŸæœ›è·¯å¾„: {self.model_path}")
        
        print("\n" + "="*60)
        print("âœ… å®Œæ•´æµæ°´çº¿æ‰§è¡Œå®Œæˆï¼")
        print("="*60)
        
        return True


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description='ç»Ÿä¸€çš„æ•°æ®å¤„ç†æµæ°´çº¿',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # å®Œæ•´æµç¨‹ï¼ˆçˆ¬å– â†’ åˆ†æ â†’ ç”Ÿæˆ â†’ è®­ç»ƒï¼‰
  python3 scripts/core/pipeline.py --site m.shuhaige.net --category éƒ½å¸‚ --count 10
  
  # æ•°æ®æ•´ç†æ¨¡å¼ï¼ˆå·²æœ‰æ•°æ®ï¼‰
  python3 scripts/core/pipeline.py --organize --skip scrape
  
  # åªè®­ç»ƒï¼ˆå·²æœ‰æ•°æ®ï¼‰
  python3 scripts/core/pipeline.py --skip scrape,analyze,generate
  
  # å¢é‡è®­ç»ƒ
  python3 scripts/core/pipeline.py --incremental --skip scrape,analyze,generate
        """
    )
    
    parser.add_argument('--site', type=str, default=None,
                       help='ç½‘ç«™åç§°ï¼ˆå¦‚ï¼šm.shuhaige.netï¼‰ï¼Œä¸æä¾›åˆ™ä½¿ç”¨å·²æœ‰æ•°æ®')
    parser.add_argument('--category', type=str, default=None,
                       help='å°è¯´ç±»å‹ï¼ˆå¦‚ï¼šéƒ½å¸‚ã€ç„å¹»ç­‰ï¼‰ï¼Œä¸æä¾›åˆ™ä½¿ç”¨å·²æœ‰æ•°æ®')
    parser.add_argument('--count', type=int, default=10,
                       help='çˆ¬å–æ•°é‡ï¼ˆé»˜è®¤ï¼š10ï¼Œä»…çˆ¬å–æ¨¡å¼éœ€è¦ï¼‰')
    parser.add_argument('--output', '-o', default='data/training',
                       help='è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤ï¼šdata/trainingï¼‰')
    parser.add_argument('--use-ai', action='store_true',
                       help='ä½¿ç”¨AIç”Ÿæˆæ”¹å†™æ ·æœ¬')
    parser.add_argument('--epochs', type=int, default=20,
                       help='è®­ç»ƒè½®æ•°ï¼ˆé»˜è®¤ï¼š20ï¼‰')
    parser.add_argument('--batch-size', type=int, default=16,
                       help='æ‰¹æ¬¡å¤§å°ï¼ˆé»˜è®¤ï¼š16ï¼‰')
    parser.add_argument('--incremental', action='store_true',
                       help='å¢é‡è®­ç»ƒï¼ˆé»˜è®¤ï¼šåŸºç¡€è®­ç»ƒï¼‰')
    parser.add_argument('--skip', type=str, default='',
                       help='è·³è¿‡çš„æ­¥éª¤ï¼Œç”¨é€—å·åˆ†éš”ï¼ˆå¦‚ï¼šscrape,analyze,generateï¼‰')
    parser.add_argument('--organize', action='store_true',
                       help='æ•´ç†æ•°æ®ï¼ˆæ¸…ç†ã€åˆ†ç±»ç­‰ï¼‰')
    parser.add_argument('--filter-completed', action='store_true', default=True,
                       help='åªçˆ¬å–å·²å®Œç»“çš„å°è¯´ï¼ˆé»˜è®¤å¯ç”¨ï¼Œä»…çˆ¬å–æ¨¡å¼ï¼‰')
    parser.add_argument('--no-filter-completed', dest='filter_completed', action='store_false',
                       help='ä¸ç­›é€‰ï¼Œçˆ¬å–æ‰€æœ‰å°è¯´ï¼ˆä»…çˆ¬å–æ¨¡å¼ï¼‰')
    
    args = parser.parse_args()
    
    skip_steps = [s.strip() for s in args.skip.split(',') if s.strip()]
    
    pipeline = DataPipeline(args.output)
    
    # éªŒè¯å‚æ•°
    if args.site and not args.category:
        parser.error("--site éœ€è¦é…åˆ --category ä½¿ç”¨")
    if args.category and not args.site:
        parser.error("--category éœ€è¦é…åˆ --site ä½¿ç”¨")
    
    success = pipeline.run_full_pipeline(
        site_name=args.site,
        category=args.category,
        count=args.count,
        use_ai=args.use_ai,
        epochs=args.epochs,
        batch_size=args.batch_size,
        incremental=args.incremental,
        organize_data=args.organize,
        skip_steps=skip_steps
    )
    
    sys.exit(0 if success else 1)


if __name__ == '__main__':
    main()

